{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Learning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install dependencies for AI gym to run properly (shouldn't take more than a minute). If running on google cloud or running locally, only need to run once. Colab may require installing everytime the vm shuts down."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg\n",
      "  Using cached ffmpeg-1.4-py3-none-any.whl\n",
      "Collecting xvfbwrapper\n",
      "  Downloading xvfbwrapper-0.2.9.tar.gz (5.6 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting PyOpenGL\n",
      "  Using cached PyOpenGL-3.1.7-py3-none-any.whl.metadata (3.2 kB)\n",
      "Downloading PyOpenGL-3.1.7-py3-none-any.whl (2.4 MB)\n",
      "   ---------------------------------------- 2.4/2.4 MB 38.2 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: xvfbwrapper\n",
      "  Building wheel for xvfbwrapper (setup.py): started\n",
      "  Building wheel for xvfbwrapper (setup.py): finished with status 'done'\n",
      "  Created wheel for xvfbwrapper: filename=xvfbwrapper-0.2.9-py3-none-any.whl size=5009 sha256=bc4ff1e911956830c64dfd5ae5e5d004e80477cde5b1040723fc1b0b2d10d3bf\n",
      "  Stored in directory: c:\\users\\vaibh\\appdata\\local\\pip\\cache\\wheels\\aa\\09\\0e\\c0fa4c721cfb0a003121597a24181add912b7488054d2311ad\n",
      "Successfully built xvfbwrapper\n",
      "Installing collected packages: xvfbwrapper, PyOpenGL, ffmpeg\n",
      "Successfully installed PyOpenGL-3.1.7 ffmpeg-1.4 xvfbwrapper-0.2.9\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install gym pyvirtualdisplay\n",
    "# !pip3 install ffmpeg xvfbwrapper PyOpenGL\n",
    "# !sudo apt-get install -y xvfb python-opengl ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip3 install --upgrade setuptools --user\n",
    "# !pip3 install ez_setup \n",
    "# !pip3 install gym[atari] \n",
    "# !pip3 install gym[accept-rom-license] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this assignment we will implement the Deep Q-Learning algorithm with Experience Replay as described in breakthrough paper __\"Playing Atari with Deep Reinforcement Learning\"__. We will train an agent to play the famous game of __Breakout__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, we initialize our game of __Breakout__ and you can see how the environment looks like. For further documentation of the of the environment refer to https://gym.openai.com/envs. \n",
    "\n",
    "In breakout, we will use 3 actions \"fire\", \"left\", and \"right\". \"fire\" is only used to reset the game when a life is lost, \"left\" moves the agent left and \"right\" moves the agent right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "state = env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a DQN Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we create a DQN Agent. This agent is defined in the __agent.py__. The corresponding neural network is defined in the __model.py__. Once you've created a working DQN agent, use the code in agent.py to create a double DQN agent in __agent_double.py__. Set the flag \"double_dqn\" to True to train the double DQN agent.\n",
    "\n",
    "__Evaluation Reward__ : The average reward received in the past 100 episodes/games.\n",
    "\n",
    "__Frame__ : Number of frames processed in total.\n",
    "\n",
    "__Memory Size__ : The current size of the replay memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "double_dqn = False # set to True if using double DQN agent\n",
    "\n",
    "if double_dqn:\n",
    "    from agent_double import Agent\n",
    "else:\n",
    "    from agent import Agent\n",
    "\n",
    "agent = Agent(action_size)\n",
    "evaluation_reward = deque(maxlen=evaluation_reward_length)\n",
    "frame = 0\n",
    "memory_size = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this training loop, we do not render the screen because it slows down training signficantly. To watch the agent play the game, run the code in next section \"Visualize Agent Performance\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Personal_Stuff\\UIUC\\Sem_2\\DL for CV (CS_444)\\Assignments\\assignment5\\agent.py:49: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  a = torch.tensor(a)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0   score: 2.0   memory length: 219   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 1   score: 2.0   memory length: 417   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 2   score: 2.0   memory length: 635   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 2.0\n",
      "episode: 3   score: 1.0   memory length: 785   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 4   score: 1.0   memory length: 956   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 5   score: 1.0   memory length: 1128   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 6   score: 1.0   memory length: 1299   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.4285714285714286\n",
      "episode: 7   score: 3.0   memory length: 1547   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.625\n",
      "episode: 8   score: 0.0   memory length: 1670   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.4444444444444444\n",
      "episode: 9   score: 4.0   memory length: 1966   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 10   score: 0.0   memory length: 2089   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5454545454545454\n",
      "episode: 11   score: 3.0   memory length: 2334   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.6666666666666667\n",
      "episode: 12   score: 3.0   memory length: 2580   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.7692307692307692\n",
      "episode: 13   score: 0.0   memory length: 2702   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6428571428571428\n",
      "episode: 14   score: 0.0   memory length: 2825   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5333333333333334\n",
      "episode: 15   score: 1.0   memory length: 2976   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 16   score: 0.0   memory length: 3099   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.411764705882353\n",
      "episode: 17   score: 1.0   memory length: 3249   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.3888888888888888\n",
      "episode: 18   score: 2.0   memory length: 3465   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.4210526315789473\n",
      "episode: 19   score: 0.0   memory length: 3588   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 20   score: 1.0   memory length: 3759   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.3333333333333333\n",
      "episode: 21   score: 1.0   memory length: 3928   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.3181818181818181\n",
      "episode: 22   score: 1.0   memory length: 4079   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3043478260869565\n",
      "episode: 23   score: 0.0   memory length: 4202   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 24   score: 0.0   memory length: 4324   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 25   score: 0.0   memory length: 4446   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.1538461538461537\n",
      "episode: 26   score: 3.0   memory length: 4690   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.2222222222222223\n",
      "episode: 27   score: 2.0   memory length: 4907   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 28   score: 0.0   memory length: 5030   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.206896551724138\n",
      "episode: 29   score: 2.0   memory length: 5230   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.2333333333333334\n",
      "episode: 30   score: 1.0   memory length: 5381   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.2258064516129032\n",
      "episode: 31   score: 2.0   memory length: 5578   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 32   score: 1.0   memory length: 5729   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.2424242424242424\n",
      "episode: 33   score: 5.0   memory length: 6065   epsilon: 1.0    steps: 336    lr: 0.0001     evaluation reward: 1.3529411764705883\n",
      "episode: 34   score: 0.0   memory length: 6187   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.3142857142857143\n",
      "episode: 35   score: 1.0   memory length: 6338   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.3055555555555556\n",
      "episode: 36   score: 2.0   memory length: 6536   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.3243243243243243\n",
      "episode: 37   score: 0.0   memory length: 6658   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2894736842105263\n",
      "episode: 38   score: 0.0   memory length: 6780   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2564102564102564\n",
      "episode: 39   score: 1.0   memory length: 6952   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 40   score: 3.0   memory length: 7201   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.2926829268292683\n",
      "episode: 41   score: 0.0   memory length: 7324   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2619047619047619\n",
      "episode: 42   score: 0.0   memory length: 7447   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2325581395348837\n",
      "episode: 43   score: 1.0   memory length: 7615   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.2272727272727273\n",
      "episode: 44   score: 2.0   memory length: 7835   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.2444444444444445\n",
      "episode: 45   score: 0.0   memory length: 7957   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2173913043478262\n",
      "episode: 46   score: 0.0   memory length: 8079   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.1914893617021276\n",
      "episode: 47   score: 0.0   memory length: 8202   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1666666666666667\n",
      "episode: 48   score: 0.0   memory length: 8325   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1428571428571428\n",
      "episode: 49   score: 0.0   memory length: 8448   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.12\n",
      "episode: 50   score: 0.0   memory length: 8570   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.0980392156862746\n",
      "episode: 51   score: 0.0   memory length: 8693   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.0769230769230769\n",
      "episode: 52   score: 2.0   memory length: 8913   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.0943396226415094\n",
      "episode: 53   score: 3.0   memory length: 9159   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.1296296296296295\n",
      "episode: 54   score: 0.0   memory length: 9282   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1090909090909091\n",
      "episode: 55   score: 1.0   memory length: 9451   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.1071428571428572\n",
      "episode: 56   score: 0.0   memory length: 9574   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.087719298245614\n",
      "episode: 57   score: 2.0   memory length: 9793   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.103448275862069\n",
      "episode: 58   score: 2.0   memory length: 10011   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.11864406779661\n",
      "episode: 59   score: 0.0   memory length: 10133   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 60   score: 2.0   memory length: 10334   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.1147540983606556\n",
      "episode: 61   score: 1.0   memory length: 10503   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.1129032258064515\n",
      "episode: 62   score: 1.0   memory length: 10653   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.1111111111111112\n",
      "episode: 63   score: 3.0   memory length: 10880   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.140625\n",
      "episode: 64   score: 0.0   memory length: 11002   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.123076923076923\n",
      "episode: 65   score: 1.0   memory length: 11153   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.121212121212121\n",
      "episode: 66   score: 0.0   memory length: 11275   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.1044776119402986\n",
      "episode: 67   score: 0.0   memory length: 11397   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.088235294117647\n",
      "episode: 68   score: 0.0   memory length: 11519   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.0724637681159421\n",
      "episode: 69   score: 3.0   memory length: 11766   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.1\n",
      "episode: 70   score: 3.0   memory length: 11996   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.1267605633802817\n",
      "episode: 71   score: 0.0   memory length: 12119   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1111111111111112\n",
      "episode: 72   score: 2.0   memory length: 12317   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.1232876712328768\n",
      "episode: 73   score: 2.0   memory length: 12533   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.135135135135135\n",
      "episode: 74   score: 0.0   memory length: 12656   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.12\n",
      "episode: 75   score: 0.0   memory length: 12778   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.105263157894737\n",
      "episode: 76   score: 2.0   memory length: 12976   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.1168831168831168\n",
      "episode: 77   score: 3.0   memory length: 13201   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.141025641025641\n",
      "episode: 78   score: 3.0   memory length: 13447   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.1645569620253164\n",
      "episode: 79   score: 5.0   memory length: 13775   epsilon: 1.0    steps: 328    lr: 0.0001     evaluation reward: 1.2125\n",
      "episode: 80   score: 0.0   memory length: 13898   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1975308641975309\n",
      "episode: 81   score: 1.0   memory length: 14066   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.1951219512195121\n",
      "episode: 82   score: 0.0   memory length: 14188   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.180722891566265\n",
      "episode: 83   score: 0.0   memory length: 14310   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.1666666666666667\n",
      "episode: 84   score: 1.0   memory length: 14462   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.1647058823529413\n",
      "episode: 85   score: 1.0   memory length: 14612   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.1627906976744187\n",
      "episode: 86   score: 0.0   memory length: 14735   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1494252873563218\n",
      "episode: 87   score: 2.0   memory length: 14950   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.1590909090909092\n",
      "episode: 88   score: 0.0   memory length: 15073   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.146067415730337\n",
      "episode: 89   score: 2.0   memory length: 15294   epsilon: 1.0    steps: 221    lr: 0.0001     evaluation reward: 1.1555555555555554\n",
      "episode: 90   score: 1.0   memory length: 15462   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.1538461538461537\n",
      "episode: 91   score: 4.0   memory length: 15739   epsilon: 1.0    steps: 277    lr: 0.0001     evaluation reward: 1.184782608695652\n",
      "episode: 92   score: 0.0   memory length: 15862   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.1720430107526882\n",
      "episode: 93   score: 2.0   memory length: 16041   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.1808510638297873\n",
      "episode: 94   score: 2.0   memory length: 16239   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.1894736842105262\n",
      "episode: 95   score: 3.0   memory length: 16488   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.2083333333333333\n",
      "episode: 96   score: 2.0   memory length: 16685   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.2164948453608246\n",
      "episode: 97   score: 0.0   memory length: 16807   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.2040816326530612\n",
      "episode: 98   score: 2.0   memory length: 17025   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.2121212121212122\n",
      "episode: 99   score: 1.0   memory length: 17176   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 100   score: 0.0   memory length: 17299   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 101   score: 4.0   memory length: 17598   epsilon: 1.0    steps: 299    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 102   score: 1.0   memory length: 17749   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 103   score: 1.0   memory length: 17918   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 104   score: 2.0   memory length: 18116   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 105   score: 3.0   memory length: 18342   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 106   score: 0.0   memory length: 18465   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 107   score: 2.0   memory length: 18663   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 108   score: 0.0   memory length: 18785   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 109   score: 3.0   memory length: 19033   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 110   score: 0.0   memory length: 19156   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 111   score: 3.0   memory length: 19382   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 112   score: 1.0   memory length: 19552   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 113   score: 0.0   memory length: 19675   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 114   score: 1.0   memory length: 19844   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 115   score: 0.0   memory length: 19967   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 116   score: 2.0   memory length: 20165   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 117   score: 0.0   memory length: 20288   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 118   score: 4.0   memory length: 20554   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 119   score: 1.0   memory length: 20726   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 120   score: 1.0   memory length: 20876   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 121   score: 0.0   memory length: 20999   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 122   score: 2.0   memory length: 21217   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 123   score: 3.0   memory length: 21465   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 124   score: 4.0   memory length: 21763   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 125   score: 0.0   memory length: 21886   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 126   score: 1.0   memory length: 22037   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 127   score: 2.0   memory length: 22254   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 128   score: 0.0   memory length: 22377   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 129   score: 2.0   memory length: 22595   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 130   score: 2.0   memory length: 22793   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 131   score: 0.0   memory length: 22915   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 132   score: 1.0   memory length: 23086   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 133   score: 2.0   memory length: 23304   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 134   score: 2.0   memory length: 23502   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 135   score: 0.0   memory length: 23624   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 136   score: 1.0   memory length: 23775   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 137   score: 3.0   memory length: 24038   epsilon: 1.0    steps: 263    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 138   score: 0.0   memory length: 24161   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 139   score: 2.0   memory length: 24358   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 140   score: 1.0   memory length: 24529   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 141   score: 1.0   memory length: 24699   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 142   score: 2.0   memory length: 24896   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 143   score: 0.0   memory length: 25019   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 144   score: 1.0   memory length: 25188   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 145   score: 2.0   memory length: 25404   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 146   score: 2.0   memory length: 25603   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 147   score: 3.0   memory length: 25831   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 148   score: 2.0   memory length: 26048   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 149   score: 1.0   memory length: 26219   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 150   score: 1.0   memory length: 26387   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 151   score: 1.0   memory length: 26537   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 152   score: 2.0   memory length: 26734   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 153   score: 1.0   memory length: 26886   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 154   score: 0.0   memory length: 27009   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 155   score: 3.0   memory length: 27255   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 156   score: 3.0   memory length: 27501   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 157   score: 2.0   memory length: 27699   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 158   score: 0.0   memory length: 27822   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 159   score: 2.0   memory length: 28021   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 160   score: 2.0   memory length: 28219   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 161   score: 2.0   memory length: 28399   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 162   score: 1.0   memory length: 28569   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 163   score: 1.0   memory length: 28720   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 164   score: 1.0   memory length: 28890   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 165   score: 2.0   memory length: 29090   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 166   score: 1.0   memory length: 29240   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 167   score: 3.0   memory length: 29489   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 168   score: 2.0   memory length: 29707   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 169   score: 2.0   memory length: 29905   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 170   score: 4.0   memory length: 30181   epsilon: 1.0    steps: 276    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 171   score: 1.0   memory length: 30350   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 172   score: 2.0   memory length: 30550   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 173   score: 2.0   memory length: 30747   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 174   score: 0.0   memory length: 30870   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 175   score: 1.0   memory length: 31040   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 176   score: 3.0   memory length: 31289   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 177   score: 3.0   memory length: 31536   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 178   score: 1.0   memory length: 31704   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 179   score: 1.0   memory length: 31875   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 180   score: 3.0   memory length: 32118   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 181   score: 2.0   memory length: 32315   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 182   score: 0.0   memory length: 32438   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 183   score: 0.0   memory length: 32560   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 184   score: 0.0   memory length: 32683   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 185   score: 0.0   memory length: 32805   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 186   score: 0.0   memory length: 32928   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 187   score: 2.0   memory length: 33127   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 188   score: 1.0   memory length: 33278   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 189   score: 2.0   memory length: 33477   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 190   score: 2.0   memory length: 33675   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 191   score: 1.0   memory length: 33826   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 192   score: 2.0   memory length: 34042   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 193   score: 0.0   memory length: 34164   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 194   score: 2.0   memory length: 34382   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 195   score: 0.0   memory length: 34504   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 196   score: 1.0   memory length: 34654   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 197   score: 3.0   memory length: 34919   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 198   score: 3.0   memory length: 35148   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 199   score: 4.0   memory length: 35441   epsilon: 1.0    steps: 293    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 200   score: 0.0   memory length: 35564   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 201   score: 0.0   memory length: 35686   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 202   score: 2.0   memory length: 35906   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 203   score: 4.0   memory length: 36195   epsilon: 1.0    steps: 289    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 204   score: 2.0   memory length: 36392   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 205   score: 2.0   memory length: 36607   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 206   score: 0.0   memory length: 36729   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 207   score: 1.0   memory length: 36897   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 208   score: 1.0   memory length: 37068   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 209   score: 2.0   memory length: 37268   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 210   score: 0.0   memory length: 37391   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 211   score: 1.0   memory length: 37561   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 212   score: 2.0   memory length: 37779   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 213   score: 1.0   memory length: 37949   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 214   score: 0.0   memory length: 38072   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 215   score: 3.0   memory length: 38337   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 216   score: 2.0   memory length: 38555   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 217   score: 6.0   memory length: 38972   epsilon: 1.0    steps: 417    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 218   score: 1.0   memory length: 39143   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 219   score: 1.0   memory length: 39312   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 220   score: 2.0   memory length: 39513   epsilon: 1.0    steps: 201    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 221   score: 0.0   memory length: 39635   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 222   score: 1.0   memory length: 39786   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 223   score: 0.0   memory length: 39908   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 224   score: 3.0   memory length: 40155   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 225   score: 2.0   memory length: 40352   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 226   score: 2.0   memory length: 40571   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 227   score: 2.0   memory length: 40768   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 228   score: 0.0   memory length: 40891   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 229   score: 1.0   memory length: 41060   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 230   score: 2.0   memory length: 41259   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 231   score: 3.0   memory length: 41502   epsilon: 1.0    steps: 243    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 232   score: 0.0   memory length: 41625   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 233   score: 1.0   memory length: 41794   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 234   score: 2.0   memory length: 41992   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 235   score: 1.0   memory length: 42161   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 236   score: 3.0   memory length: 42411   epsilon: 1.0    steps: 250    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 237   score: 3.0   memory length: 42638   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 238   score: 2.0   memory length: 42836   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 239   score: 2.0   memory length: 43035   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 240   score: 3.0   memory length: 43303   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 241   score: 3.0   memory length: 43529   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 242   score: 0.0   memory length: 43652   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 243   score: 1.0   memory length: 43803   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 244   score: 1.0   memory length: 43972   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 245   score: 0.0   memory length: 44095   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 246   score: 1.0   memory length: 44245   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 247   score: 3.0   memory length: 44493   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 248   score: 1.0   memory length: 44645   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 249   score: 2.0   memory length: 44862   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 250   score: 1.0   memory length: 45033   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 251   score: 1.0   memory length: 45205   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 252   score: 0.0   memory length: 45327   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 253   score: 0.0   memory length: 45450   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 254   score: 4.0   memory length: 45709   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 255   score: 2.0   memory length: 45925   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 256   score: 4.0   memory length: 46222   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 257   score: 1.0   memory length: 46373   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 258   score: 1.0   memory length: 46542   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 259   score: 1.0   memory length: 46692   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 260   score: 2.0   memory length: 46908   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 261   score: 2.0   memory length: 47105   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 262   score: 3.0   memory length: 47352   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 263   score: 2.0   memory length: 47570   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 264   score: 0.0   memory length: 47693   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 265   score: 0.0   memory length: 47815   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 266   score: 1.0   memory length: 47966   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 267   score: 2.0   memory length: 48182   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 268   score: 1.0   memory length: 48354   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 269   score: 1.0   memory length: 48523   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 270   score: 1.0   memory length: 48692   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 271   score: 0.0   memory length: 48815   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 272   score: 1.0   memory length: 48965   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 273   score: 0.0   memory length: 49088   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 274   score: 0.0   memory length: 49210   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 275   score: 1.0   memory length: 49382   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 276   score: 2.0   memory length: 49597   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 277   score: 0.0   memory length: 49719   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 278   score: 2.0   memory length: 49916   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 279   score: 0.0   memory length: 50039   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 280   score: 0.0   memory length: 50161   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 281   score: 1.0   memory length: 50311   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 282   score: 6.0   memory length: 50650   epsilon: 1.0    steps: 339    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 283   score: 1.0   memory length: 50819   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 284   score: 3.0   memory length: 51047   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 285   score: 2.0   memory length: 51244   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 286   score: 3.0   memory length: 51473   epsilon: 1.0    steps: 229    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 287   score: 2.0   memory length: 51690   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 288   score: 1.0   memory length: 51859   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 289   score: 0.0   memory length: 51981   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 290   score: 4.0   memory length: 52275   epsilon: 1.0    steps: 294    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 291   score: 0.0   memory length: 52398   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 292   score: 2.0   memory length: 52616   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 293   score: 3.0   memory length: 52841   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 294   score: 2.0   memory length: 53039   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 295   score: 2.0   memory length: 53257   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 296   score: 0.0   memory length: 53379   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 297   score: 1.0   memory length: 53530   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 298   score: 0.0   memory length: 53652   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 299   score: 2.0   memory length: 53849   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 300   score: 0.0   memory length: 53971   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 301   score: 1.0   memory length: 54140   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 302   score: 4.0   memory length: 54438   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 303   score: 2.0   memory length: 54655   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 304   score: 1.0   memory length: 54825   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 305   score: 2.0   memory length: 55025   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 306   score: 1.0   memory length: 55193   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 307   score: 1.0   memory length: 55345   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 308   score: 1.0   memory length: 55496   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 309   score: 0.0   memory length: 55618   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 310   score: 0.0   memory length: 55740   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 311   score: 2.0   memory length: 55958   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 312   score: 0.0   memory length: 56080   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 313   score: 2.0   memory length: 56278   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 314   score: 0.0   memory length: 56401   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 315   score: 0.0   memory length: 56524   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 316   score: 4.0   memory length: 56841   epsilon: 1.0    steps: 317    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 317   score: 1.0   memory length: 57011   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 318   score: 3.0   memory length: 57260   epsilon: 1.0    steps: 249    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 319   score: 3.0   memory length: 57486   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 320   score: 0.0   memory length: 57609   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 321   score: 0.0   memory length: 57732   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 322   score: 3.0   memory length: 57960   epsilon: 1.0    steps: 228    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 323   score: 0.0   memory length: 58083   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 324   score: 3.0   memory length: 58328   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 325   score: 2.0   memory length: 58546   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 326   score: 1.0   memory length: 58715   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 327   score: 2.0   memory length: 58899   epsilon: 1.0    steps: 184    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 328   score: 2.0   memory length: 59096   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 329   score: 1.0   memory length: 59267   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 330   score: 3.0   memory length: 59535   epsilon: 1.0    steps: 268    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 331   score: 2.0   memory length: 59752   epsilon: 1.0    steps: 217    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 332   score: 2.0   memory length: 59970   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 333   score: 1.0   memory length: 60139   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 334   score: 1.0   memory length: 60310   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 335   score: 2.0   memory length: 60508   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 336   score: 1.0   memory length: 60659   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 337   score: 0.0   memory length: 60782   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 338   score: 3.0   memory length: 61029   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 339   score: 1.0   memory length: 61179   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 340   score: 3.0   memory length: 61427   epsilon: 1.0    steps: 248    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 341   score: 3.0   memory length: 61673   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 342   score: 1.0   memory length: 61845   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 343   score: 4.0   memory length: 62142   epsilon: 1.0    steps: 297    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 344   score: 0.0   memory length: 62264   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 345   score: 3.0   memory length: 62508   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 346   score: 0.0   memory length: 62631   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 347   score: 4.0   memory length: 62926   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 348   score: 2.0   memory length: 63126   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 349   score: 0.0   memory length: 63249   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 350   score: 0.0   memory length: 63372   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 351   score: 4.0   memory length: 63650   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 352   score: 1.0   memory length: 63819   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 353   score: 1.0   memory length: 63971   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 354   score: 1.0   memory length: 64143   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 355   score: 1.0   memory length: 64294   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 356   score: 2.0   memory length: 64512   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 357   score: 4.0   memory length: 64803   epsilon: 1.0    steps: 291    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 358   score: 0.0   memory length: 64926   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 359   score: 4.0   memory length: 65224   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 360   score: 2.0   memory length: 65442   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 361   score: 3.0   memory length: 65689   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 362   score: 0.0   memory length: 65811   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 363   score: 2.0   memory length: 66009   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 364   score: 4.0   memory length: 66307   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 365   score: 0.0   memory length: 66430   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 366   score: 1.0   memory length: 66598   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 367   score: 1.0   memory length: 66769   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 368   score: 2.0   memory length: 66987   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 369   score: 3.0   memory length: 67253   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 370   score: 4.0   memory length: 67527   epsilon: 1.0    steps: 274    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 371   score: 0.0   memory length: 67650   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 372   score: 2.0   memory length: 67866   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 373   score: 0.0   memory length: 67988   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 374   score: 0.0   memory length: 68111   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 375   score: 0.0   memory length: 68233   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 376   score: 2.0   memory length: 68448   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 377   score: 1.0   memory length: 68617   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 378   score: 2.0   memory length: 68814   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 379   score: 3.0   memory length: 69044   epsilon: 1.0    steps: 230    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 380   score: 3.0   memory length: 69291   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 381   score: 1.0   memory length: 69460   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 382   score: 5.0   memory length: 69821   epsilon: 1.0    steps: 361    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 383   score: 0.0   memory length: 69944   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 384   score: 1.0   memory length: 70095   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 385   score: 4.0   memory length: 70407   epsilon: 1.0    steps: 312    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 386   score: 0.0   memory length: 70529   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 387   score: 4.0   memory length: 70788   epsilon: 1.0    steps: 259    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 388   score: 1.0   memory length: 70956   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 389   score: 0.0   memory length: 71078   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 390   score: 3.0   memory length: 71303   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 391   score: 3.0   memory length: 71548   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 392   score: 2.0   memory length: 71745   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 393   score: 0.0   memory length: 71868   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 394   score: 2.0   memory length: 72048   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 395   score: 3.0   memory length: 72292   epsilon: 1.0    steps: 244    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 396   score: 2.0   memory length: 72490   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 397   score: 2.0   memory length: 72688   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 398   score: 1.0   memory length: 72860   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 399   score: 0.0   memory length: 72983   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 400   score: 0.0   memory length: 73106   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 401   score: 2.0   memory length: 73306   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 402   score: 2.0   memory length: 73508   epsilon: 1.0    steps: 202    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 403   score: 4.0   memory length: 73786   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 404   score: 0.0   memory length: 73908   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 405   score: 2.0   memory length: 74108   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 406   score: 2.0   memory length: 74287   epsilon: 1.0    steps: 179    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 407   score: 1.0   memory length: 74456   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 408   score: 3.0   memory length: 74701   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 409   score: 3.0   memory length: 74948   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 410   score: 1.0   memory length: 75099   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 411   score: 1.0   memory length: 75270   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 412   score: 0.0   memory length: 75393   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 413   score: 1.0   memory length: 75563   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 414   score: 3.0   memory length: 75774   epsilon: 1.0    steps: 211    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 415   score: 2.0   memory length: 75996   epsilon: 1.0    steps: 222    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 416   score: 1.0   memory length: 76166   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 417   score: 1.0   memory length: 76335   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 418   score: 2.0   memory length: 76551   epsilon: 1.0    steps: 216    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 419   score: 0.0   memory length: 76674   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 420   score: 1.0   memory length: 76842   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 421   score: 0.0   memory length: 76965   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 422   score: 1.0   memory length: 77135   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 423   score: 3.0   memory length: 77382   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 424   score: 3.0   memory length: 77629   epsilon: 1.0    steps: 247    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 425   score: 1.0   memory length: 77800   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 426   score: 2.0   memory length: 78018   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 427   score: 1.0   memory length: 78187   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 428   score: 0.0   memory length: 78310   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 429   score: 1.0   memory length: 78461   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 430   score: 0.0   memory length: 78584   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 431   score: 1.0   memory length: 78752   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 432   score: 1.0   memory length: 78924   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 433   score: 4.0   memory length: 79202   epsilon: 1.0    steps: 278    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 434   score: 2.0   memory length: 79383   epsilon: 1.0    steps: 181    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 435   score: 0.0   memory length: 79506   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 436   score: 0.0   memory length: 79629   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 437   score: 4.0   memory length: 79927   epsilon: 1.0    steps: 298    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 438   score: 4.0   memory length: 80243   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 439   score: 2.0   memory length: 80440   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 440   score: 2.0   memory length: 80620   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 441   score: 1.0   memory length: 80770   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 442   score: 1.0   memory length: 80922   epsilon: 1.0    steps: 152    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 443   score: 0.0   memory length: 81045   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 444   score: 2.0   memory length: 81264   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 445   score: 1.0   memory length: 81435   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 446   score: 2.0   memory length: 81635   epsilon: 1.0    steps: 200    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 447   score: 2.0   memory length: 81853   epsilon: 1.0    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 448   score: 2.0   memory length: 82068   epsilon: 1.0    steps: 215    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 449   score: 2.0   memory length: 82266   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 450   score: 4.0   memory length: 82584   epsilon: 1.0    steps: 318    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 451   score: 0.0   memory length: 82707   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 452   score: 1.0   memory length: 82878   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 453   score: 2.0   memory length: 83076   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 454   score: 1.0   memory length: 83247   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 455   score: 2.0   memory length: 83429   epsilon: 1.0    steps: 182    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 456   score: 0.0   memory length: 83552   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 457   score: 4.0   memory length: 83847   epsilon: 1.0    steps: 295    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 458   score: 2.0   memory length: 84044   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 459   score: 2.0   memory length: 84224   epsilon: 1.0    steps: 180    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 460   score: 3.0   memory length: 84493   epsilon: 1.0    steps: 269    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 461   score: 2.0   memory length: 84713   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 462   score: 3.0   memory length: 84964   epsilon: 1.0    steps: 251    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 463   score: 0.0   memory length: 85087   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 464   score: 1.0   memory length: 85256   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 465   score: 0.0   memory length: 85379   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 466   score: 0.0   memory length: 85502   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 467   score: 1.0   memory length: 85653   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 468   score: 0.0   memory length: 85776   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 469   score: 0.0   memory length: 85898   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 470   score: 3.0   memory length: 86164   epsilon: 1.0    steps: 266    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 471   score: 4.0   memory length: 86439   epsilon: 1.0    steps: 275    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 472   score: 2.0   memory length: 86659   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 473   score: 4.0   memory length: 86975   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 474   score: 1.0   memory length: 87126   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 475   score: 1.0   memory length: 87277   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 476   score: 2.0   memory length: 87475   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 477   score: 2.0   memory length: 87673   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 478   score: 0.0   memory length: 87796   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 479   score: 0.0   memory length: 87919   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 480   score: 1.0   memory length: 88070   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 481   score: 0.0   memory length: 88192   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 482   score: 3.0   memory length: 88457   epsilon: 1.0    steps: 265    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 483   score: 2.0   memory length: 88677   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 484   score: 0.0   memory length: 88800   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 485   score: 1.0   memory length: 88970   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 486   score: 0.0   memory length: 89092   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 487   score: 0.0   memory length: 89215   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 488   score: 0.0   memory length: 89338   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 489   score: 1.0   memory length: 89489   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 490   score: 3.0   memory length: 89716   epsilon: 1.0    steps: 227    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 491   score: 1.0   memory length: 89886   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 492   score: 0.0   memory length: 90009   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 493   score: 1.0   memory length: 90178   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 494   score: 3.0   memory length: 90423   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 495   score: 0.0   memory length: 90546   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 496   score: 0.0   memory length: 90668   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 497   score: 1.0   memory length: 90819   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 498   score: 0.0   memory length: 90941   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 499   score: 2.0   memory length: 91139   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 500   score: 0.0   memory length: 91261   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 501   score: 4.0   memory length: 91577   epsilon: 1.0    steps: 316    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 502   score: 3.0   memory length: 91808   epsilon: 1.0    steps: 231    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 503   score: 0.0   memory length: 91931   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 504   score: 0.0   memory length: 92053   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 505   score: 1.0   memory length: 92221   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 506   score: 1.0   memory length: 92372   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 507   score: 0.0   memory length: 92495   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 508   score: 1.0   memory length: 92663   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 509   score: 1.0   memory length: 92813   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 510   score: 1.0   memory length: 92981   epsilon: 1.0    steps: 168    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 511   score: 0.0   memory length: 93103   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 512   score: 3.0   memory length: 93328   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 513   score: 1.0   memory length: 93497   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 514   score: 0.0   memory length: 93620   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 515   score: 2.0   memory length: 93840   epsilon: 1.0    steps: 220    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 516   score: 0.0   memory length: 93962   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 517   score: 0.0   memory length: 94084   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 518   score: 3.0   memory length: 94310   epsilon: 1.0    steps: 226    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 519   score: 1.0   memory length: 94480   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 520   score: 0.0   memory length: 94603   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 521   score: 1.0   memory length: 94772   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 522   score: 0.0   memory length: 94894   epsilon: 1.0    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 523   score: 2.0   memory length: 95092   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 524   score: 2.0   memory length: 95290   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 525   score: 4.0   memory length: 95586   epsilon: 1.0    steps: 296    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 526   score: 3.0   memory length: 95832   epsilon: 1.0    steps: 246    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 527   score: 1.0   memory length: 96003   epsilon: 1.0    steps: 171    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 528   score: 0.0   memory length: 96126   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 529   score: 1.0   memory length: 96295   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 530   score: 1.0   memory length: 96465   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 531   score: 2.0   memory length: 96662   epsilon: 1.0    steps: 197    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 532   score: 0.0   memory length: 96785   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 533   score: 2.0   memory length: 97004   epsilon: 1.0    steps: 219    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 534   score: 3.0   memory length: 97249   epsilon: 1.0    steps: 245    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 535   score: 3.0   memory length: 97516   epsilon: 1.0    steps: 267    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 536   score: 3.0   memory length: 97741   epsilon: 1.0    steps: 225    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 537   score: 1.0   memory length: 97891   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 538   score: 0.0   memory length: 98014   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 539   score: 1.0   memory length: 98186   epsilon: 1.0    steps: 172    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 540   score: 0.0   memory length: 98309   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 541   score: 0.0   memory length: 98432   epsilon: 1.0    steps: 123    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 542   score: 1.0   memory length: 98602   epsilon: 1.0    steps: 170    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 543   score: 2.0   memory length: 98801   epsilon: 1.0    steps: 199    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 544   score: 4.0   memory length: 99086   epsilon: 1.0    steps: 285    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 545   score: 1.0   memory length: 99255   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 546   score: 1.0   memory length: 99406   epsilon: 1.0    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 547   score: 2.0   memory length: 99604   epsilon: 1.0    steps: 198    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 548   score: 1.0   memory length: 99773   epsilon: 1.0    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 549   score: 1.0   memory length: 99923   epsilon: 1.0    steps: 150    lr: 0.0001     evaluation reward: 1.31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Personal_Stuff\\UIUC\\Sem_2\\DL for CV (CS_444)\\Assignments\\assignment5\\memory.py:29: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  sample = np.array(sample, dtype=object)\n",
      "e:\\Personal_Stuff\\UIUC\\Sem_2\\DL for CV (CS_444)\\Assignments\\assignment5\\agent.py:67: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "  mini_batch = np.array(mini_batch, dtype=object).transpose()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 550   score: 0.0   memory length: 100046   epsilon: 0.999906940000002    steps: 123    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 551   score: 3.0   memory length: 100293   epsilon: 0.9994178800000126    steps: 247    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 552   score: 2.0   memory length: 100509   epsilon: 0.9989902000000219    steps: 216    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 553   score: 1.0   memory length: 100659   epsilon: 0.9986932000000284    steps: 150    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 554   score: 2.0   memory length: 100857   epsilon: 0.9983011600000369    steps: 198    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 555   score: 1.0   memory length: 101008   epsilon: 0.9980021800000434    steps: 151    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 556   score: 1.0   memory length: 101178   epsilon: 0.9976655800000507    steps: 170    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 557   score: 0.0   memory length: 101300   epsilon: 0.9974240200000559    steps: 122    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 558   score: 0.0   memory length: 101423   epsilon: 0.9971804800000612    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 559   score: 1.0   memory length: 101592   epsilon: 0.9968458600000685    steps: 169    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 560   score: 3.0   memory length: 101819   epsilon: 0.9963964000000782    steps: 227    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 561   score: 1.0   memory length: 101990   epsilon: 0.9960578200000856    steps: 171    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 562   score: 1.0   memory length: 102159   epsilon: 0.9957232000000928    steps: 169    lr: 0.0001     evaluation reward: 1.21\n",
      "episode: 563   score: 2.0   memory length: 102357   epsilon: 0.9953311600001014    steps: 198    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 564   score: 4.0   memory length: 102675   epsilon: 0.994701520000115    steps: 318    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 565   score: 2.0   memory length: 102891   epsilon: 0.9942738400001243    steps: 216    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 566   score: 1.0   memory length: 103061   epsilon: 0.9939372400001316    steps: 170    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 567   score: 0.0   memory length: 103183   epsilon: 0.9936956800001369    steps: 122    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 568   score: 4.0   memory length: 103457   epsilon: 0.9931531600001486    steps: 274    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 569   score: 3.0   memory length: 103705   epsilon: 0.9926621200001593    steps: 248    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 570   score: 1.0   memory length: 103874   epsilon: 0.9923275000001666    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 571   score: 2.0   memory length: 104090   epsilon: 0.9918998200001758    steps: 216    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 572   score: 0.0   memory length: 104213   epsilon: 0.9916562800001811    steps: 123    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 573   score: 1.0   memory length: 104364   epsilon: 0.9913573000001876    steps: 151    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 574   score: 0.0   memory length: 104487   epsilon: 0.9911137600001929    steps: 123    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 575   score: 0.0   memory length: 104609   epsilon: 0.9908722000001982    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 576   score: 2.0   memory length: 104827   epsilon: 0.9904405600002075    steps: 218    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 577   score: 3.0   memory length: 105078   epsilon: 0.9899435800002183    steps: 251    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 578   score: 0.0   memory length: 105200   epsilon: 0.9897020200002236    steps: 122    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 579   score: 3.0   memory length: 105447   epsilon: 0.9892129600002342    steps: 247    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 580   score: 1.0   memory length: 105598   epsilon: 0.9889139800002407    steps: 151    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 581   score: 1.0   memory length: 105767   epsilon: 0.9885793600002479    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 582   score: 3.0   memory length: 106034   epsilon: 0.9880507000002594    steps: 267    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 583   score: 3.0   memory length: 106281   epsilon: 0.98756164000027    steps: 247    lr: 0.0001     evaluation reward: 1.3\n",
      "episode: 584   score: 3.0   memory length: 106530   epsilon: 0.9870686200002807    steps: 249    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 585   score: 1.0   memory length: 106682   epsilon: 0.9867676600002873    steps: 152    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 586   score: 3.0   memory length: 106908   epsilon: 0.986320180000297    steps: 226    lr: 0.0001     evaluation reward: 1.36\n",
      "episode: 587   score: 3.0   memory length: 107134   epsilon: 0.9858727000003067    steps: 226    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 588   score: 2.0   memory length: 107332   epsilon: 0.9854806600003152    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 589   score: 3.0   memory length: 107598   epsilon: 0.9849539800003266    steps: 266    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 590   score: 1.0   memory length: 107766   epsilon: 0.9846213400003339    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 591   score: 1.0   memory length: 107918   epsilon: 0.9843203800003404    steps: 152    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 592   score: 4.0   memory length: 108212   epsilon: 0.983738260000353    steps: 294    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 593   score: 4.0   memory length: 108510   epsilon: 0.9831482200003658    steps: 298    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 594   score: 4.0   memory length: 108803   epsilon: 0.9825680800003784    steps: 293    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 595   score: 0.0   memory length: 108926   epsilon: 0.9823245400003837    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 596   score: 1.0   memory length: 109095   epsilon: 0.981989920000391    steps: 169    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 597   score: 0.0   memory length: 109218   epsilon: 0.9817463800003963    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 598   score: 0.0   memory length: 109341   epsilon: 0.9815028400004016    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 599   score: 0.0   memory length: 109463   epsilon: 0.9812612800004068    steps: 122    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 600   score: 3.0   memory length: 109735   epsilon: 0.9807227200004185    steps: 272    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 601   score: 2.0   memory length: 109933   epsilon: 0.980330680000427    steps: 198    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 602   score: 0.0   memory length: 110055   epsilon: 0.9800891200004322    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 603   score: 3.0   memory length: 110324   epsilon: 0.9795565000004438    steps: 269    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 604   score: 2.0   memory length: 110543   epsilon: 0.9791228800004532    steps: 219    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 605   score: 1.0   memory length: 110714   epsilon: 0.9787843000004606    steps: 171    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 606   score: 1.0   memory length: 110882   epsilon: 0.9784516600004678    steps: 168    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 607   score: 0.0   memory length: 111005   epsilon: 0.9782081200004731    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 608   score: 0.0   memory length: 111128   epsilon: 0.9779645800004784    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 609   score: 0.0   memory length: 111250   epsilon: 0.9777230200004836    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 610   score: 2.0   memory length: 111447   epsilon: 0.9773329600004921    steps: 197    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 611   score: 4.0   memory length: 111723   epsilon: 0.9767864800005039    steps: 276    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 612   score: 1.0   memory length: 111893   epsilon: 0.9764498800005112    steps: 170    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 613   score: 1.0   memory length: 112043   epsilon: 0.9761528800005177    steps: 150    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 614   score: 2.0   memory length: 112259   epsilon: 0.975725200000527    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 615   score: 0.0   memory length: 112382   epsilon: 0.9754816600005323    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 616   score: 0.0   memory length: 112505   epsilon: 0.9752381200005376    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 617   score: 0.0   memory length: 112628   epsilon: 0.9749945800005428    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 618   score: 1.0   memory length: 112797   epsilon: 0.9746599600005501    steps: 169    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 619   score: 0.0   memory length: 112919   epsilon: 0.9744184000005554    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 620   score: 2.0   memory length: 113116   epsilon: 0.9740283400005638    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 621   score: 3.0   memory length: 113364   epsilon: 0.9735373000005745    steps: 248    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 622   score: 0.0   memory length: 113487   epsilon: 0.9732937600005798    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 623   score: 2.0   memory length: 113706   epsilon: 0.9728601400005892    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 624   score: 2.0   memory length: 113906   epsilon: 0.9724641400005978    steps: 200    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 625   score: 3.0   memory length: 114172   epsilon: 0.9719374600006092    steps: 266    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 626   score: 0.0   memory length: 114294   epsilon: 0.9716959000006145    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 627   score: 2.0   memory length: 114510   epsilon: 0.9712682200006237    steps: 216    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 628   score: 0.0   memory length: 114632   epsilon: 0.971026660000629    steps: 122    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 629   score: 1.0   memory length: 114800   epsilon: 0.9706940200006362    steps: 168    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 630   score: 0.0   memory length: 114923   epsilon: 0.9704504800006415    steps: 123    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 631   score: 1.0   memory length: 115092   epsilon: 0.9701158600006488    steps: 169    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 632   score: 0.0   memory length: 115215   epsilon: 0.969872320000654    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 633   score: 4.0   memory length: 115511   epsilon: 0.9692862400006668    steps: 296    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 634   score: 0.0   memory length: 115634   epsilon: 0.969042700000672    steps: 123    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 635   score: 0.0   memory length: 115756   epsilon: 0.9688011400006773    steps: 122    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 636   score: 2.0   memory length: 115972   epsilon: 0.9683734600006866    steps: 216    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 637   score: 0.0   memory length: 116095   epsilon: 0.9681299200006919    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 638   score: 0.0   memory length: 116218   epsilon: 0.9678863800006972    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 639   score: 2.0   memory length: 116437   epsilon: 0.9674527600007066    steps: 219    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 640   score: 1.0   memory length: 116587   epsilon: 0.967155760000713    steps: 150    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 641   score: 1.0   memory length: 116756   epsilon: 0.9668211400007203    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 642   score: 2.0   memory length: 116953   epsilon: 0.9664310800007287    steps: 197    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 643   score: 0.0   memory length: 117076   epsilon: 0.966187540000734    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 644   score: 1.0   memory length: 117227   epsilon: 0.9658885600007405    steps: 151    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 645   score: 2.0   memory length: 117425   epsilon: 0.965496520000749    steps: 198    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 646   score: 1.0   memory length: 117593   epsilon: 0.9651638800007563    steps: 168    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 647   score: 3.0   memory length: 117819   epsilon: 0.964716400000766    steps: 226    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 648   score: 1.0   memory length: 117970   epsilon: 0.9644174200007725    steps: 151    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 649   score: 1.0   memory length: 118139   epsilon: 0.9640828000007797    steps: 169    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 650   score: 2.0   memory length: 118356   epsilon: 0.963653140000789    steps: 217    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 651   score: 0.0   memory length: 118479   epsilon: 0.9634096000007943    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 652   score: 1.0   memory length: 118650   epsilon: 0.9630710200008017    steps: 171    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 653   score: 4.0   memory length: 118907   epsilon: 0.9625621600008127    steps: 257    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 654   score: 0.0   memory length: 119030   epsilon: 0.962318620000818    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 655   score: 0.0   memory length: 119153   epsilon: 0.9620750800008233    steps: 123    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 656   score: 0.0   memory length: 119275   epsilon: 0.9618335200008286    steps: 122    lr: 0.0001     evaluation reward: 1.39\n",
      "episode: 657   score: 2.0   memory length: 119493   epsilon: 0.9614018800008379    steps: 218    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 658   score: 0.0   memory length: 119616   epsilon: 0.9611583400008432    steps: 123    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 659   score: 0.0   memory length: 119738   epsilon: 0.9609167800008485    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 660   score: 1.0   memory length: 119888   epsilon: 0.9606197800008549    steps: 150    lr: 0.0001     evaluation reward: 1.38\n",
      "episode: 661   score: 0.0   memory length: 120011   epsilon: 0.9603762400008602    steps: 123    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 662   score: 1.0   memory length: 120161   epsilon: 0.9600792400008666    steps: 150    lr: 0.0001     evaluation reward: 1.37\n",
      "episode: 663   score: 0.0   memory length: 120283   epsilon: 0.9598376800008719    steps: 122    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 664   score: 1.0   memory length: 120434   epsilon: 0.9595387000008784    steps: 151    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 665   score: 1.0   memory length: 120584   epsilon: 0.9592417000008848    steps: 150    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 666   score: 3.0   memory length: 120828   epsilon: 0.9587585800008953    steps: 244    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 667   score: 2.0   memory length: 121028   epsilon: 0.9583625800009039    steps: 200    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 668   score: 3.0   memory length: 121273   epsilon: 0.9578774800009144    steps: 245    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 669   score: 1.0   memory length: 121442   epsilon: 0.9575428600009217    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 670   score: 3.0   memory length: 121706   epsilon: 0.957020140000933    steps: 264    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 671   score: 0.0   memory length: 121828   epsilon: 0.9567785800009383    steps: 122    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 672   score: 1.0   memory length: 121979   epsilon: 0.9564796000009448    steps: 151    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 673   score: 2.0   memory length: 122177   epsilon: 0.9560875600009533    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 674   score: 1.0   memory length: 122346   epsilon: 0.9557529400009606    steps: 169    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 675   score: 0.0   memory length: 122469   epsilon: 0.9555094000009658    steps: 123    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 676   score: 1.0   memory length: 122639   epsilon: 0.9551728000009732    steps: 170    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 677   score: 1.0   memory length: 122808   epsilon: 0.9548381800009804    steps: 169    lr: 0.0001     evaluation reward: 1.32\n",
      "episode: 678   score: 2.0   memory length: 123006   epsilon: 0.9544461400009889    steps: 198    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 679   score: 3.0   memory length: 123258   epsilon: 0.9539471800009998    steps: 252    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 680   score: 0.0   memory length: 123380   epsilon: 0.953705620001005    steps: 122    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 681   score: 3.0   memory length: 123626   epsilon: 0.9532185400010156    steps: 246    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 682   score: 1.0   memory length: 123795   epsilon: 0.9528839200010228    steps: 169    lr: 0.0001     evaluation reward: 1.33\n",
      "episode: 683   score: 1.0   memory length: 123964   epsilon: 0.9525493000010301    steps: 169    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 684   score: 1.0   memory length: 124132   epsilon: 0.9522166600010373    steps: 168    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 685   score: 1.0   memory length: 124301   epsilon: 0.9518820400010446    steps: 169    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 686   score: 2.0   memory length: 124499   epsilon: 0.9514900000010531    steps: 198    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 687   score: 1.0   memory length: 124650   epsilon: 0.9511910200010596    steps: 151    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 688   score: 3.0   memory length: 124897   epsilon: 0.9507019600010702    steps: 247    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 689   score: 0.0   memory length: 125019   epsilon: 0.9504604000010755    steps: 122    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 690   score: 0.0   memory length: 125142   epsilon: 0.9502168600010807    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 691   score: 0.0   memory length: 125265   epsilon: 0.949973320001086    steps: 123    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 692   score: 1.0   memory length: 125433   epsilon: 0.9496406800010933    steps: 168    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 693   score: 1.0   memory length: 125601   epsilon: 0.9493080400011005    steps: 168    lr: 0.0001     evaluation reward: 1.16\n",
      "episode: 694   score: 2.0   memory length: 125819   epsilon: 0.9488764000011098    steps: 218    lr: 0.0001     evaluation reward: 1.14\n",
      "episode: 695   score: 4.0   memory length: 126113   epsilon: 0.9482942800011225    steps: 294    lr: 0.0001     evaluation reward: 1.18\n",
      "episode: 696   score: 2.0   memory length: 126311   epsilon: 0.947902240001131    steps: 198    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 697   score: 1.0   memory length: 126482   epsilon: 0.9475636600011383    steps: 171    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 698   score: 2.0   memory length: 126682   epsilon: 0.9471676600011469    steps: 200    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 699   score: 1.0   memory length: 126833   epsilon: 0.9468686800011534    steps: 151    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 700   score: 2.0   memory length: 127051   epsilon: 0.9464370400011628    steps: 218    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 701   score: 3.0   memory length: 127305   epsilon: 0.9459341200011737    steps: 254    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 702   score: 0.0   memory length: 127428   epsilon: 0.945690580001179    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 703   score: 2.0   memory length: 127625   epsilon: 0.9453005200011875    steps: 197    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 704   score: 0.0   memory length: 127747   epsilon: 0.9450589600011927    steps: 122    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 705   score: 0.0   memory length: 127870   epsilon: 0.944815420001198    steps: 123    lr: 0.0001     evaluation reward: 1.19\n",
      "episode: 706   score: 2.0   memory length: 128085   epsilon: 0.9443897200012072    steps: 215    lr: 0.0001     evaluation reward: 1.2\n",
      "episode: 707   score: 2.0   memory length: 128284   epsilon: 0.9439957000012158    steps: 199    lr: 0.0001     evaluation reward: 1.22\n",
      "episode: 708   score: 1.0   memory length: 128435   epsilon: 0.9436967200012223    steps: 151    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 709   score: 2.0   memory length: 128633   epsilon: 0.9433046800012308    steps: 198    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 710   score: 2.0   memory length: 128851   epsilon: 0.9428730400012402    steps: 218    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 711   score: 3.0   memory length: 129099   epsilon: 0.9423820000012508    steps: 248    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 712   score: 2.0   memory length: 129296   epsilon: 0.9419919400012593    steps: 197    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 713   score: 1.0   memory length: 129465   epsilon: 0.9416573200012666    steps: 169    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 714   score: 0.0   memory length: 129588   epsilon: 0.9414137800012718    steps: 123    lr: 0.0001     evaluation reward: 1.23\n",
      "episode: 715   score: 1.0   memory length: 129756   epsilon: 0.9410811400012791    steps: 168    lr: 0.0001     evaluation reward: 1.24\n",
      "episode: 716   score: 1.0   memory length: 129924   epsilon: 0.9407485000012863    steps: 168    lr: 0.0001     evaluation reward: 1.25\n",
      "episode: 717   score: 1.0   memory length: 130074   epsilon: 0.9404515000012927    steps: 150    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 718   score: 2.0   memory length: 130272   epsilon: 0.9400594600013013    steps: 198    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 719   score: 2.0   memory length: 130470   epsilon: 0.9396674200013098    steps: 198    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 720   score: 2.0   memory length: 130650   epsilon: 0.9393110200013175    steps: 180    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 721   score: 0.0   memory length: 130772   epsilon: 0.9390694600013227    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 722   score: 1.0   memory length: 130942   epsilon: 0.93873286000133    steps: 170    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 723   score: 3.0   memory length: 131172   epsilon: 0.9382774600013399    steps: 230    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 724   score: 3.0   memory length: 131399   epsilon: 0.9378280000013497    steps: 227    lr: 0.0001     evaluation reward: 1.29\n",
      "episode: 725   score: 0.0   memory length: 131521   epsilon: 0.9375864400013549    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 726   score: 2.0   memory length: 131718   epsilon: 0.9371963800013634    steps: 197    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 727   score: 0.0   memory length: 131840   epsilon: 0.9369548200013686    steps: 122    lr: 0.0001     evaluation reward: 1.26\n",
      "episode: 728   score: 1.0   memory length: 132011   epsilon: 0.936616240001376    steps: 171    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 729   score: 1.0   memory length: 132180   epsilon: 0.9362816200013833    steps: 169    lr: 0.0001     evaluation reward: 1.27\n",
      "episode: 730   score: 1.0   memory length: 132330   epsilon: 0.9359846200013897    steps: 150    lr: 0.0001     evaluation reward: 1.28\n",
      "episode: 731   score: 4.0   memory length: 132605   epsilon: 0.9354401200014015    steps: 275    lr: 0.0001     evaluation reward: 1.31\n",
      "episode: 732   score: 3.0   memory length: 132848   epsilon: 0.934958980001412    steps: 243    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 733   score: 4.0   memory length: 133125   epsilon: 0.9344105200014239    steps: 277    lr: 0.0001     evaluation reward: 1.34\n",
      "episode: 734   score: 1.0   memory length: 133276   epsilon: 0.9341115400014304    steps: 151    lr: 0.0001     evaluation reward: 1.35\n",
      "episode: 735   score: 7.0   memory length: 133527   epsilon: 0.9336145600014412    steps: 251    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 736   score: 3.0   memory length: 133791   epsilon: 0.9330918400014525    steps: 264    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 737   score: 3.0   memory length: 134016   epsilon: 0.9326463400014622    steps: 225    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 738   score: 0.0   memory length: 134138   epsilon: 0.9324047800014674    steps: 122    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 739   score: 2.0   memory length: 134336   epsilon: 0.9320127400014759    steps: 198    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 740   score: 0.0   memory length: 134459   epsilon: 0.9317692000014812    steps: 123    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 741   score: 0.0   memory length: 134582   epsilon: 0.9315256600014865    steps: 123    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 742   score: 0.0   memory length: 134705   epsilon: 0.9312821200014918    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 743   score: 1.0   memory length: 134856   epsilon: 0.9309831400014983    steps: 151    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 744   score: 3.0   memory length: 135081   epsilon: 0.930537640001508    steps: 225    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 745   score: 1.0   memory length: 135250   epsilon: 0.9302030200015152    steps: 169    lr: 0.0001     evaluation reward: 1.44\n",
      "episode: 746   score: 0.0   memory length: 135373   epsilon: 0.9299594800015205    steps: 123    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 747   score: 0.0   memory length: 135495   epsilon: 0.9297179200015258    steps: 122    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 748   score: 1.0   memory length: 135664   epsilon: 0.929383300001533    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 749   score: 2.0   memory length: 135881   epsilon: 0.9289536400015423    steps: 217    lr: 0.0001     evaluation reward: 1.41\n",
      "episode: 750   score: 1.0   memory length: 136050   epsilon: 0.9286190200015496    steps: 169    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 751   score: 2.0   memory length: 136248   epsilon: 0.9282269800015581    steps: 198    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 752   score: 2.0   memory length: 136466   epsilon: 0.9277953400015675    steps: 218    lr: 0.0001     evaluation reward: 1.43\n",
      "episode: 753   score: 1.0   memory length: 136636   epsilon: 0.9274587400015748    steps: 170    lr: 0.0001     evaluation reward: 1.4\n",
      "episode: 754   score: 2.0   memory length: 136836   epsilon: 0.9270627400015834    steps: 200    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 755   score: 0.0   memory length: 136959   epsilon: 0.9268192000015887    steps: 123    lr: 0.0001     evaluation reward: 1.42\n",
      "episode: 756   score: 4.0   memory length: 137233   epsilon: 0.9262766800016005    steps: 274    lr: 0.0001     evaluation reward: 1.46\n",
      "episode: 757   score: 6.0   memory length: 137602   epsilon: 0.9255460600016163    steps: 369    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 758   score: 3.0   memory length: 137847   epsilon: 0.9250609600016269    steps: 245    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 759   score: 0.0   memory length: 137969   epsilon: 0.9248194000016321    steps: 122    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 760   score: 5.0   memory length: 138291   epsilon: 0.9241818400016459    steps: 322    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 761   score: 1.0   memory length: 138462   epsilon: 0.9238432600016533    steps: 171    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 762   score: 2.0   memory length: 138680   epsilon: 0.9234116200016627    steps: 218    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 763   score: 3.0   memory length: 138946   epsilon: 0.9228849400016741    steps: 266    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 764   score: 2.0   memory length: 139144   epsilon: 0.9224929000016826    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 765   score: 1.0   memory length: 139313   epsilon: 0.9221582800016899    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 766   score: 0.0   memory length: 139435   epsilon: 0.9219167200016951    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 767   score: 0.0   memory length: 139557   epsilon: 0.9216751600017004    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 768   score: 2.0   memory length: 139774   epsilon: 0.9212455000017097    steps: 217    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 769   score: 3.0   memory length: 140024   epsilon: 0.9207505000017204    steps: 250    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 770   score: 0.0   memory length: 140147   epsilon: 0.9205069600017257    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 771   score: 2.0   memory length: 140365   epsilon: 0.9200753200017351    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 772   score: 3.0   memory length: 140629   epsilon: 0.9195526000017464    steps: 264    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 773   score: 1.0   memory length: 140780   epsilon: 0.9192536200017529    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 774   score: 0.0   memory length: 140903   epsilon: 0.9190100800017582    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 775   score: 0.0   memory length: 141026   epsilon: 0.9187665400017635    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 776   score: 1.0   memory length: 141178   epsilon: 0.91846558000177    steps: 152    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 777   score: 1.0   memory length: 141329   epsilon: 0.9181666000017765    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 778   score: 0.0   memory length: 141452   epsilon: 0.9179230600017818    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 779   score: 3.0   memory length: 141703   epsilon: 0.9174260800017926    steps: 251    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 780   score: 4.0   memory length: 141979   epsilon: 0.9168796000018045    steps: 276    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 781   score: 4.0   memory length: 142273   epsilon: 0.9162974800018171    steps: 294    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 782   score: 0.0   memory length: 142396   epsilon: 0.9160539400018224    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 783   score: 1.0   memory length: 142565   epsilon: 0.9157193200018297    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 784   score: 2.0   memory length: 142762   epsilon: 0.9153292600018381    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 785   score: 2.0   memory length: 142962   epsilon: 0.9149332600018467    steps: 200    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 786   score: 3.0   memory length: 143209   epsilon: 0.9144442000018573    steps: 247    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 787   score: 1.0   memory length: 143360   epsilon: 0.9141452200018638    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 788   score: 0.0   memory length: 143482   epsilon: 0.9139036600018691    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 789   score: 0.0   memory length: 143605   epsilon: 0.9136601200018744    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 790   score: 0.0   memory length: 143727   epsilon: 0.9134185600018796    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 791   score: 1.0   memory length: 143896   epsilon: 0.9130839400018869    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 792   score: 3.0   memory length: 144140   epsilon: 0.9126008200018974    steps: 244    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 793   score: 2.0   memory length: 144320   epsilon: 0.9122444200019051    steps: 180    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 794   score: 3.0   memory length: 144550   epsilon: 0.911789020001915    steps: 230    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 795   score: 3.0   memory length: 144797   epsilon: 0.9112999600019256    steps: 247    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 796   score: 2.0   memory length: 145016   epsilon: 0.910866340001935    steps: 219    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 797   score: 2.0   memory length: 145233   epsilon: 0.9104366800019443    steps: 217    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 798   score: 0.0   memory length: 145355   epsilon: 0.9101951200019496    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 799   score: 4.0   memory length: 145650   epsilon: 0.9096110200019623    steps: 295    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 800   score: 0.0   memory length: 145773   epsilon: 0.9093674800019675    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 801   score: 1.0   memory length: 145944   epsilon: 0.9090289000019749    steps: 171    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 802   score: 3.0   memory length: 146190   epsilon: 0.9085418200019855    steps: 246    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 803   score: 2.0   memory length: 146407   epsilon: 0.9081121600019948    steps: 217    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 804   score: 2.0   memory length: 146587   epsilon: 0.9077557600020025    steps: 180    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 805   score: 2.0   memory length: 146804   epsilon: 0.9073261000020119    steps: 217    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 806   score: 0.0   memory length: 146926   epsilon: 0.9070845400020171    steps: 122    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 807   score: 0.0   memory length: 147049   epsilon: 0.9068410000020224    steps: 123    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 808   score: 5.0   memory length: 147392   epsilon: 0.9061618600020371    steps: 343    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 809   score: 3.0   memory length: 147620   epsilon: 0.9057104200020469    steps: 228    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 810   score: 2.0   memory length: 147818   epsilon: 0.9053183800020554    steps: 198    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 811   score: 3.0   memory length: 148088   epsilon: 0.904783780002067    steps: 270    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 812   score: 1.0   memory length: 148239   epsilon: 0.9044848000020735    steps: 151    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 813   score: 0.0   memory length: 148361   epsilon: 0.9042432400020788    steps: 122    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 814   score: 5.0   memory length: 148704   epsilon: 0.9035641000020935    steps: 343    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 815   score: 2.0   memory length: 148904   epsilon: 0.9031681000021021    steps: 200    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 816   score: 1.0   memory length: 149055   epsilon: 0.9028691200021086    steps: 151    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 817   score: 1.0   memory length: 149206   epsilon: 0.9025701400021151    steps: 151    lr: 0.0001     evaluation reward: 1.74\n",
      "episode: 818   score: 0.0   memory length: 149328   epsilon: 0.9023285800021204    steps: 122    lr: 0.0001     evaluation reward: 1.72\n",
      "episode: 819   score: 3.0   memory length: 149555   epsilon: 0.9018791200021301    steps: 227    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 820   score: 2.0   memory length: 149752   epsilon: 0.9014890600021386    steps: 197    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 821   score: 2.0   memory length: 149950   epsilon: 0.9010970200021471    steps: 198    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 822   score: 1.0   memory length: 150119   epsilon: 0.9007624000021544    steps: 169    lr: 0.0001     evaluation reward: 1.75\n",
      "episode: 823   score: 1.0   memory length: 150270   epsilon: 0.9004634200021608    steps: 151    lr: 0.0001     evaluation reward: 1.73\n",
      "episode: 824   score: 0.0   memory length: 150393   epsilon: 0.9002198800021661    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 825   score: 0.0   memory length: 150516   epsilon: 0.8999763400021714    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 826   score: 2.0   memory length: 150734   epsilon: 0.8995447000021808    steps: 218    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 827   score: 0.0   memory length: 150857   epsilon: 0.8993011600021861    steps: 123    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 828   score: 2.0   memory length: 151075   epsilon: 0.8988695200021954    steps: 218    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 829   score: 0.0   memory length: 151197   epsilon: 0.8986279600022007    steps: 122    lr: 0.0001     evaluation reward: 1.7\n",
      "episode: 830   score: 2.0   memory length: 151395   epsilon: 0.8982359200022092    steps: 198    lr: 0.0001     evaluation reward: 1.71\n",
      "episode: 831   score: 2.0   memory length: 151577   epsilon: 0.897875560002217    steps: 182    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 832   score: 2.0   memory length: 151775   epsilon: 0.8974835200022255    steps: 198    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 833   score: 0.0   memory length: 151898   epsilon: 0.8972399800022308    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 834   score: 3.0   memory length: 152142   epsilon: 0.8967568600022413    steps: 244    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 835   score: 2.0   memory length: 152340   epsilon: 0.8963648200022498    steps: 198    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 836   score: 1.0   memory length: 152491   epsilon: 0.8960658400022563    steps: 151    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 837   score: 2.0   memory length: 152707   epsilon: 0.8956381600022656    steps: 216    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 838   score: 1.0   memory length: 152877   epsilon: 0.8953015600022729    steps: 170    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 839   score: 3.0   memory length: 153104   epsilon: 0.8948521000022827    steps: 227    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 840   score: 2.0   memory length: 153322   epsilon: 0.894420460002292    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 841   score: 2.0   memory length: 153519   epsilon: 0.8940304000023005    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 842   score: 0.0   memory length: 153642   epsilon: 0.8937868600023058    steps: 123    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 843   score: 0.0   memory length: 153765   epsilon: 0.8935433200023111    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 844   score: 3.0   memory length: 154029   epsilon: 0.8930206000023224    steps: 264    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 845   score: 4.0   memory length: 154286   epsilon: 0.8925117400023335    steps: 257    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 846   score: 0.0   memory length: 154409   epsilon: 0.8922682000023388    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 847   score: 1.0   memory length: 154578   epsilon: 0.891933580002346    steps: 169    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 848   score: 0.0   memory length: 154701   epsilon: 0.8916900400023513    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 849   score: 3.0   memory length: 154968   epsilon: 0.8911613800023628    steps: 267    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 850   score: 0.0   memory length: 155090   epsilon: 0.890919820002368    steps: 122    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 851   score: 2.0   memory length: 155307   epsilon: 0.8904901600023774    steps: 217    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 852   score: 5.0   memory length: 155651   epsilon: 0.8898090400023921    steps: 344    lr: 0.0001     evaluation reward: 1.69\n",
      "episode: 853   score: 0.0   memory length: 155774   epsilon: 0.8895655000023974    steps: 123    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 854   score: 0.0   memory length: 155897   epsilon: 0.8893219600024027    steps: 123    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 855   score: 2.0   memory length: 156115   epsilon: 0.8888903200024121    steps: 218    lr: 0.0001     evaluation reward: 1.68\n",
      "episode: 856   score: 2.0   memory length: 156313   epsilon: 0.8884982800024206    steps: 198    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 857   score: 2.0   memory length: 156492   epsilon: 0.8881438600024283    steps: 179    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 858   score: 0.0   memory length: 156614   epsilon: 0.8879023000024335    steps: 122    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 859   score: 2.0   memory length: 156831   epsilon: 0.8874726400024429    steps: 217    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 860   score: 1.0   memory length: 156982   epsilon: 0.8871736600024493    steps: 151    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 861   score: 3.0   memory length: 157249   epsilon: 0.8866450000024608    steps: 267    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 862   score: 2.0   memory length: 157469   epsilon: 0.8862094000024703    steps: 220    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 863   score: 1.0   memory length: 157638   epsilon: 0.8858747800024775    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 864   score: 0.0   memory length: 157761   epsilon: 0.8856312400024828    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 865   score: 0.0   memory length: 157884   epsilon: 0.8853877000024881    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 866   score: 2.0   memory length: 158082   epsilon: 0.8849956600024966    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 867   score: 3.0   memory length: 158326   epsilon: 0.8845125400025071    steps: 244    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 868   score: 1.0   memory length: 158477   epsilon: 0.8842135600025136    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 869   score: 1.0   memory length: 158628   epsilon: 0.8839145800025201    steps: 151    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 870   score: 1.0   memory length: 158799   epsilon: 0.8835760000025275    steps: 171    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 871   score: 2.0   memory length: 158997   epsilon: 0.883183960002536    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 872   score: 1.0   memory length: 159166   epsilon: 0.8828493400025432    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 873   score: 0.0   memory length: 159288   epsilon: 0.8826077800025485    steps: 122    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 874   score: 5.0   memory length: 159635   epsilon: 0.8819207200025634    steps: 347    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 875   score: 1.0   memory length: 159806   epsilon: 0.8815821400025707    steps: 171    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 876   score: 3.0   memory length: 160052   epsilon: 0.8810950600025813    steps: 246    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 877   score: 2.0   memory length: 160270   epsilon: 0.8806634200025907    steps: 218    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 878   score: 4.0   memory length: 160566   epsilon: 0.8800773400026034    steps: 296    lr: 0.0001     evaluation reward: 1.67\n",
      "episode: 879   score: 2.0   memory length: 160784   epsilon: 0.8796457000026128    steps: 218    lr: 0.0001     evaluation reward: 1.66\n",
      "episode: 880   score: 1.0   memory length: 160955   epsilon: 0.8793071200026201    steps: 171    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 881   score: 3.0   memory length: 161205   epsilon: 0.8788121200026309    steps: 250    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 882   score: 1.0   memory length: 161355   epsilon: 0.8785151200026373    steps: 150    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 883   score: 0.0   memory length: 161478   epsilon: 0.8782715800026426    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 884   score: 3.0   memory length: 161705   epsilon: 0.8778221200026524    steps: 227    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 885   score: 2.0   memory length: 161920   epsilon: 0.8773964200026616    steps: 215    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 886   score: 0.0   memory length: 162042   epsilon: 0.8771548600026668    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 887   score: 1.0   memory length: 162192   epsilon: 0.8768578600026733    steps: 150    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 888   score: 0.0   memory length: 162315   epsilon: 0.8766143200026786    steps: 123    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 889   score: 1.0   memory length: 162466   epsilon: 0.8763153400026851    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 890   score: 1.0   memory length: 162616   epsilon: 0.8760183400026915    steps: 150    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 891   score: 2.0   memory length: 162834   epsilon: 0.8755867000027009    steps: 218    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 892   score: 3.0   memory length: 163101   epsilon: 0.8750580400027124    steps: 267    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 893   score: 4.0   memory length: 163376   epsilon: 0.8745135400027242    steps: 275    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 894   score: 0.0   memory length: 163498   epsilon: 0.8742719800027294    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 895   score: 2.0   memory length: 163695   epsilon: 0.8738819200027379    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 896   score: 0.0   memory length: 163818   epsilon: 0.8736383800027432    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 897   score: 1.0   memory length: 163987   epsilon: 0.8733037600027505    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 898   score: 0.0   memory length: 164110   epsilon: 0.8730602200027557    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 899   score: 2.0   memory length: 164331   epsilon: 0.8726226400027652    steps: 221    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 900   score: 0.0   memory length: 164454   epsilon: 0.8723791000027705    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 901   score: 0.0   memory length: 164577   epsilon: 0.8721355600027758    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 902   score: 7.0   memory length: 164952   epsilon: 0.8713930600027919    steps: 375    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 903   score: 3.0   memory length: 165214   epsilon: 0.8708743000028032    steps: 262    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 904   score: 2.0   memory length: 165433   epsilon: 0.8704406800028126    steps: 219    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 905   score: 1.0   memory length: 165602   epsilon: 0.8701060600028199    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 906   score: 0.0   memory length: 165725   epsilon: 0.8698625200028252    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 907   score: 2.0   memory length: 165926   epsilon: 0.8694645400028338    steps: 201    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 908   score: 2.0   memory length: 166126   epsilon: 0.8690685400028424    steps: 200    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 909   score: 1.0   memory length: 166276   epsilon: 0.8687715400028488    steps: 150    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 910   score: 0.0   memory length: 166399   epsilon: 0.8685280000028541    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 911   score: 0.0   memory length: 166522   epsilon: 0.8682844600028594    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 912   score: 2.0   memory length: 166741   epsilon: 0.8678508400028688    steps: 219    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 913   score: 2.0   memory length: 166957   epsilon: 0.8674231600028781    steps: 216    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 914   score: 4.0   memory length: 167270   epsilon: 0.8668034200028916    steps: 313    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 915   score: 0.0   memory length: 167393   epsilon: 0.8665598800028969    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 916   score: 2.0   memory length: 167576   epsilon: 0.8661975400029047    steps: 183    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 917   score: 0.0   memory length: 167698   epsilon: 0.86595598000291    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 918   score: 2.0   memory length: 167914   epsilon: 0.8655283000029192    steps: 216    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 919   score: 0.0   memory length: 168037   epsilon: 0.8652847600029245    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 920   score: 2.0   memory length: 168254   epsilon: 0.8648551000029339    steps: 217    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 921   score: 2.0   memory length: 168451   epsilon: 0.8644650400029423    steps: 197    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 922   score: 1.0   memory length: 168619   epsilon: 0.8641324000029496    steps: 168    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 923   score: 1.0   memory length: 168787   epsilon: 0.8637997600029568    steps: 168    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 924   score: 0.0   memory length: 168910   epsilon: 0.8635562200029621    steps: 123    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 925   score: 3.0   memory length: 169157   epsilon: 0.8630671600029727    steps: 247    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 926   score: 3.0   memory length: 169386   epsilon: 0.8626137400029825    steps: 229    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 927   score: 0.0   memory length: 169509   epsilon: 0.8623702000029878    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 928   score: 0.0   memory length: 169632   epsilon: 0.8621266600029931    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 929   score: 3.0   memory length: 169881   epsilon: 0.8616336400030038    steps: 249    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 930   score: 2.0   memory length: 170099   epsilon: 0.8612020000030132    steps: 218    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 931   score: 3.0   memory length: 170325   epsilon: 0.8607545200030229    steps: 226    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 932   score: 0.0   memory length: 170448   epsilon: 0.8605109800030282    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 933   score: 1.0   memory length: 170617   epsilon: 0.8601763600030354    steps: 169    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 934   score: 2.0   memory length: 170834   epsilon: 0.8597467000030448    steps: 217    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 935   score: 1.0   memory length: 171006   epsilon: 0.8594061400030522    steps: 172    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 936   score: 0.0   memory length: 171129   epsilon: 0.8591626000030574    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 937   score: 3.0   memory length: 171399   epsilon: 0.858628000003069    steps: 270    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 938   score: 0.0   memory length: 171522   epsilon: 0.8583844600030743    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 939   score: 0.0   memory length: 171645   epsilon: 0.8581409200030796    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 940   score: 8.0   memory length: 171985   epsilon: 0.8574677200030942    steps: 340    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 941   score: 1.0   memory length: 172153   epsilon: 0.8571350800031015    steps: 168    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 942   score: 2.0   memory length: 172336   epsilon: 0.8567727400031093    steps: 183    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 943   score: 4.0   memory length: 172614   epsilon: 0.8562223000031213    steps: 278    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 944   score: 2.0   memory length: 172812   epsilon: 0.8558302600031298    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 945   score: 1.0   memory length: 172981   epsilon: 0.855495640003137    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 946   score: 0.0   memory length: 173104   epsilon: 0.8552521000031423    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 947   score: 6.0   memory length: 173501   epsilon: 0.8544660400031594    steps: 397    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 948   score: 3.0   memory length: 173765   epsilon: 0.8539433200031707    steps: 264    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 949   score: 1.0   memory length: 173936   epsilon: 0.8536047400031781    steps: 171    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 950   score: 0.0   memory length: 174059   epsilon: 0.8533612000031834    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 951   score: 2.0   memory length: 174256   epsilon: 0.8529711400031919    steps: 197    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 952   score: 0.0   memory length: 174379   epsilon: 0.8527276000031971    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 953   score: 0.0   memory length: 174502   epsilon: 0.8524840600032024    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 954   score: 3.0   memory length: 174713   epsilon: 0.8520662800032115    steps: 211    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 955   score: 2.0   memory length: 174929   epsilon: 0.8516386000032208    steps: 216    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 956   score: 4.0   memory length: 175204   epsilon: 0.8510941000032326    steps: 275    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 957   score: 2.0   memory length: 175388   epsilon: 0.8507297800032405    steps: 184    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 958   score: 2.0   memory length: 175586   epsilon: 0.850337740003249    steps: 198    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 959   score: 2.0   memory length: 175805   epsilon: 0.8499041200032584    steps: 219    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 960   score: 0.0   memory length: 175927   epsilon: 0.8496625600032637    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 961   score: 0.0   memory length: 176049   epsilon: 0.8494210000032689    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 962   score: 1.0   memory length: 176218   epsilon: 0.8490863800032762    steps: 169    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 963   score: 2.0   memory length: 176436   epsilon: 0.8486547400032856    steps: 218    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 964   score: 2.0   memory length: 176651   epsilon: 0.8482290400032948    steps: 215    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 965   score: 2.0   memory length: 176848   epsilon: 0.8478389800033033    steps: 197    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 966   score: 0.0   memory length: 176970   epsilon: 0.8475974200033085    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 967   score: 0.0   memory length: 177093   epsilon: 0.8473538800033138    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 968   score: 3.0   memory length: 177340   epsilon: 0.8468648200033244    steps: 247    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 969   score: 2.0   memory length: 177522   epsilon: 0.8465044600033322    steps: 182    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 970   score: 2.0   memory length: 177720   epsilon: 0.8461124200033407    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 971   score: 0.0   memory length: 177843   epsilon: 0.845868880003346    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 972   score: 1.0   memory length: 178012   epsilon: 0.8455342600033533    steps: 169    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 973   score: 2.0   memory length: 178210   epsilon: 0.8451422200033618    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 974   score: 0.0   memory length: 178333   epsilon: 0.8448986800033671    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 975   score: 1.0   memory length: 178502   epsilon: 0.8445640600033744    steps: 169    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 976   score: 3.0   memory length: 178750   epsilon: 0.844073020003385    steps: 248    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 977   score: 4.0   memory length: 179048   epsilon: 0.8434829800033978    steps: 298    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 978   score: 2.0   memory length: 179246   epsilon: 0.8430909400034063    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 979   score: 0.0   memory length: 179369   epsilon: 0.8428474000034116    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 980   score: 0.0   memory length: 179491   epsilon: 0.8426058400034169    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 981   score: 2.0   memory length: 179689   epsilon: 0.8422138000034254    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 982   score: 2.0   memory length: 179888   epsilon: 0.8418197800034339    steps: 199    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 983   score: 2.0   memory length: 180086   epsilon: 0.8414277400034424    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 984   score: 0.0   memory length: 180209   epsilon: 0.8411842000034477    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 985   score: 4.0   memory length: 180502   epsilon: 0.8406040600034603    steps: 293    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 986   score: 2.0   memory length: 180700   epsilon: 0.8402120200034688    steps: 198    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 987   score: 1.0   memory length: 180851   epsilon: 0.8399130400034753    steps: 151    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 988   score: 3.0   memory length: 181077   epsilon: 0.839465560003485    steps: 226    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 989   score: 1.0   memory length: 181245   epsilon: 0.8391329200034923    steps: 168    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 990   score: 3.0   memory length: 181471   epsilon: 0.838685440003502    steps: 226    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 991   score: 3.0   memory length: 181696   epsilon: 0.8382399400035117    steps: 225    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 992   score: 2.0   memory length: 181894   epsilon: 0.8378479000035202    steps: 198    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 993   score: 1.0   memory length: 182063   epsilon: 0.8375132800035274    steps: 169    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 994   score: 2.0   memory length: 182279   epsilon: 0.8370856000035367    steps: 216    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 995   score: 2.0   memory length: 182477   epsilon: 0.8366935600035452    steps: 198    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 996   score: 1.0   memory length: 182628   epsilon: 0.8363945800035517    steps: 151    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 997   score: 0.0   memory length: 182750   epsilon: 0.836153020003557    steps: 122    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 998   score: 2.0   memory length: 182967   epsilon: 0.8357233600035663    steps: 217    lr: 0.0001     evaluation reward: 1.64\n",
      "episode: 999   score: 1.0   memory length: 183119   epsilon: 0.8354224000035728    steps: 152    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1000   score: 2.0   memory length: 183319   epsilon: 0.8350264000035814    steps: 200    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1001   score: 0.0   memory length: 183441   epsilon: 0.8347848400035867    steps: 122    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1002   score: 4.0   memory length: 183726   epsilon: 0.8342205400035989    steps: 285    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1003   score: 3.0   memory length: 183953   epsilon: 0.8337710800036087    steps: 227    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1004   score: 2.0   memory length: 184171   epsilon: 0.833339440003618    steps: 218    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1005   score: 1.0   memory length: 184322   epsilon: 0.8330404600036245    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1006   score: 0.0   memory length: 184445   epsilon: 0.8327969200036298    steps: 123    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1007   score: 3.0   memory length: 184692   epsilon: 0.8323078600036404    steps: 247    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1008   score: 1.0   memory length: 184843   epsilon: 0.8320088800036469    steps: 151    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1009   score: 2.0   memory length: 185060   epsilon: 0.8315792200036562    steps: 217    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1010   score: 0.0   memory length: 185183   epsilon: 0.8313356800036615    steps: 123    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1011   score: 2.0   memory length: 185381   epsilon: 0.83094364000367    steps: 198    lr: 0.0001     evaluation reward: 1.65\n",
      "episode: 1012   score: 0.0   memory length: 185503   epsilon: 0.8307020800036753    steps: 122    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1013   score: 1.0   memory length: 185653   epsilon: 0.8304050800036817    steps: 150    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1014   score: 0.0   memory length: 185776   epsilon: 0.830161540003687    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1015   score: 1.0   memory length: 185926   epsilon: 0.8298645400036935    steps: 150    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1016   score: 3.0   memory length: 186174   epsilon: 0.8293735000037041    steps: 248    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1017   score: 1.0   memory length: 186325   epsilon: 0.8290745200037106    steps: 151    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1018   score: 0.0   memory length: 186448   epsilon: 0.8288309800037159    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1019   score: 2.0   memory length: 186647   epsilon: 0.8284369600037245    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1020   score: 2.0   memory length: 186846   epsilon: 0.828042940003733    steps: 199    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1021   score: 1.0   memory length: 186996   epsilon: 0.8277459400037395    steps: 150    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1022   score: 0.0   memory length: 187119   epsilon: 0.8275024000037448    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1023   score: 1.0   memory length: 187269   epsilon: 0.8272054000037512    steps: 150    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1024   score: 2.0   memory length: 187451   epsilon: 0.826845040003759    steps: 182    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1025   score: 0.0   memory length: 187573   epsilon: 0.8266034800037643    steps: 122    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1026   score: 1.0   memory length: 187742   epsilon: 0.8262688600037715    steps: 169    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1027   score: 2.0   memory length: 187960   epsilon: 0.8258372200037809    steps: 218    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1028   score: 3.0   memory length: 188188   epsilon: 0.8253857800037907    steps: 228    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1029   score: 0.0   memory length: 188311   epsilon: 0.825142240003796    steps: 123    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1030   score: 4.0   memory length: 188585   epsilon: 0.8245997200038078    steps: 274    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1031   score: 3.0   memory length: 188854   epsilon: 0.8240671000038193    steps: 269    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1032   score: 0.0   memory length: 188976   epsilon: 0.8238255400038246    steps: 122    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1033   score: 2.0   memory length: 189196   epsilon: 0.823389940003834    steps: 220    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1034   score: 0.0   memory length: 189319   epsilon: 0.8231464000038393    steps: 123    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1035   score: 2.0   memory length: 189517   epsilon: 0.8227543600038478    steps: 198    lr: 0.0001     evaluation reward: 1.6\n",
      "episode: 1036   score: 2.0   memory length: 189698   epsilon: 0.8223959800038556    steps: 181    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1037   score: 2.0   memory length: 189895   epsilon: 0.8220059200038641    steps: 197    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1038   score: 1.0   memory length: 190063   epsilon: 0.8216732800038713    steps: 168    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1039   score: 1.0   memory length: 190232   epsilon: 0.8213386600038786    steps: 169    lr: 0.0001     evaluation reward: 1.63\n",
      "episode: 1040   score: 0.0   memory length: 190354   epsilon: 0.8210971000038838    steps: 122    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1041   score: 1.0   memory length: 190524   epsilon: 0.8207605000038911    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1042   score: 4.0   memory length: 190820   epsilon: 0.8201744200039038    steps: 296    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1043   score: 3.0   memory length: 191050   epsilon: 0.8197190200039137    steps: 230    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1044   score: 3.0   memory length: 191276   epsilon: 0.8192715400039234    steps: 226    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1045   score: 1.0   memory length: 191445   epsilon: 0.8189369200039307    steps: 169    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1046   score: 0.0   memory length: 191567   epsilon: 0.818695360003936    steps: 122    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1047   score: 0.0   memory length: 191690   epsilon: 0.8184518200039412    steps: 123    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1048   score: 0.0   memory length: 191812   epsilon: 0.8182102600039465    steps: 122    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1049   score: 2.0   memory length: 191991   epsilon: 0.8178558400039542    steps: 179    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1050   score: 2.0   memory length: 192213   epsilon: 0.8174162800039637    steps: 222    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1051   score: 0.0   memory length: 192336   epsilon: 0.817172740003969    steps: 123    lr: 0.0001     evaluation reward: 1.49\n",
      "episode: 1052   score: 3.0   memory length: 192565   epsilon: 0.8167193200039788    steps: 229    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1053   score: 2.0   memory length: 192763   epsilon: 0.8163272800039874    steps: 198    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1054   score: 0.0   memory length: 192885   epsilon: 0.8160857200039926    steps: 122    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1055   score: 2.0   memory length: 193102   epsilon: 0.8156560600040019    steps: 217    lr: 0.0001     evaluation reward: 1.51\n",
      "episode: 1056   score: 0.0   memory length: 193225   epsilon: 0.8154125200040072    steps: 123    lr: 0.0001     evaluation reward: 1.47\n",
      "episode: 1057   score: 0.0   memory length: 193347   epsilon: 0.8151709600040125    steps: 122    lr: 0.0001     evaluation reward: 1.45\n",
      "episode: 1058   score: 5.0   memory length: 193689   epsilon: 0.8144938000040272    steps: 342    lr: 0.0001     evaluation reward: 1.48\n",
      "episode: 1059   score: 4.0   memory length: 193985   epsilon: 0.8139077200040399    steps: 296    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1060   score: 2.0   memory length: 194203   epsilon: 0.8134760800040493    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1061   score: 3.0   memory length: 194450   epsilon: 0.8129870200040599    steps: 247    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1062   score: 2.0   memory length: 194648   epsilon: 0.8125949800040684    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1063   score: 0.0   memory length: 194771   epsilon: 0.8123514400040737    steps: 123    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1064   score: 0.0   memory length: 194893   epsilon: 0.8121098800040789    steps: 122    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1065   score: 0.0   memory length: 195015   epsilon: 0.8118683200040842    steps: 122    lr: 0.0001     evaluation reward: 1.5\n",
      "episode: 1066   score: 2.0   memory length: 195233   epsilon: 0.8114366800040935    steps: 218    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1067   score: 2.0   memory length: 195451   epsilon: 0.8110050400041029    steps: 218    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1068   score: 3.0   memory length: 195676   epsilon: 0.8105595400041126    steps: 225    lr: 0.0001     evaluation reward: 1.54\n",
      "episode: 1069   score: 0.0   memory length: 195799   epsilon: 0.8103160000041179    steps: 123    lr: 0.0001     evaluation reward: 1.52\n",
      "episode: 1070   score: 3.0   memory length: 196064   epsilon: 0.8097913000041292    steps: 265    lr: 0.0001     evaluation reward: 1.53\n",
      "episode: 1071   score: 4.0   memory length: 196327   epsilon: 0.8092705600041405    steps: 263    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1072   score: 3.0   memory length: 196573   epsilon: 0.8087834800041511    steps: 246    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1073   score: 2.0   memory length: 196771   epsilon: 0.8083914400041596    steps: 198    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1074   score: 3.0   memory length: 197017   epsilon: 0.8079043600041702    steps: 246    lr: 0.0001     evaluation reward: 1.62\n",
      "episode: 1075   score: 0.0   memory length: 197140   epsilon: 0.8076608200041755    steps: 123    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1076   score: 3.0   memory length: 197412   epsilon: 0.8071222600041872    steps: 272    lr: 0.0001     evaluation reward: 1.61\n",
      "episode: 1077   score: 0.0   memory length: 197535   epsilon: 0.8068787200041925    steps: 123    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1078   score: 0.0   memory length: 197658   epsilon: 0.8066351800041978    steps: 123    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1079   score: 1.0   memory length: 197828   epsilon: 0.8062985800042051    steps: 170    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1080   score: 2.0   memory length: 198025   epsilon: 0.8059085200042135    steps: 197    lr: 0.0001     evaluation reward: 1.58\n",
      "episode: 1081   score: 0.0   memory length: 198148   epsilon: 0.8056649800042188    steps: 123    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1082   score: 1.0   memory length: 198318   epsilon: 0.8053283800042261    steps: 170    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1083   score: 3.0   memory length: 198544   epsilon: 0.8048809000042358    steps: 226    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1084   score: 3.0   memory length: 198771   epsilon: 0.8044314400042456    steps: 227    lr: 0.0001     evaluation reward: 1.59\n",
      "episode: 1085   score: 1.0   memory length: 198923   epsilon: 0.8041304800042521    steps: 152    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1086   score: 1.0   memory length: 199073   epsilon: 0.8038334800042586    steps: 150    lr: 0.0001     evaluation reward: 1.55\n",
      "episode: 1087   score: 2.0   memory length: 199271   epsilon: 0.8034414400042671    steps: 198    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1088   score: 3.0   memory length: 199496   epsilon: 0.8029959400042768    steps: 225    lr: 0.0001     evaluation reward: 1.56\n",
      "episode: 1089   score: 2.0   memory length: 199694   epsilon: 0.8026039000042853    steps: 198    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1090   score: 3.0   memory length: 199919   epsilon: 0.802158400004295    steps: 225    lr: 0.0001     evaluation reward: 1.57\n",
      "episode: 1091   score: 2.0   memory length: 200137   epsilon: 0.8017267600043043    steps: 218    lr: 4e-05     evaluation reward: 1.56\n",
      "episode: 1092   score: 0.0   memory length: 200260   epsilon: 0.8014832200043096    steps: 123    lr: 4e-05     evaluation reward: 1.54\n",
      "episode: 1093   score: 1.0   memory length: 200429   epsilon: 0.8011486000043169    steps: 169    lr: 4e-05     evaluation reward: 1.54\n",
      "episode: 1094   score: 1.0   memory length: 200580   epsilon: 0.8008496200043234    steps: 151    lr: 4e-05     evaluation reward: 1.53\n",
      "episode: 1095   score: 3.0   memory length: 200825   epsilon: 0.8003645200043339    steps: 245    lr: 4e-05     evaluation reward: 1.54\n",
      "episode: 1096   score: 1.0   memory length: 200976   epsilon: 0.8000655400043404    steps: 151    lr: 4e-05     evaluation reward: 1.54\n",
      "episode: 1097   score: 2.0   memory length: 201174   epsilon: 0.7996735000043489    steps: 198    lr: 4e-05     evaluation reward: 1.56\n",
      "episode: 1098   score: 1.0   memory length: 201345   epsilon: 0.7993349200043562    steps: 171    lr: 4e-05     evaluation reward: 1.55\n",
      "episode: 1099   score: 0.0   memory length: 201468   epsilon: 0.7990913800043615    steps: 123    lr: 4e-05     evaluation reward: 1.54\n",
      "episode: 1100   score: 1.0   memory length: 201637   epsilon: 0.7987567600043688    steps: 169    lr: 4e-05     evaluation reward: 1.53\n",
      "episode: 1101   score: 3.0   memory length: 201889   epsilon: 0.7982578000043796    steps: 252    lr: 4e-05     evaluation reward: 1.56\n",
      "episode: 1102   score: 0.0   memory length: 202011   epsilon: 0.7980162400043849    steps: 122    lr: 4e-05     evaluation reward: 1.52\n",
      "episode: 1103   score: 0.0   memory length: 202133   epsilon: 0.7977746800043901    steps: 122    lr: 4e-05     evaluation reward: 1.49\n",
      "episode: 1104   score: 3.0   memory length: 202359   epsilon: 0.7973272000043998    steps: 226    lr: 4e-05     evaluation reward: 1.5\n",
      "episode: 1105   score: 1.0   memory length: 202509   epsilon: 0.7970302000044063    steps: 150    lr: 4e-05     evaluation reward: 1.5\n",
      "episode: 1106   score: 3.0   memory length: 202718   epsilon: 0.7966163800044153    steps: 209    lr: 4e-05     evaluation reward: 1.53\n",
      "episode: 1107   score: 1.0   memory length: 202889   epsilon: 0.7962778000044226    steps: 171    lr: 4e-05     evaluation reward: 1.51\n",
      "episode: 1108   score: 9.0   memory length: 203239   epsilon: 0.7955848000044377    steps: 350    lr: 4e-05     evaluation reward: 1.59\n",
      "episode: 1109   score: 0.0   memory length: 203362   epsilon: 0.7953412600044429    steps: 123    lr: 4e-05     evaluation reward: 1.57\n",
      "episode: 1110   score: 0.0   memory length: 203484   epsilon: 0.7950997000044482    steps: 122    lr: 4e-05     evaluation reward: 1.57\n",
      "episode: 1111   score: 3.0   memory length: 203730   epsilon: 0.7946126200044588    steps: 246    lr: 4e-05     evaluation reward: 1.58\n",
      "episode: 1112   score: 0.0   memory length: 203853   epsilon: 0.794369080004464    steps: 123    lr: 4e-05     evaluation reward: 1.58\n",
      "episode: 1113   score: 0.0   memory length: 203976   epsilon: 0.7941255400044693    steps: 123    lr: 4e-05     evaluation reward: 1.57\n",
      "episode: 1114   score: 0.0   memory length: 204098   epsilon: 0.7938839800044746    steps: 122    lr: 4e-05     evaluation reward: 1.57\n",
      "episode: 1115   score: 4.0   memory length: 204392   epsilon: 0.7933018600044872    steps: 294    lr: 4e-05     evaluation reward: 1.6\n",
      "episode: 1116   score: 2.0   memory length: 204610   epsilon: 0.7928702200044966    steps: 218    lr: 4e-05     evaluation reward: 1.59\n",
      "episode: 1117   score: 5.0   memory length: 204940   epsilon: 0.7922168200045108    steps: 330    lr: 4e-05     evaluation reward: 1.63\n",
      "episode: 1118   score: 3.0   memory length: 205151   epsilon: 0.7917990400045198    steps: 211    lr: 4e-05     evaluation reward: 1.66\n",
      "episode: 1119   score: 2.0   memory length: 205370   epsilon: 0.7913654200045293    steps: 219    lr: 4e-05     evaluation reward: 1.66\n",
      "episode: 1120   score: 0.0   memory length: 205493   epsilon: 0.7911218800045345    steps: 123    lr: 4e-05     evaluation reward: 1.64\n",
      "episode: 1121   score: 2.0   memory length: 205710   epsilon: 0.7906922200045439    steps: 217    lr: 4e-05     evaluation reward: 1.65\n",
      "episode: 1122   score: 1.0   memory length: 205860   epsilon: 0.7903952200045503    steps: 150    lr: 4e-05     evaluation reward: 1.66\n",
      "episode: 1123   score: 3.0   memory length: 206088   epsilon: 0.7899437800045601    steps: 228    lr: 4e-05     evaluation reward: 1.68\n",
      "episode: 1124   score: 0.0   memory length: 206210   epsilon: 0.7897022200045654    steps: 122    lr: 4e-05     evaluation reward: 1.66\n",
      "episode: 1125   score: 2.0   memory length: 206410   epsilon: 0.789306220004574    steps: 200    lr: 4e-05     evaluation reward: 1.68\n",
      "episode: 1126   score: 2.0   memory length: 206627   epsilon: 0.7888765600045833    steps: 217    lr: 4e-05     evaluation reward: 1.69\n",
      "episode: 1127   score: 1.0   memory length: 206798   epsilon: 0.7885379800045906    steps: 171    lr: 4e-05     evaluation reward: 1.68\n",
      "episode: 1128   score: 1.0   memory length: 206966   epsilon: 0.7882053400045979    steps: 168    lr: 4e-05     evaluation reward: 1.66\n",
      "episode: 1129   score: 3.0   memory length: 207229   epsilon: 0.7876846000046092    steps: 263    lr: 4e-05     evaluation reward: 1.69\n",
      "episode: 1130   score: 6.0   memory length: 207560   epsilon: 0.7870292200046234    steps: 331    lr: 4e-05     evaluation reward: 1.71\n",
      "episode: 1131   score: 4.0   memory length: 207856   epsilon: 0.7864431400046361    steps: 296    lr: 4e-05     evaluation reward: 1.72\n",
      "episode: 1132   score: 3.0   memory length: 208102   epsilon: 0.7859560600046467    steps: 246    lr: 4e-05     evaluation reward: 1.75\n",
      "episode: 1133   score: 2.0   memory length: 208301   epsilon: 0.7855620400046552    steps: 199    lr: 4e-05     evaluation reward: 1.75\n",
      "episode: 1134   score: 4.0   memory length: 208614   epsilon: 0.7849423000046687    steps: 313    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1135   score: 1.0   memory length: 208764   epsilon: 0.7846453000046751    steps: 150    lr: 4e-05     evaluation reward: 1.78\n",
      "episode: 1136   score: 3.0   memory length: 209011   epsilon: 0.7841562400046858    steps: 247    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1137   score: 3.0   memory length: 209273   epsilon: 0.783637480004697    steps: 262    lr: 4e-05     evaluation reward: 1.8\n",
      "episode: 1138   score: 0.0   memory length: 209395   epsilon: 0.7833959200047023    steps: 122    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1139   score: 4.0   memory length: 209689   epsilon: 0.7828138000047149    steps: 294    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1140   score: 0.0   memory length: 209812   epsilon: 0.7825702600047202    steps: 123    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1141   score: 1.0   memory length: 209963   epsilon: 0.7822712800047267    steps: 151    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1142   score: 1.0   memory length: 210113   epsilon: 0.7819742800047331    steps: 150    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1143   score: 1.0   memory length: 210281   epsilon: 0.7816416400047403    steps: 168    lr: 4e-05     evaluation reward: 1.77\n",
      "episode: 1144   score: 2.0   memory length: 210479   epsilon: 0.7812496000047489    steps: 198    lr: 4e-05     evaluation reward: 1.76\n",
      "episode: 1145   score: 2.0   memory length: 210659   epsilon: 0.7808932000047566    steps: 180    lr: 4e-05     evaluation reward: 1.77\n",
      "episode: 1146   score: 4.0   memory length: 210957   epsilon: 0.7803031600047694    steps: 298    lr: 4e-05     evaluation reward: 1.81\n",
      "episode: 1147   score: 2.0   memory length: 211172   epsilon: 0.7798774600047786    steps: 215    lr: 4e-05     evaluation reward: 1.83\n",
      "episode: 1148   score: 4.0   memory length: 211470   epsilon: 0.7792874200047915    steps: 298    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1149   score: 1.0   memory length: 211639   epsilon: 0.7789528000047987    steps: 169    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1150   score: 1.0   memory length: 211809   epsilon: 0.778616200004806    steps: 170    lr: 4e-05     evaluation reward: 1.85\n",
      "episode: 1151   score: 0.0   memory length: 211931   epsilon: 0.7783746400048113    steps: 122    lr: 4e-05     evaluation reward: 1.85\n",
      "episode: 1152   score: 2.0   memory length: 212110   epsilon: 0.778020220004819    steps: 179    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1153   score: 2.0   memory length: 212308   epsilon: 0.7776281800048275    steps: 198    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1154   score: 4.0   memory length: 212584   epsilon: 0.7770817000048393    steps: 276    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1155   score: 1.0   memory length: 212734   epsilon: 0.7767847000048458    steps: 150    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1156   score: 0.0   memory length: 212857   epsilon: 0.7765411600048511    steps: 123    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1157   score: 2.0   memory length: 213054   epsilon: 0.7761511000048595    steps: 197    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1158   score: 3.0   memory length: 213299   epsilon: 0.7756660000048701    steps: 245    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1159   score: 3.0   memory length: 213526   epsilon: 0.7752165400048798    steps: 227    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1160   score: 0.0   memory length: 213648   epsilon: 0.7749749800048851    steps: 122    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1161   score: 0.0   memory length: 213771   epsilon: 0.7747314400048904    steps: 123    lr: 4e-05     evaluation reward: 1.81\n",
      "episode: 1162   score: 0.0   memory length: 213893   epsilon: 0.7744898800048956    steps: 122    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1163   score: 0.0   memory length: 214015   epsilon: 0.7742483200049008    steps: 122    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1164   score: 2.0   memory length: 214214   epsilon: 0.7738543000049094    steps: 199    lr: 4e-05     evaluation reward: 1.81\n",
      "episode: 1165   score: 1.0   memory length: 214386   epsilon: 0.7735137400049168    steps: 172    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1166   score: 1.0   memory length: 214537   epsilon: 0.7732147600049233    steps: 151    lr: 4e-05     evaluation reward: 1.81\n",
      "episode: 1167   score: 0.0   memory length: 214660   epsilon: 0.7729712200049286    steps: 123    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1168   score: 3.0   memory length: 214909   epsilon: 0.7724782000049393    steps: 249    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1169   score: 3.0   memory length: 215175   epsilon: 0.7719515200049507    steps: 266    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1170   score: 0.0   memory length: 215297   epsilon: 0.771709960004956    steps: 122    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1171   score: 4.0   memory length: 215555   epsilon: 0.771199120004967    steps: 258    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1172   score: 2.0   memory length: 215772   epsilon: 0.7707694600049764    steps: 217    lr: 4e-05     evaluation reward: 1.78\n",
      "episode: 1173   score: 1.0   memory length: 215923   epsilon: 0.7704704800049829    steps: 151    lr: 4e-05     evaluation reward: 1.77\n",
      "episode: 1174   score: 2.0   memory length: 216124   epsilon: 0.7700725000049915    steps: 201    lr: 4e-05     evaluation reward: 1.76\n",
      "episode: 1175   score: 3.0   memory length: 216350   epsilon: 0.7696250200050012    steps: 226    lr: 4e-05     evaluation reward: 1.79\n",
      "episode: 1176   score: 2.0   memory length: 216530   epsilon: 0.769268620005009    steps: 180    lr: 4e-05     evaluation reward: 1.78\n",
      "episode: 1177   score: 3.0   memory length: 216777   epsilon: 0.7687795600050196    steps: 247    lr: 4e-05     evaluation reward: 1.81\n",
      "episode: 1178   score: 1.0   memory length: 216927   epsilon: 0.768482560005026    steps: 150    lr: 4e-05     evaluation reward: 1.82\n",
      "episode: 1179   score: 4.0   memory length: 217169   epsilon: 0.7680034000050364    steps: 242    lr: 4e-05     evaluation reward: 1.85\n",
      "episode: 1180   score: 1.0   memory length: 217338   epsilon: 0.7676687800050437    steps: 169    lr: 4e-05     evaluation reward: 1.84\n",
      "episode: 1181   score: 4.0   memory length: 217632   epsilon: 0.7670866600050563    steps: 294    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1182   score: 1.0   memory length: 217803   epsilon: 0.7667480800050637    steps: 171    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1183   score: 3.0   memory length: 218048   epsilon: 0.7662629800050742    steps: 245    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1184   score: 0.0   memory length: 218170   epsilon: 0.7660214200050794    steps: 122    lr: 4e-05     evaluation reward: 1.85\n",
      "episode: 1185   score: 2.0   memory length: 218368   epsilon: 0.765629380005088    steps: 198    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1186   score: 3.0   memory length: 218594   epsilon: 0.7651819000050977    steps: 226    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1187   score: 2.0   memory length: 218792   epsilon: 0.7647898600051062    steps: 198    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1188   score: 2.0   memory length: 218972   epsilon: 0.7644334600051139    steps: 180    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1189   score: 3.0   memory length: 219218   epsilon: 0.7639463800051245    steps: 246    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1190   score: 1.0   memory length: 219369   epsilon: 0.763647400005131    steps: 151    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1191   score: 3.0   memory length: 219638   epsilon: 0.7631147800051425    steps: 269    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1192   score: 0.0   memory length: 219761   epsilon: 0.7628712400051478    steps: 123    lr: 4e-05     evaluation reward: 1.87\n",
      "episode: 1193   score: 0.0   memory length: 219883   epsilon: 0.7626296800051531    steps: 122    lr: 4e-05     evaluation reward: 1.86\n",
      "episode: 1194   score: 4.0   memory length: 220165   epsilon: 0.7620713200051652    steps: 282    lr: 4e-05     evaluation reward: 1.89\n",
      "episode: 1195   score: 2.0   memory length: 220381   epsilon: 0.7616436400051745    steps: 216    lr: 4e-05     evaluation reward: 1.88\n",
      "episode: 1196   score: 7.0   memory length: 220778   epsilon: 0.7608575800051915    steps: 397    lr: 4e-05     evaluation reward: 1.94\n",
      "episode: 1197   score: 2.0   memory length: 220960   epsilon: 0.7604972200051994    steps: 182    lr: 4e-05     evaluation reward: 1.94\n",
      "episode: 1198   score: 3.0   memory length: 221207   epsilon: 0.76000816000521    steps: 247    lr: 4e-05     evaluation reward: 1.96\n",
      "episode: 1199   score: 3.0   memory length: 221436   epsilon: 0.7595547400052198    steps: 229    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1200   score: 2.0   memory length: 221634   epsilon: 0.7591627000052283    steps: 198    lr: 4e-05     evaluation reward: 2.0\n",
      "episode: 1201   score: 1.0   memory length: 221803   epsilon: 0.7588280800052356    steps: 169    lr: 4e-05     evaluation reward: 1.98\n",
      "episode: 1202   score: 4.0   memory length: 222086   epsilon: 0.7582677400052478    steps: 283    lr: 4e-05     evaluation reward: 2.02\n",
      "episode: 1203   score: 1.0   memory length: 222237   epsilon: 0.7579687600052543    steps: 151    lr: 4e-05     evaluation reward: 2.03\n",
      "episode: 1204   score: 3.0   memory length: 222506   epsilon: 0.7574361400052658    steps: 269    lr: 4e-05     evaluation reward: 2.03\n",
      "episode: 1205   score: 2.0   memory length: 222689   epsilon: 0.7570738000052737    steps: 183    lr: 4e-05     evaluation reward: 2.04\n",
      "episode: 1206   score: 1.0   memory length: 222839   epsilon: 0.7567768000052801    steps: 150    lr: 4e-05     evaluation reward: 2.02\n",
      "episode: 1207   score: 4.0   memory length: 223114   epsilon: 0.756232300005292    steps: 275    lr: 4e-05     evaluation reward: 2.05\n",
      "episode: 1208   score: 3.0   memory length: 223340   epsilon: 0.7557848200053017    steps: 226    lr: 4e-05     evaluation reward: 1.99\n",
      "episode: 1209   score: 3.0   memory length: 223588   epsilon: 0.7552937800053123    steps: 248    lr: 4e-05     evaluation reward: 2.02\n",
      "episode: 1210   score: 1.0   memory length: 223757   epsilon: 0.7549591600053196    steps: 169    lr: 4e-05     evaluation reward: 2.03\n",
      "episode: 1211   score: 2.0   memory length: 223975   epsilon: 0.754527520005329    steps: 218    lr: 4e-05     evaluation reward: 2.02\n",
      "episode: 1212   score: 0.0   memory length: 224097   epsilon: 0.7542859600053342    steps: 122    lr: 4e-05     evaluation reward: 2.02\n",
      "episode: 1213   score: 3.0   memory length: 224364   epsilon: 0.7537573000053457    steps: 267    lr: 4e-05     evaluation reward: 2.05\n",
      "episode: 1214   score: 1.0   memory length: 224514   epsilon: 0.7534603000053521    steps: 150    lr: 4e-05     evaluation reward: 2.06\n",
      "episode: 1215   score: 3.0   memory length: 224740   epsilon: 0.7530128200053618    steps: 226    lr: 4e-05     evaluation reward: 2.05\n",
      "episode: 1216   score: 1.0   memory length: 224891   epsilon: 0.7527138400053683    steps: 151    lr: 4e-05     evaluation reward: 2.04\n",
      "episode: 1217   score: 2.0   memory length: 225106   epsilon: 0.7522881400053776    steps: 215    lr: 4e-05     evaluation reward: 2.01\n",
      "episode: 1218   score: 2.0   memory length: 225324   epsilon: 0.751856500005387    steps: 218    lr: 4e-05     evaluation reward: 2.0\n",
      "episode: 1219   score: 3.0   memory length: 225534   epsilon: 0.751440700005396    steps: 210    lr: 4e-05     evaluation reward: 2.01\n",
      "episode: 1220   score: 6.0   memory length: 225884   epsilon: 0.750747700005411    steps: 350    lr: 4e-05     evaluation reward: 2.07\n",
      "episode: 1221   score: 1.0   memory length: 226053   epsilon: 0.7504130800054183    steps: 169    lr: 4e-05     evaluation reward: 2.06\n",
      "episode: 1222   score: 1.0   memory length: 226203   epsilon: 0.7501160800054247    steps: 150    lr: 4e-05     evaluation reward: 2.06\n",
      "episode: 1223   score: 3.0   memory length: 226452   epsilon: 0.7496230600054354    steps: 249    lr: 4e-05     evaluation reward: 2.06\n",
      "episode: 1224   score: 7.0   memory length: 226837   epsilon: 0.748860760005452    steps: 385    lr: 4e-05     evaluation reward: 2.13\n",
      "episode: 1225   score: 1.0   memory length: 226988   epsilon: 0.7485617800054585    steps: 151    lr: 4e-05     evaluation reward: 2.12\n",
      "episode: 1226   score: 1.0   memory length: 227156   epsilon: 0.7482291400054657    steps: 168    lr: 4e-05     evaluation reward: 2.11\n",
      "episode: 1227   score: 3.0   memory length: 227367   epsilon: 0.7478113600054748    steps: 211    lr: 4e-05     evaluation reward: 2.13\n",
      "episode: 1228   score: 3.0   memory length: 227615   epsilon: 0.7473203200054854    steps: 248    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1229   score: 1.0   memory length: 227767   epsilon: 0.747019360005492    steps: 152    lr: 4e-05     evaluation reward: 2.13\n",
      "episode: 1230   score: 2.0   memory length: 227948   epsilon: 0.7466609800054997    steps: 181    lr: 4e-05     evaluation reward: 2.09\n",
      "episode: 1231   score: 3.0   memory length: 228179   epsilon: 0.7462036000055097    steps: 231    lr: 4e-05     evaluation reward: 2.08\n",
      "episode: 1232   score: 3.0   memory length: 228428   epsilon: 0.7457105800055204    steps: 249    lr: 4e-05     evaluation reward: 2.08\n",
      "episode: 1233   score: 2.0   memory length: 228626   epsilon: 0.7453185400055289    steps: 198    lr: 4e-05     evaluation reward: 2.08\n",
      "episode: 1234   score: 2.0   memory length: 228807   epsilon: 0.7449601600055367    steps: 181    lr: 4e-05     evaluation reward: 2.06\n",
      "episode: 1235   score: 2.0   memory length: 229025   epsilon: 0.744528520005546    steps: 218    lr: 4e-05     evaluation reward: 2.07\n",
      "episode: 1236   score: 0.0   memory length: 229148   epsilon: 0.7442849800055513    steps: 123    lr: 4e-05     evaluation reward: 2.04\n",
      "episode: 1237   score: 3.0   memory length: 229396   epsilon: 0.743793940005562    steps: 248    lr: 4e-05     evaluation reward: 2.04\n",
      "episode: 1238   score: 1.0   memory length: 229548   epsilon: 0.7434929800055685    steps: 152    lr: 4e-05     evaluation reward: 2.05\n",
      "episode: 1239   score: 2.0   memory length: 229729   epsilon: 0.7431346000055763    steps: 181    lr: 4e-05     evaluation reward: 2.03\n",
      "episode: 1240   score: 7.0   memory length: 230098   epsilon: 0.7424039800055922    steps: 369    lr: 4e-05     evaluation reward: 2.1\n",
      "episode: 1241   score: 6.0   memory length: 230424   epsilon: 0.7417585000056062    steps: 326    lr: 4e-05     evaluation reward: 2.15\n",
      "episode: 1242   score: 2.0   memory length: 230623   epsilon: 0.7413644800056147    steps: 199    lr: 4e-05     evaluation reward: 2.16\n",
      "episode: 1243   score: 1.0   memory length: 230774   epsilon: 0.7410655000056212    steps: 151    lr: 4e-05     evaluation reward: 2.16\n",
      "episode: 1244   score: 3.0   memory length: 231021   epsilon: 0.7405764400056318    steps: 247    lr: 4e-05     evaluation reward: 2.17\n",
      "episode: 1245   score: 6.0   memory length: 231364   epsilon: 0.7398973000056466    steps: 343    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1246   score: 3.0   memory length: 231577   epsilon: 0.7394755600056557    steps: 213    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1247   score: 3.0   memory length: 231828   epsilon: 0.7389785800056665    steps: 251    lr: 4e-05     evaluation reward: 2.21\n",
      "episode: 1248   score: 3.0   memory length: 232056   epsilon: 0.7385271400056763    steps: 228    lr: 4e-05     evaluation reward: 2.2\n",
      "episode: 1249   score: 3.0   memory length: 232303   epsilon: 0.7380380800056869    steps: 247    lr: 4e-05     evaluation reward: 2.22\n",
      "episode: 1250   score: 5.0   memory length: 232629   epsilon: 0.737392600005701    steps: 326    lr: 4e-05     evaluation reward: 2.26\n",
      "episode: 1251   score: 2.0   memory length: 232826   epsilon: 0.7370025400057094    steps: 197    lr: 4e-05     evaluation reward: 2.28\n",
      "episode: 1252   score: 6.0   memory length: 233182   epsilon: 0.7362976600057247    steps: 356    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1253   score: 2.0   memory length: 233401   epsilon: 0.7358640400057341    steps: 219    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1254   score: 3.0   memory length: 233646   epsilon: 0.7353789400057447    steps: 245    lr: 4e-05     evaluation reward: 2.31\n",
      "episode: 1255   score: 2.0   memory length: 233843   epsilon: 0.7349888800057531    steps: 197    lr: 4e-05     evaluation reward: 2.32\n",
      "episode: 1256   score: 5.0   memory length: 234159   epsilon: 0.7343632000057667    steps: 316    lr: 4e-05     evaluation reward: 2.37\n",
      "episode: 1257   score: 0.0   memory length: 234281   epsilon: 0.734121640005772    steps: 122    lr: 4e-05     evaluation reward: 2.35\n",
      "episode: 1258   score: 6.0   memory length: 234658   epsilon: 0.7333751800057882    steps: 377    lr: 4e-05     evaluation reward: 2.38\n",
      "episode: 1259   score: 3.0   memory length: 234926   epsilon: 0.7328445400057997    steps: 268    lr: 4e-05     evaluation reward: 2.38\n",
      "episode: 1260   score: 2.0   memory length: 235146   epsilon: 0.7324089400058091    steps: 220    lr: 4e-05     evaluation reward: 2.4\n",
      "episode: 1261   score: 4.0   memory length: 235403   epsilon: 0.7319000800058202    steps: 257    lr: 4e-05     evaluation reward: 2.44\n",
      "episode: 1262   score: 0.0   memory length: 235526   epsilon: 0.7316565400058255    steps: 123    lr: 4e-05     evaluation reward: 2.44\n",
      "episode: 1263   score: 1.0   memory length: 235695   epsilon: 0.7313219200058327    steps: 169    lr: 4e-05     evaluation reward: 2.45\n",
      "episode: 1264   score: 2.0   memory length: 235892   epsilon: 0.7309318600058412    steps: 197    lr: 4e-05     evaluation reward: 2.45\n",
      "episode: 1265   score: 1.0   memory length: 236060   epsilon: 0.7305992200058484    steps: 168    lr: 4e-05     evaluation reward: 2.45\n",
      "episode: 1266   score: 4.0   memory length: 236357   epsilon: 0.7300111600058612    steps: 297    lr: 4e-05     evaluation reward: 2.48\n",
      "episode: 1267   score: 1.0   memory length: 236508   epsilon: 0.7297121800058677    steps: 151    lr: 4e-05     evaluation reward: 2.49\n",
      "episode: 1268   score: 5.0   memory length: 236815   epsilon: 0.7291043200058809    steps: 307    lr: 4e-05     evaluation reward: 2.51\n",
      "episode: 1269   score: 5.0   memory length: 237108   epsilon: 0.7285241800058935    steps: 293    lr: 4e-05     evaluation reward: 2.53\n",
      "episode: 1270   score: 2.0   memory length: 237327   epsilon: 0.7280905600059029    steps: 219    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1271   score: 1.0   memory length: 237478   epsilon: 0.7277915800059094    steps: 151    lr: 4e-05     evaluation reward: 2.52\n",
      "episode: 1272   score: 4.0   memory length: 237773   epsilon: 0.7272074800059221    steps: 295    lr: 4e-05     evaluation reward: 2.54\n",
      "episode: 1273   score: 3.0   memory length: 238001   epsilon: 0.7267560400059319    steps: 228    lr: 4e-05     evaluation reward: 2.56\n",
      "episode: 1274   score: 3.0   memory length: 238248   epsilon: 0.7262669800059425    steps: 247    lr: 4e-05     evaluation reward: 2.57\n",
      "episode: 1275   score: 2.0   memory length: 238466   epsilon: 0.7258353400059518    steps: 218    lr: 4e-05     evaluation reward: 2.56\n",
      "episode: 1276   score: 3.0   memory length: 238697   epsilon: 0.7253779600059618    steps: 231    lr: 4e-05     evaluation reward: 2.57\n",
      "episode: 1277   score: 1.0   memory length: 238848   epsilon: 0.7250789800059683    steps: 151    lr: 4e-05     evaluation reward: 2.55\n",
      "episode: 1278   score: 6.0   memory length: 239238   epsilon: 0.724306780005985    steps: 390    lr: 4e-05     evaluation reward: 2.6\n",
      "episode: 1279   score: 0.0   memory length: 239360   epsilon: 0.7240652200059903    steps: 122    lr: 4e-05     evaluation reward: 2.56\n",
      "episode: 1280   score: 4.0   memory length: 239613   epsilon: 0.7235642800060011    steps: 253    lr: 4e-05     evaluation reward: 2.59\n",
      "episode: 1281   score: 3.0   memory length: 239858   epsilon: 0.7230791800060117    steps: 245    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1282   score: 2.0   memory length: 240038   epsilon: 0.7227227800060194    steps: 180    lr: 4e-05     evaluation reward: 2.59\n",
      "episode: 1283   score: 2.0   memory length: 240238   epsilon: 0.722326780006028    steps: 200    lr: 4e-05     evaluation reward: 2.58\n",
      "episode: 1284   score: 4.0   memory length: 240555   epsilon: 0.7216991200060416    steps: 317    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1285   score: 2.0   memory length: 240755   epsilon: 0.7213031200060502    steps: 200    lr: 4e-05     evaluation reward: 2.62\n",
      "episode: 1286   score: 5.0   memory length: 241081   epsilon: 0.7206576400060642    steps: 326    lr: 4e-05     evaluation reward: 2.64\n",
      "episode: 1287   score: 5.0   memory length: 241404   epsilon: 0.7200181000060781    steps: 323    lr: 4e-05     evaluation reward: 2.67\n",
      "episode: 1288   score: 3.0   memory length: 241632   epsilon: 0.7195666600060879    steps: 228    lr: 4e-05     evaluation reward: 2.68\n",
      "episode: 1289   score: 4.0   memory length: 241930   epsilon: 0.7189766200061007    steps: 298    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1290   score: 1.0   memory length: 242081   epsilon: 0.7186776400061072    steps: 151    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1291   score: 3.0   memory length: 242294   epsilon: 0.7182559000061164    steps: 213    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1292   score: 4.0   memory length: 242611   epsilon: 0.71762824000613    steps: 317    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1293   score: 4.0   memory length: 242868   epsilon: 0.7171193800061411    steps: 257    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1294   score: 1.0   memory length: 243019   epsilon: 0.7168204000061476    steps: 151    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1295   score: 3.0   memory length: 243263   epsilon: 0.716337280006158    steps: 244    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1296   score: 5.0   memory length: 243573   epsilon: 0.7157234800061714    steps: 310    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1297   score: 1.0   memory length: 243723   epsilon: 0.7154264800061778    steps: 150    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1298   score: 4.0   memory length: 243997   epsilon: 0.7148839600061896    steps: 274    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1299   score: 4.0   memory length: 244272   epsilon: 0.7143394600062014    steps: 275    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1300   score: 2.0   memory length: 244472   epsilon: 0.71394346000621    steps: 200    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1301   score: 0.0   memory length: 244595   epsilon: 0.7136999200062153    steps: 123    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1302   score: 4.0   memory length: 244893   epsilon: 0.7131098800062281    steps: 298    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1303   score: 1.0   memory length: 245064   epsilon: 0.7127713000062355    steps: 171    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1304   score: 3.0   memory length: 245329   epsilon: 0.7122466000062468    steps: 265    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1305   score: 0.0   memory length: 245451   epsilon: 0.7120050400062521    steps: 122    lr: 4e-05     evaluation reward: 2.71\n",
      "episode: 1306   score: 2.0   memory length: 245631   epsilon: 0.7116486400062598    steps: 180    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1307   score: 1.0   memory length: 245781   epsilon: 0.7113516400062663    steps: 150    lr: 4e-05     evaluation reward: 2.69\n",
      "episode: 1308   score: 4.0   memory length: 246022   epsilon: 0.7108744600062766    steps: 241    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1309   score: 3.0   memory length: 246269   epsilon: 0.7103854000062872    steps: 247    lr: 4e-05     evaluation reward: 2.7\n",
      "episode: 1310   score: 5.0   memory length: 246541   epsilon: 0.7098468400062989    steps: 272    lr: 4e-05     evaluation reward: 2.74\n",
      "episode: 1311   score: 1.0   memory length: 246692   epsilon: 0.7095478600063054    steps: 151    lr: 4e-05     evaluation reward: 2.73\n",
      "episode: 1312   score: 3.0   memory length: 246918   epsilon: 0.7091003800063151    steps: 226    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1313   score: 2.0   memory length: 247098   epsilon: 0.7087439800063229    steps: 180    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1314   score: 2.0   memory length: 247296   epsilon: 0.7083519400063314    steps: 198    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1315   score: 3.0   memory length: 247562   epsilon: 0.7078252600063428    steps: 266    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1316   score: 1.0   memory length: 247713   epsilon: 0.7075262800063493    steps: 151    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1317   score: 4.0   memory length: 247964   epsilon: 0.7070293000063601    steps: 251    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1318   score: 2.0   memory length: 248144   epsilon: 0.7066729000063678    steps: 180    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1319   score: 3.0   memory length: 248369   epsilon: 0.7062274000063775    steps: 225    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1320   score: 3.0   memory length: 248579   epsilon: 0.7058116000063865    steps: 210    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1321   score: 2.0   memory length: 248759   epsilon: 0.7054552000063943    steps: 180    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1322   score: 1.0   memory length: 248909   epsilon: 0.7051582000064007    steps: 150    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1323   score: 3.0   memory length: 249135   epsilon: 0.7047107200064104    steps: 226    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1324   score: 3.0   memory length: 249364   epsilon: 0.7042573000064203    steps: 229    lr: 4e-05     evaluation reward: 2.72\n",
      "episode: 1325   score: 6.0   memory length: 249719   epsilon: 0.7035544000064355    steps: 355    lr: 4e-05     evaluation reward: 2.77\n",
      "episode: 1326   score: 2.0   memory length: 249900   epsilon: 0.7031960200064433    steps: 181    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1327   score: 1.0   memory length: 250051   epsilon: 0.7028970400064498    steps: 151    lr: 4e-05     evaluation reward: 2.76\n",
      "episode: 1328   score: 2.0   memory length: 250269   epsilon: 0.7024654000064592    steps: 218    lr: 4e-05     evaluation reward: 2.75\n",
      "episode: 1329   score: 4.0   memory length: 250559   epsilon: 0.7018912000064716    steps: 290    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1330   score: 2.0   memory length: 250740   epsilon: 0.7015328200064794    steps: 181    lr: 4e-05     evaluation reward: 2.78\n",
      "episode: 1331   score: 7.0   memory length: 251170   epsilon: 0.7006814200064979    steps: 430    lr: 4e-05     evaluation reward: 2.82\n",
      "episode: 1332   score: 5.0   memory length: 251533   epsilon: 0.6999626800065135    steps: 363    lr: 4e-05     evaluation reward: 2.84\n",
      "episode: 1333   score: 4.0   memory length: 251793   epsilon: 0.6994478800065247    steps: 260    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1334   score: 3.0   memory length: 252039   epsilon: 0.6989608000065353    steps: 246    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1335   score: 2.0   memory length: 252257   epsilon: 0.6985291600065446    steps: 218    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1336   score: 4.0   memory length: 252531   epsilon: 0.6979866400065564    steps: 274    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1337   score: 5.0   memory length: 252876   epsilon: 0.6973035400065712    steps: 345    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1338   score: 3.0   memory length: 253122   epsilon: 0.6968164600065818    steps: 246    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1339   score: 6.0   memory length: 253498   epsilon: 0.696071980006598    steps: 376    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1340   score: 1.0   memory length: 253667   epsilon: 0.6957373600066052    steps: 169    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1341   score: 5.0   memory length: 253995   epsilon: 0.6950879200066193    steps: 328    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1342   score: 3.0   memory length: 254224   epsilon: 0.6946345000066292    steps: 229    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1343   score: 3.0   memory length: 254472   epsilon: 0.6941434600066398    steps: 248    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1344   score: 4.0   memory length: 254729   epsilon: 0.6936346000066509    steps: 257    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1345   score: 1.0   memory length: 254898   epsilon: 0.6932999800066582    steps: 169    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1346   score: 2.0   memory length: 255080   epsilon: 0.692939620006666    steps: 182    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1347   score: 5.0   memory length: 255410   epsilon: 0.6922862200066802    steps: 330    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1348   score: 1.0   memory length: 255561   epsilon: 0.6919872400066867    steps: 151    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1349   score: 2.0   memory length: 255760   epsilon: 0.6915932200066952    steps: 199    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1350   score: 2.0   memory length: 255942   epsilon: 0.691232860006703    steps: 182    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1351   score: 4.0   memory length: 256235   epsilon: 0.6906527200067156    steps: 293    lr: 4e-05     evaluation reward: 2.88\n",
      "episode: 1352   score: 4.0   memory length: 256528   epsilon: 0.6900725800067282    steps: 293    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1353   score: 2.0   memory length: 256728   epsilon: 0.6896765800067368    steps: 200    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1354   score: 3.0   memory length: 256941   epsilon: 0.689254840006746    steps: 213    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1355   score: 3.0   memory length: 257151   epsilon: 0.688839040006755    steps: 210    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1356   score: 3.0   memory length: 257377   epsilon: 0.6883915600067647    steps: 226    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1357   score: 2.0   memory length: 257557   epsilon: 0.6880351600067725    steps: 180    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1358   score: 5.0   memory length: 257895   epsilon: 0.687365920006787    steps: 338    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1359   score: 2.0   memory length: 258092   epsilon: 0.6869758600067954    steps: 197    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1360   score: 2.0   memory length: 258290   epsilon: 0.686583820006804    steps: 198    lr: 4e-05     evaluation reward: 2.85\n",
      "episode: 1361   score: 2.0   memory length: 258488   epsilon: 0.6861917800068125    steps: 198    lr: 4e-05     evaluation reward: 2.83\n",
      "episode: 1362   score: 3.0   memory length: 258717   epsilon: 0.6857383600068223    steps: 229    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1363   score: 4.0   memory length: 259012   epsilon: 0.685154260006835    steps: 295    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1364   score: 5.0   memory length: 259319   epsilon: 0.6845464000068482    steps: 307    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1365   score: 1.0   memory length: 259470   epsilon: 0.6842474200068547    steps: 151    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1366   score: 2.0   memory length: 259667   epsilon: 0.6838573600068631    steps: 197    lr: 4e-05     evaluation reward: 2.9\n",
      "episode: 1367   score: 3.0   memory length: 259892   epsilon: 0.6834118600068728    steps: 225    lr: 4e-05     evaluation reward: 2.92\n",
      "episode: 1368   score: 2.0   memory length: 260092   epsilon: 0.6830158600068814    steps: 200    lr: 4e-05     evaluation reward: 2.89\n",
      "episode: 1369   score: 2.0   memory length: 260290   epsilon: 0.6826238200068899    steps: 198    lr: 4e-05     evaluation reward: 2.86\n",
      "episode: 1370   score: 3.0   memory length: 260536   epsilon: 0.6821367400069005    steps: 246    lr: 4e-05     evaluation reward: 2.87\n",
      "episode: 1371   score: 8.0   memory length: 260966   epsilon: 0.681285340006919    steps: 430    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1372   score: 4.0   memory length: 261243   epsilon: 0.6807368800069309    steps: 277    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1373   score: 4.0   memory length: 261500   epsilon: 0.6802280200069419    steps: 257    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1374   score: 3.0   memory length: 261750   epsilon: 0.6797330200069527    steps: 250    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1375   score: 1.0   memory length: 261921   epsilon: 0.67939444000696    steps: 171    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1376   score: 5.0   memory length: 262233   epsilon: 0.6787766800069734    steps: 312    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1377   score: 2.0   memory length: 262430   epsilon: 0.6783866200069819    steps: 197    lr: 4e-05     evaluation reward: 2.97\n",
      "episode: 1378   score: 5.0   memory length: 262757   epsilon: 0.677739160006996    steps: 327    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1379   score: 2.0   memory length: 262939   epsilon: 0.6773788000070038    steps: 182    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1380   score: 2.0   memory length: 263137   epsilon: 0.6769867600070123    steps: 198    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1381   score: 3.0   memory length: 263365   epsilon: 0.6765353200070221    steps: 228    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1382   score: 6.0   memory length: 263739   epsilon: 0.6757948000070382    steps: 374    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1383   score: 4.0   memory length: 264034   epsilon: 0.6752107000070509    steps: 295    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1384   score: 3.0   memory length: 264262   epsilon: 0.6747592600070607    steps: 228    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1385   score: 2.0   memory length: 264478   epsilon: 0.6743315800070699    steps: 216    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1386   score: 2.0   memory length: 264659   epsilon: 0.6739732000070777    steps: 181    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1387   score: 1.0   memory length: 264809   epsilon: 0.6736762000070842    steps: 150    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1388   score: 3.0   memory length: 265058   epsilon: 0.6731831800070949    steps: 249    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1389   score: 3.0   memory length: 265304   epsilon: 0.6726961000071054    steps: 246    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1390   score: 1.0   memory length: 265455   epsilon: 0.6723971200071119    steps: 151    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1391   score: 4.0   memory length: 265769   epsilon: 0.6717754000071254    steps: 314    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1392   score: 3.0   memory length: 265996   epsilon: 0.6713259400071352    steps: 227    lr: 4e-05     evaluation reward: 2.93\n",
      "episode: 1393   score: 2.0   memory length: 266178   epsilon: 0.670965580007143    steps: 182    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1394   score: 5.0   memory length: 266505   epsilon: 0.6703181200071571    steps: 327    lr: 4e-05     evaluation reward: 2.95\n",
      "episode: 1395   score: 2.0   memory length: 266702   epsilon: 0.6699280600071655    steps: 197    lr: 4e-05     evaluation reward: 2.94\n",
      "episode: 1396   score: 2.0   memory length: 266918   epsilon: 0.6695003800071748    steps: 216    lr: 4e-05     evaluation reward: 2.91\n",
      "episode: 1397   score: 6.0   memory length: 267279   epsilon: 0.6687856000071903    steps: 361    lr: 4e-05     evaluation reward: 2.96\n",
      "episode: 1398   score: 8.0   memory length: 267575   epsilon: 0.6681995200072031    steps: 296    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1399   score: 4.0   memory length: 267849   epsilon: 0.6676570000072148    steps: 274    lr: 4e-05     evaluation reward: 3.0\n",
      "episode: 1400   score: 4.0   memory length: 268124   epsilon: 0.6671125000072267    steps: 275    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1401   score: 2.0   memory length: 268322   epsilon: 0.6667204600072352    steps: 198    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1402   score: 1.0   memory length: 268491   epsilon: 0.6663858400072424    steps: 169    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1403   score: 2.0   memory length: 268689   epsilon: 0.665993800007251    steps: 198    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1404   score: 2.0   memory length: 268887   epsilon: 0.6656017600072595    steps: 198    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1405   score: 1.0   memory length: 269038   epsilon: 0.665302780007266    steps: 151    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1406   score: 4.0   memory length: 269295   epsilon: 0.664793920007277    steps: 257    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1407   score: 4.0   memory length: 269592   epsilon: 0.6642058600072898    steps: 297    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1408   score: 3.0   memory length: 269839   epsilon: 0.6637168000073004    steps: 247    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1409   score: 4.0   memory length: 270114   epsilon: 0.6631723000073122    steps: 275    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1410   score: 3.0   memory length: 270325   epsilon: 0.6627545200073213    steps: 211    lr: 4e-05     evaluation reward: 3.05\n",
      "episode: 1411   score: 5.0   memory length: 270632   epsilon: 0.6621466600073345    steps: 307    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1412   score: 2.0   memory length: 270847   epsilon: 0.6617209600073437    steps: 215    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1413   score: 3.0   memory length: 271093   epsilon: 0.6612338800073543    steps: 246    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1414   score: 0.0   memory length: 271216   epsilon: 0.6609903400073596    steps: 123    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1415   score: 4.0   memory length: 271471   epsilon: 0.6604854400073705    steps: 255    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1416   score: 3.0   memory length: 271697   epsilon: 0.6600379600073802    steps: 226    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1417   score: 0.0   memory length: 271820   epsilon: 0.6597944200073855    steps: 123    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1418   score: 4.0   memory length: 272098   epsilon: 0.6592439800073975    steps: 278    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1419   score: 4.0   memory length: 272375   epsilon: 0.6586955200074094    steps: 277    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1420   score: 5.0   memory length: 272704   epsilon: 0.6580441000074235    steps: 329    lr: 4e-05     evaluation reward: 3.11\n",
      "episode: 1421   score: 3.0   memory length: 272933   epsilon: 0.6575906800074334    steps: 229    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1422   score: 6.0   memory length: 273297   epsilon: 0.656869960007449    steps: 364    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1423   score: 3.0   memory length: 273523   epsilon: 0.6564224800074587    steps: 226    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1424   score: 2.0   memory length: 273721   epsilon: 0.6560304400074672    steps: 198    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1425   score: 5.0   memory length: 274029   epsilon: 0.6554206000074805    steps: 308    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1426   score: 5.0   memory length: 274352   epsilon: 0.6547810600074944    steps: 323    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1427   score: 1.0   memory length: 274502   epsilon: 0.6544840600075008    steps: 150    lr: 4e-05     evaluation reward: 3.18\n",
      "episode: 1428   score: 4.0   memory length: 274757   epsilon: 0.6539791600075118    steps: 255    lr: 4e-05     evaluation reward: 3.2\n",
      "episode: 1429   score: 1.0   memory length: 274907   epsilon: 0.6536821600075182    steps: 150    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1430   score: 1.0   memory length: 275058   epsilon: 0.6533831800075247    steps: 151    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1431   score: 3.0   memory length: 275286   epsilon: 0.6529317400075345    steps: 228    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1432   score: 3.0   memory length: 275518   epsilon: 0.6524723800075445    steps: 232    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1433   score: 2.0   memory length: 275717   epsilon: 0.652078360007553    steps: 199    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1434   score: 3.0   memory length: 275947   epsilon: 0.6516229600075629    steps: 230    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1435   score: 3.0   memory length: 276212   epsilon: 0.6510982600075743    steps: 265    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1436   score: 3.0   memory length: 276457   epsilon: 0.6506131600075848    steps: 245    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1437   score: 4.0   memory length: 276730   epsilon: 0.6500726200075966    steps: 273    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1438   score: 5.0   memory length: 277039   epsilon: 0.6494608000076099    steps: 309    lr: 4e-05     evaluation reward: 3.09\n",
      "episode: 1439   score: 1.0   memory length: 277190   epsilon: 0.6491618200076164    steps: 151    lr: 4e-05     evaluation reward: 3.04\n",
      "episode: 1440   score: 4.0   memory length: 277466   epsilon: 0.6486153400076282    steps: 276    lr: 4e-05     evaluation reward: 3.07\n",
      "episode: 1441   score: 4.0   memory length: 277763   epsilon: 0.648027280007641    steps: 297    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1442   score: 0.0   memory length: 277886   epsilon: 0.6477837400076463    steps: 123    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1443   score: 3.0   memory length: 278132   epsilon: 0.6472966600076568    steps: 246    lr: 4e-05     evaluation reward: 3.03\n",
      "episode: 1444   score: 0.0   memory length: 278255   epsilon: 0.6470531200076621    steps: 123    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1445   score: 3.0   memory length: 278484   epsilon: 0.646599700007672    steps: 229    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1446   score: 0.0   memory length: 278606   epsilon: 0.6463581400076772    steps: 122    lr: 4e-05     evaluation reward: 2.99\n",
      "episode: 1447   score: 4.0   memory length: 278881   epsilon: 0.645813640007689    steps: 275    lr: 4e-05     evaluation reward: 2.98\n",
      "episode: 1448   score: 4.0   memory length: 279195   epsilon: 0.6451919200077025    steps: 314    lr: 4e-05     evaluation reward: 3.01\n",
      "episode: 1449   score: 3.0   memory length: 279424   epsilon: 0.6447385000077124    steps: 229    lr: 4e-05     evaluation reward: 3.02\n",
      "episode: 1450   score: 6.0   memory length: 279812   epsilon: 0.6439702600077291    steps: 388    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1451   score: 6.0   memory length: 280167   epsilon: 0.6432673600077443    steps: 355    lr: 4e-05     evaluation reward: 3.08\n",
      "episode: 1452   score: 2.0   memory length: 280349   epsilon: 0.6429070000077521    steps: 182    lr: 4e-05     evaluation reward: 3.06\n",
      "episode: 1453   score: 6.0   memory length: 280739   epsilon: 0.6421348000077689    steps: 390    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1454   score: 3.0   memory length: 281004   epsilon: 0.6416101000077803    steps: 265    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1455   score: 5.0   memory length: 281325   epsilon: 0.6409745200077941    steps: 321    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1456   score: 1.0   memory length: 281494   epsilon: 0.6406399000078014    steps: 169    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1457   score: 4.0   memory length: 281735   epsilon: 0.6401627200078117    steps: 241    lr: 4e-05     evaluation reward: 3.12\n",
      "episode: 1458   score: 3.0   memory length: 281965   epsilon: 0.6397073200078216    steps: 230    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1459   score: 2.0   memory length: 282150   epsilon: 0.6393410200078296    steps: 185    lr: 4e-05     evaluation reward: 3.1\n",
      "episode: 1460   score: 5.0   memory length: 282471   epsilon: 0.6387054400078434    steps: 321    lr: 4e-05     evaluation reward: 3.13\n",
      "episode: 1461   score: 5.0   memory length: 282786   epsilon: 0.6380817400078569    steps: 315    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1462   score: 3.0   memory length: 283030   epsilon: 0.6375986200078674    steps: 244    lr: 4e-05     evaluation reward: 3.16\n",
      "episode: 1463   score: 5.0   memory length: 283332   epsilon: 0.6370006600078804    steps: 302    lr: 4e-05     evaluation reward: 3.17\n",
      "episode: 1464   score: 3.0   memory length: 283543   epsilon: 0.6365828800078894    steps: 211    lr: 4e-05     evaluation reward: 3.15\n",
      "episode: 1465   score: 5.0   memory length: 283831   epsilon: 0.6360126400079018    steps: 288    lr: 4e-05     evaluation reward: 3.19\n",
      "episode: 1466   score: 5.0   memory length: 284130   epsilon: 0.6354206200079147    steps: 299    lr: 4e-05     evaluation reward: 3.22\n",
      "episode: 1467   score: 2.0   memory length: 284330   epsilon: 0.6350246200079233    steps: 200    lr: 4e-05     evaluation reward: 3.21\n",
      "episode: 1468   score: 4.0   memory length: 284590   epsilon: 0.6345098200079344    steps: 260    lr: 4e-05     evaluation reward: 3.23\n",
      "episode: 1469   score: 4.0   memory length: 284847   epsilon: 0.6340009600079455    steps: 257    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1470   score: 6.0   memory length: 285205   epsilon: 0.6332921200079609    steps: 358    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1471   score: 8.0   memory length: 285677   epsilon: 0.6323575600079812    steps: 472    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1472   score: 2.0   memory length: 285859   epsilon: 0.631997200007989    steps: 182    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1473   score: 4.0   memory length: 286135   epsilon: 0.6314507200080008    steps: 276    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1474   score: 3.0   memory length: 286364   epsilon: 0.6309973000080107    steps: 229    lr: 4e-05     evaluation reward: 3.26\n",
      "episode: 1475   score: 4.0   memory length: 286637   epsilon: 0.6304567600080224    steps: 273    lr: 4e-05     evaluation reward: 3.29\n",
      "episode: 1476   score: 3.0   memory length: 286863   epsilon: 0.6300092800080321    steps: 226    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1477   score: 0.0   memory length: 286985   epsilon: 0.6297677200080374    steps: 122    lr: 4e-05     evaluation reward: 3.25\n",
      "episode: 1478   score: 7.0   memory length: 287366   epsilon: 0.6290133400080538    steps: 381    lr: 4e-05     evaluation reward: 3.27\n",
      "episode: 1479   score: 3.0   memory length: 287576   epsilon: 0.6285975400080628    steps: 210    lr: 4e-05     evaluation reward: 3.28\n",
      "episode: 1480   score: 8.0   memory length: 288012   epsilon: 0.6277342600080815    steps: 436    lr: 4e-05     evaluation reward: 3.34\n",
      "episode: 1481   score: 2.0   memory length: 288210   epsilon: 0.62734222000809    steps: 198    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1482   score: 5.0   memory length: 288555   epsilon: 0.6266591200081049    steps: 345    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1483   score: 2.0   memory length: 288735   epsilon: 0.6263027200081126    steps: 180    lr: 4e-05     evaluation reward: 3.3\n",
      "episode: 1484   score: 6.0   memory length: 289058   epsilon: 0.6256631800081265    steps: 323    lr: 4e-05     evaluation reward: 3.33\n",
      "episode: 1485   score: 1.0   memory length: 289227   epsilon: 0.6253285600081337    steps: 169    lr: 4e-05     evaluation reward: 3.32\n",
      "episode: 1486   score: 5.0   memory length: 289531   epsilon: 0.6247266400081468    steps: 304    lr: 4e-05     evaluation reward: 3.35\n",
      "episode: 1487   score: 7.0   memory length: 289954   epsilon: 0.623889100008165    steps: 423    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1488   score: 3.0   memory length: 290182   epsilon: 0.6234376600081748    steps: 228    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1489   score: 3.0   memory length: 290411   epsilon: 0.6229842400081846    steps: 229    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1490   score: 3.0   memory length: 290655   epsilon: 0.6225011200081951    steps: 244    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1491   score: 4.0   memory length: 290909   epsilon: 0.621998200008206    steps: 254    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1492   score: 4.0   memory length: 291209   epsilon: 0.6214042000082189    steps: 300    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1493   score: 3.0   memory length: 291437   epsilon: 0.6209527600082287    steps: 228    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1494   score: 6.0   memory length: 291759   epsilon: 0.6203152000082426    steps: 322    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1495   score: 3.0   memory length: 291987   epsilon: 0.6198637600082524    steps: 228    lr: 4e-05     evaluation reward: 3.47\n",
      "episode: 1496   score: 4.0   memory length: 292262   epsilon: 0.6193192600082642    steps: 275    lr: 4e-05     evaluation reward: 3.49\n",
      "episode: 1497   score: 2.0   memory length: 292460   epsilon: 0.6189272200082727    steps: 198    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1498   score: 5.0   memory length: 292785   epsilon: 0.6182837200082867    steps: 325    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1499   score: 3.0   memory length: 293032   epsilon: 0.6177946600082973    steps: 247    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1500   score: 1.0   memory length: 293182   epsilon: 0.6174976600083038    steps: 150    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1501   score: 3.0   memory length: 293410   epsilon: 0.6170462200083136    steps: 228    lr: 4e-05     evaluation reward: 3.39\n",
      "episode: 1502   score: 2.0   memory length: 293608   epsilon: 0.6166541800083221    steps: 198    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1503   score: 2.0   memory length: 293807   epsilon: 0.6162601600083306    steps: 199    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1504   score: 5.0   memory length: 294130   epsilon: 0.6156206200083445    steps: 323    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1505   score: 1.0   memory length: 294280   epsilon: 0.615323620008351    steps: 150    lr: 4e-05     evaluation reward: 3.43\n",
      "episode: 1506   score: 2.0   memory length: 294477   epsilon: 0.6149335600083594    steps: 197    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1507   score: 3.0   memory length: 294723   epsilon: 0.61444648000837    steps: 246    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1508   score: 5.0   memory length: 295030   epsilon: 0.6138386200083832    steps: 307    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1509   score: 4.0   memory length: 295271   epsilon: 0.6133614400083935    steps: 241    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1510   score: 1.0   memory length: 295422   epsilon: 0.6130624600084    steps: 151    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1511   score: 3.0   memory length: 295631   epsilon: 0.612648640008409    steps: 209    lr: 4e-05     evaluation reward: 3.38\n",
      "episode: 1512   score: 5.0   memory length: 295946   epsilon: 0.6120249400084226    steps: 315    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1513   score: 4.0   memory length: 296219   epsilon: 0.6114844000084343    steps: 273    lr: 4e-05     evaluation reward: 3.42\n",
      "episode: 1514   score: 2.0   memory length: 296417   epsilon: 0.6110923600084428    steps: 198    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1515   score: 0.0   memory length: 296540   epsilon: 0.6108488200084481    steps: 123    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1516   score: 4.0   memory length: 296836   epsilon: 0.6102627400084608    steps: 296    lr: 4e-05     evaluation reward: 3.41\n",
      "episode: 1517   score: 5.0   memory length: 297133   epsilon: 0.6096746800084736    steps: 297    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1518   score: 4.0   memory length: 297431   epsilon: 0.6090846400084864    steps: 298    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1519   score: 4.0   memory length: 297687   epsilon: 0.6085777600084974    steps: 256    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1520   score: 3.0   memory length: 297917   epsilon: 0.6081223600085073    steps: 230    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1521   score: 4.0   memory length: 298176   epsilon: 0.6076095400085184    steps: 259    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1522   score: 1.0   memory length: 298344   epsilon: 0.6072769000085256    steps: 168    lr: 4e-05     evaluation reward: 3.4\n",
      "episode: 1523   score: 8.0   memory length: 298787   epsilon: 0.6063997600085447    steps: 443    lr: 4e-05     evaluation reward: 3.45\n",
      "episode: 1524   score: 3.0   memory length: 299036   epsilon: 0.6059067400085554    steps: 249    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1525   score: 5.0   memory length: 299346   epsilon: 0.6052929400085687    steps: 310    lr: 4e-05     evaluation reward: 3.46\n",
      "episode: 1526   score: 3.0   memory length: 299557   epsilon: 0.6048751600085778    steps: 211    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1527   score: 1.0   memory length: 299708   epsilon: 0.6045761800085843    steps: 151    lr: 4e-05     evaluation reward: 3.44\n",
      "episode: 1528   score: 7.0   memory length: 300120   epsilon: 0.603760420008602    steps: 412    lr: 1.6000000000000003e-05     evaluation reward: 3.47\n",
      "episode: 1529   score: 2.0   memory length: 300337   epsilon: 0.6033307600086113    steps: 217    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1530   score: 1.0   memory length: 300509   epsilon: 0.6029902000086187    steps: 172    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1531   score: 3.0   memory length: 300739   epsilon: 0.6025348000086286    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1532   score: 5.0   memory length: 301051   epsilon: 0.601917040008642    steps: 312    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1533   score: 3.0   memory length: 301279   epsilon: 0.6014656000086518    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1534   score: 4.0   memory length: 301555   epsilon: 0.6009191200086637    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1535   score: 3.0   memory length: 301800   epsilon: 0.6004340200086742    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1536   score: 3.0   memory length: 302026   epsilon: 0.5999865400086839    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1537   score: 2.0   memory length: 302243   epsilon: 0.5995568800086932    steps: 217    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1538   score: 2.0   memory length: 302440   epsilon: 0.5991668200087017    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 3.47\n",
      "episode: 1539   score: 2.0   memory length: 302619   epsilon: 0.5988124000087094    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1540   score: 4.0   memory length: 302882   epsilon: 0.5982916600087207    steps: 263    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1541   score: 5.0   memory length: 303191   epsilon: 0.597679840008734    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1542   score: 2.0   memory length: 303371   epsilon: 0.5973234400087417    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 3.51\n",
      "episode: 1543   score: 1.0   memory length: 303522   epsilon: 0.5970244600087482    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1544   score: 3.0   memory length: 303734   epsilon: 0.5966047000087573    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1545   score: 1.0   memory length: 303886   epsilon: 0.5963037400087638    steps: 152    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1546   score: 0.0   memory length: 304008   epsilon: 0.5960621800087691    steps: 122    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1547   score: 0.0   memory length: 304131   epsilon: 0.5958186400087744    steps: 123    lr: 1.6000000000000003e-05     evaluation reward: 3.46\n",
      "episode: 1548   score: 3.0   memory length: 304359   epsilon: 0.5953672000087842    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.45\n",
      "episode: 1549   score: 0.0   memory length: 304481   epsilon: 0.5951256400087894    steps: 122    lr: 1.6000000000000003e-05     evaluation reward: 3.42\n",
      "episode: 1550   score: 4.0   memory length: 304757   epsilon: 0.5945791600088013    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.4\n",
      "episode: 1551   score: 1.0   memory length: 304907   epsilon: 0.5942821600088077    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1552   score: 10.0   memory length: 305430   epsilon: 0.5932466200088302    steps: 523    lr: 1.6000000000000003e-05     evaluation reward: 3.43\n",
      "episode: 1553   score: 0.0   memory length: 305552   epsilon: 0.5930050600088355    steps: 122    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1554   score: 3.0   memory length: 305782   epsilon: 0.5925496600088453    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1555   score: 2.0   memory length: 305961   epsilon: 0.592195240008853    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1556   score: 2.0   memory length: 306159   epsilon: 0.5918032000088616    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1557   score: 6.0   memory length: 306514   epsilon: 0.5911003000088768    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1558   score: 3.0   memory length: 306760   epsilon: 0.5906132200088874    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1559   score: 3.0   memory length: 307007   epsilon: 0.590124160008898    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.38\n",
      "episode: 1560   score: 6.0   memory length: 307335   epsilon: 0.5894747200089121    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 3.39\n",
      "episode: 1561   score: 3.0   memory length: 307564   epsilon: 0.5890213000089219    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1562   score: 2.0   memory length: 307744   epsilon: 0.5886649000089297    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 3.36\n",
      "episode: 1563   score: 6.0   memory length: 308099   epsilon: 0.5879620000089449    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1564   score: 4.0   memory length: 308357   epsilon: 0.587451160008956    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 3.38\n",
      "episode: 1565   score: 3.0   memory length: 308570   epsilon: 0.5870294200089652    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.36\n",
      "episode: 1566   score: 4.0   memory length: 308846   epsilon: 0.586482940008977    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1567   score: 4.0   memory length: 309143   epsilon: 0.5858948800089898    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1568   score: 1.0   memory length: 309312   epsilon: 0.5855602600089971    steps: 169    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1569   score: 3.0   memory length: 309541   epsilon: 0.5851068400090069    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1570   score: 2.0   memory length: 309757   epsilon: 0.5846791600090162    steps: 216    lr: 1.6000000000000003e-05     evaluation reward: 3.29\n",
      "episode: 1571   score: 2.0   memory length: 309937   epsilon: 0.584322760009024    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 3.23\n",
      "episode: 1572   score: 3.0   memory length: 310166   epsilon: 0.5838693400090338    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.24\n",
      "episode: 1573   score: 5.0   memory length: 310468   epsilon: 0.5832713800090468    steps: 302    lr: 1.6000000000000003e-05     evaluation reward: 3.25\n",
      "episode: 1574   score: 1.0   memory length: 310618   epsilon: 0.5829743800090532    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.23\n",
      "episode: 1575   score: 9.0   memory length: 311074   epsilon: 0.5820715000090728    steps: 456    lr: 1.6000000000000003e-05     evaluation reward: 3.28\n",
      "episode: 1576   score: 3.0   memory length: 311340   epsilon: 0.5815448200090843    steps: 266    lr: 1.6000000000000003e-05     evaluation reward: 3.28\n",
      "episode: 1577   score: 5.0   memory length: 311666   epsilon: 0.5808993400090983    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1578   score: 2.0   memory length: 311847   epsilon: 0.580540960009106    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.28\n",
      "episode: 1579   score: 6.0   memory length: 312207   epsilon: 0.5798281600091215    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1580   score: 5.0   memory length: 312515   epsilon: 0.5792183200091348    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 3.28\n",
      "episode: 1581   score: 3.0   memory length: 312729   epsilon: 0.578794600009144    steps: 214    lr: 1.6000000000000003e-05     evaluation reward: 3.29\n",
      "episode: 1582   score: 3.0   memory length: 312942   epsilon: 0.5783728600091531    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.27\n",
      "episode: 1583   score: 3.0   memory length: 313172   epsilon: 0.577917460009163    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.28\n",
      "episode: 1584   score: 2.0   memory length: 313370   epsilon: 0.5775254200091715    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.24\n",
      "episode: 1585   score: 3.0   memory length: 313580   epsilon: 0.5771096200091805    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 3.26\n",
      "episode: 1586   score: 0.0   memory length: 313703   epsilon: 0.5768660800091858    steps: 123    lr: 1.6000000000000003e-05     evaluation reward: 3.21\n",
      "episode: 1587   score: 7.0   memory length: 314088   epsilon: 0.5761037800092024    steps: 385    lr: 1.6000000000000003e-05     evaluation reward: 3.21\n",
      "episode: 1588   score: 5.0   memory length: 314397   epsilon: 0.5754919600092157    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 3.23\n",
      "episode: 1589   score: 5.0   memory length: 314699   epsilon: 0.5748940000092286    steps: 302    lr: 1.6000000000000003e-05     evaluation reward: 3.25\n",
      "episode: 1590   score: 2.0   memory length: 314917   epsilon: 0.574462360009238    steps: 218    lr: 1.6000000000000003e-05     evaluation reward: 3.24\n",
      "episode: 1591   score: 2.0   memory length: 315116   epsilon: 0.5740683400092466    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.22\n",
      "episode: 1592   score: 4.0   memory length: 315376   epsilon: 0.5735535400092577    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.22\n",
      "episode: 1593   score: 4.0   memory length: 315656   epsilon: 0.5729991400092698    steps: 280    lr: 1.6000000000000003e-05     evaluation reward: 3.23\n",
      "episode: 1594   score: 5.0   memory length: 315994   epsilon: 0.5723299000092843    steps: 338    lr: 1.6000000000000003e-05     evaluation reward: 3.22\n",
      "episode: 1595   score: 6.0   memory length: 316370   epsilon: 0.5715854200093005    steps: 376    lr: 1.6000000000000003e-05     evaluation reward: 3.25\n",
      "episode: 1596   score: 6.0   memory length: 316730   epsilon: 0.5708726200093159    steps: 360    lr: 1.6000000000000003e-05     evaluation reward: 3.27\n",
      "episode: 1597   score: 5.0   memory length: 317038   epsilon: 0.5702627800093292    steps: 308    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1598   score: 5.0   memory length: 317365   epsilon: 0.5696153200093432    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1599   score: 3.0   memory length: 317612   epsilon: 0.5691262600093538    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1600   score: 3.0   memory length: 317825   epsilon: 0.568704520009363    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1601   score: 3.0   memory length: 318072   epsilon: 0.5682154600093736    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1602   score: 2.0   memory length: 318251   epsilon: 0.5678610400093813    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1603   score: 1.0   memory length: 318401   epsilon: 0.5675640400093878    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1604   score: 5.0   memory length: 318697   epsilon: 0.5669779600094005    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1605   score: 2.0   memory length: 318898   epsilon: 0.5665799800094091    steps: 201    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1606   score: 1.0   memory length: 319070   epsilon: 0.5662394200094165    steps: 172    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1607   score: 3.0   memory length: 319301   epsilon: 0.5657820400094264    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1608   score: 2.0   memory length: 319483   epsilon: 0.5654216800094343    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.28\n",
      "episode: 1609   score: 1.0   memory length: 319634   epsilon: 0.5651227000094408    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.25\n",
      "episode: 1610   score: 4.0   memory length: 319895   epsilon: 0.564605920009452    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.28\n",
      "episode: 1611   score: 2.0   memory length: 320076   epsilon: 0.5642475400094598    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.27\n",
      "episode: 1612   score: 5.0   memory length: 320368   epsilon: 0.5636693800094723    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 3.27\n",
      "episode: 1613   score: 3.0   memory length: 320577   epsilon: 0.5632555600094813    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 3.26\n",
      "episode: 1614   score: 1.0   memory length: 320727   epsilon: 0.5629585600094877    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.25\n",
      "episode: 1615   score: 2.0   memory length: 320909   epsilon: 0.5625982000094956    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.27\n",
      "episode: 1616   score: 3.0   memory length: 321156   epsilon: 0.5621091400095062    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.26\n",
      "episode: 1617   score: 1.0   memory length: 321307   epsilon: 0.5618101600095127    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.22\n",
      "episode: 1618   score: 2.0   memory length: 321487   epsilon: 0.5614537600095204    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 3.2\n",
      "episode: 1619   score: 4.0   memory length: 321728   epsilon: 0.5609765800095308    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 3.2\n",
      "episode: 1620   score: 3.0   memory length: 321954   epsilon: 0.5605291000095405    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.2\n",
      "episode: 1621   score: 3.0   memory length: 322179   epsilon: 0.5600836000095502    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 3.19\n",
      "episode: 1622   score: 3.0   memory length: 322408   epsilon: 0.55963018000956    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.21\n",
      "episode: 1623   score: 7.0   memory length: 322796   epsilon: 0.5588619400095767    steps: 388    lr: 1.6000000000000003e-05     evaluation reward: 3.2\n",
      "episode: 1624   score: 5.0   memory length: 323119   epsilon: 0.5582224000095906    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 3.22\n",
      "episode: 1625   score: 4.0   memory length: 323379   epsilon: 0.5577076000096017    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.21\n",
      "episode: 1626   score: 3.0   memory length: 323624   epsilon: 0.5572225000096123    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 3.21\n",
      "episode: 1627   score: 6.0   memory length: 323978   epsilon: 0.5565215800096275    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 3.26\n",
      "episode: 1628   score: 2.0   memory length: 324177   epsilon: 0.556127560009636    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.21\n",
      "episode: 1629   score: 2.0   memory length: 324376   epsilon: 0.5557335400096446    steps: 199    lr: 1.6000000000000003e-05     evaluation reward: 3.21\n",
      "episode: 1630   score: 6.0   memory length: 324769   epsilon: 0.5549554000096615    steps: 393    lr: 1.6000000000000003e-05     evaluation reward: 3.26\n",
      "episode: 1631   score: 1.0   memory length: 324919   epsilon: 0.5546584000096679    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.24\n",
      "episode: 1632   score: 5.0   memory length: 325262   epsilon: 0.5539792600096827    steps: 343    lr: 1.6000000000000003e-05     evaluation reward: 3.24\n",
      "episode: 1633   score: 2.0   memory length: 325462   epsilon: 0.5535832600096913    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 3.23\n",
      "episode: 1634   score: 3.0   memory length: 325675   epsilon: 0.5531615200097004    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.22\n",
      "episode: 1635   score: 3.0   memory length: 325906   epsilon: 0.5527041400097104    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.22\n",
      "episode: 1636   score: 1.0   memory length: 326057   epsilon: 0.5524051600097168    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.2\n",
      "episode: 1637   score: 1.0   memory length: 326207   epsilon: 0.5521081600097233    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.19\n",
      "episode: 1638   score: 3.0   memory length: 326434   epsilon: 0.551658700009733    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.2\n",
      "episode: 1639   score: 5.0   memory length: 326762   epsilon: 0.5510092600097471    steps: 328    lr: 1.6000000000000003e-05     evaluation reward: 3.23\n",
      "episode: 1640   score: 2.0   memory length: 326941   epsilon: 0.5506548400097548    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 3.21\n",
      "episode: 1641   score: 6.0   memory length: 327293   epsilon: 0.54995788000977    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 3.22\n",
      "episode: 1642   score: 2.0   memory length: 327472   epsilon: 0.5496034600097777    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 3.22\n",
      "episode: 1643   score: 2.0   memory length: 327651   epsilon: 0.5492490400097854    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 3.23\n",
      "episode: 1644   score: 4.0   memory length: 327892   epsilon: 0.5487718600097957    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 3.24\n",
      "episode: 1645   score: 3.0   memory length: 328117   epsilon: 0.5483263600098054    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 3.26\n",
      "episode: 1646   score: 4.0   memory length: 328373   epsilon: 0.5478194800098164    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1647   score: 3.0   memory length: 328602   epsilon: 0.5473660600098262    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1648   score: 3.0   memory length: 328830   epsilon: 0.546914620009836    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1649   score: 3.0   memory length: 329042   epsilon: 0.5464948600098452    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.36\n",
      "episode: 1650   score: 1.0   memory length: 329211   epsilon: 0.5461602400098524    steps: 169    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1651   score: 1.0   memory length: 329361   epsilon: 0.5458632400098589    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1652   score: 7.0   memory length: 329606   epsilon: 0.5453781400098694    steps: 245    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1653   score: 3.0   memory length: 329813   epsilon: 0.5449682800098783    steps: 207    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1654   score: 2.0   memory length: 329993   epsilon: 0.544611880009886    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1655   score: 5.0   memory length: 330320   epsilon: 0.5439644200099001    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1656   score: 3.0   memory length: 330569   epsilon: 0.5434714000099108    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 3.36\n",
      "episode: 1657   score: 1.0   memory length: 330720   epsilon: 0.5431724200099173    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1658   score: 5.0   memory length: 331061   epsilon: 0.5424972400099319    steps: 341    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1659   score: 3.0   memory length: 331327   epsilon: 0.5419705600099434    steps: 266    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1660   score: 4.0   memory length: 331568   epsilon: 0.5414933800099537    steps: 241    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1661   score: 2.0   memory length: 331789   epsilon: 0.5410558000099632    steps: 221    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1662   score: 6.0   memory length: 332187   epsilon: 0.5402677600099803    steps: 398    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1663   score: 4.0   memory length: 332443   epsilon: 0.5397608800099913    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1664   score: 1.0   memory length: 332593   epsilon: 0.5394638800099978    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.29\n",
      "episode: 1665   score: 3.0   memory length: 332804   epsilon: 0.5390461000100069    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.29\n",
      "episode: 1666   score: 5.0   memory length: 333128   epsilon: 0.5384045800100208    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1667   score: 3.0   memory length: 333353   epsilon: 0.5379590800100305    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 3.29\n",
      "episode: 1668   score: 3.0   memory length: 333583   epsilon: 0.5375036800100403    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1669   score: 3.0   memory length: 333808   epsilon: 0.53705818001005    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1670   score: 3.0   memory length: 334036   epsilon: 0.5366067400100598    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1671   score: 3.0   memory length: 334248   epsilon: 0.5361869800100689    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1672   score: 4.0   memory length: 334545   epsilon: 0.5355989200100817    steps: 297    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1673   score: 2.0   memory length: 334726   epsilon: 0.5352405400100895    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1674   score: 5.0   memory length: 335014   epsilon: 0.5346703000101019    steps: 288    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1675   score: 8.0   memory length: 335445   epsilon: 0.5338169200101204    steps: 431    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1676   score: 5.0   memory length: 335732   epsilon: 0.5332486600101327    steps: 287    lr: 1.6000000000000003e-05     evaluation reward: 3.36\n",
      "episode: 1677   score: 3.0   memory length: 335958   epsilon: 0.5328011800101424    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1678   score: 3.0   memory length: 336186   epsilon: 0.5323497400101522    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.35\n",
      "episode: 1679   score: 2.0   memory length: 336406   epsilon: 0.5319141400101617    steps: 220    lr: 1.6000000000000003e-05     evaluation reward: 3.31\n",
      "episode: 1680   score: 2.0   memory length: 336586   epsilon: 0.5315577400101694    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 3.28\n",
      "episode: 1681   score: 4.0   memory length: 336861   epsilon: 0.5310132400101812    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.29\n",
      "episode: 1682   score: 4.0   memory length: 337100   epsilon: 0.5305400200101915    steps: 239    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1683   score: 5.0   memory length: 337418   epsilon: 0.5299103800102052    steps: 318    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1684   score: 3.0   memory length: 337644   epsilon: 0.5294629000102149    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.33\n",
      "episode: 1685   score: 2.0   memory length: 337860   epsilon: 0.5290352200102242    steps: 216    lr: 1.6000000000000003e-05     evaluation reward: 3.32\n",
      "episode: 1686   score: 2.0   memory length: 338077   epsilon: 0.5286055600102335    steps: 217    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1687   score: 3.0   memory length: 338286   epsilon: 0.5281917400102425    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 3.3\n",
      "episode: 1688   score: 1.0   memory length: 338455   epsilon: 0.5278571200102498    steps: 169    lr: 1.6000000000000003e-05     evaluation reward: 3.26\n",
      "episode: 1689   score: 3.0   memory length: 338667   epsilon: 0.5274373600102589    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.24\n",
      "episode: 1690   score: 3.0   memory length: 338880   epsilon: 0.527015620010268    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.25\n",
      "episode: 1691   score: 3.0   memory length: 339109   epsilon: 0.5265622000102779    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.26\n",
      "episode: 1692   score: 4.0   memory length: 339385   epsilon: 0.5260157200102897    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.26\n",
      "episode: 1693   score: 3.0   memory length: 339612   epsilon: 0.5255662600102995    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.25\n",
      "episode: 1694   score: 9.0   memory length: 340074   epsilon: 0.5246515000103193    steps: 462    lr: 1.6000000000000003e-05     evaluation reward: 3.29\n",
      "episode: 1695   score: 5.0   memory length: 340364   epsilon: 0.5240773000103318    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 3.28\n",
      "episode: 1696   score: 3.0   memory length: 340589   epsilon: 0.5236318000103415    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 3.25\n",
      "episode: 1697   score: 7.0   memory length: 341013   epsilon: 0.5227922800103597    steps: 424    lr: 1.6000000000000003e-05     evaluation reward: 3.27\n",
      "episode: 1698   score: 2.0   memory length: 341192   epsilon: 0.5224378600103674    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 3.24\n",
      "episode: 1699   score: 6.0   memory length: 341583   epsilon: 0.5216636800103842    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 3.27\n",
      "episode: 1700   score: 1.0   memory length: 341733   epsilon: 0.5213666800103907    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.25\n",
      "episode: 1701   score: 6.0   memory length: 342101   epsilon: 0.5206380400104065    steps: 368    lr: 1.6000000000000003e-05     evaluation reward: 3.28\n",
      "episode: 1702   score: 8.0   memory length: 342558   epsilon: 0.5197331800104261    steps: 457    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1703   score: 3.0   memory length: 342806   epsilon: 0.5192421400104368    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 3.36\n",
      "episode: 1704   score: 3.0   memory length: 343015   epsilon: 0.5188283200104458    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 3.34\n",
      "episode: 1705   score: 5.0   memory length: 343340   epsilon: 0.5181848200104597    steps: 325    lr: 1.6000000000000003e-05     evaluation reward: 3.37\n",
      "episode: 1706   score: 5.0   memory length: 343649   epsilon: 0.517573000010473    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 3.41\n",
      "episode: 1707   score: 5.0   memory length: 343956   epsilon: 0.5169651400104862    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 3.43\n",
      "episode: 1708   score: 7.0   memory length: 344338   epsilon: 0.5162087800105026    steps: 382    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1709   score: 2.0   memory length: 344536   epsilon: 0.5158167400105111    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1710   score: 3.0   memory length: 344765   epsilon: 0.515363320010521    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1711   score: 4.0   memory length: 345024   epsilon: 0.5148505000105321    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1712   score: 3.0   memory length: 345252   epsilon: 0.5143990600105419    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.48\n",
      "episode: 1713   score: 4.0   memory length: 345506   epsilon: 0.5138961400105528    steps: 254    lr: 1.6000000000000003e-05     evaluation reward: 3.49\n",
      "episode: 1714   score: 2.0   memory length: 345703   epsilon: 0.5135060800105613    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1715   score: 5.0   memory length: 345975   epsilon: 0.512967520010573    steps: 272    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1716   score: 4.0   memory length: 346231   epsilon: 0.512460640010584    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1717   score: 5.0   memory length: 346540   epsilon: 0.5118488200105973    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1718   score: 3.0   memory length: 346751   epsilon: 0.5114310400106064    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1719   score: 7.0   memory length: 347171   epsilon: 0.5105994400106244    steps: 420    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1720   score: 4.0   memory length: 347428   epsilon: 0.5100905800106355    steps: 257    lr: 1.6000000000000003e-05     evaluation reward: 3.63\n",
      "episode: 1721   score: 2.0   memory length: 347646   epsilon: 0.5096589400106448    steps: 218    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1722   score: 6.0   memory length: 348005   epsilon: 0.5089481200106603    steps: 359    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1723   score: 2.0   memory length: 348202   epsilon: 0.5085580600106687    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1724   score: 2.0   memory length: 348402   epsilon: 0.5081620600106773    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1725   score: 3.0   memory length: 348630   epsilon: 0.5077106200106871    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1726   score: 3.0   memory length: 348859   epsilon: 0.507257200010697    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1727   score: 3.0   memory length: 349087   epsilon: 0.5068057600107068    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1728   score: 4.0   memory length: 349346   epsilon: 0.5062929400107179    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1729   score: 2.0   memory length: 349543   epsilon: 0.5059028800107264    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1730   score: 4.0   memory length: 349799   epsilon: 0.5053960000107374    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1731   score: 3.0   memory length: 350027   epsilon: 0.5049445600107472    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1732   score: 3.0   memory length: 350276   epsilon: 0.5044515400107579    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1733   score: 3.0   memory length: 350504   epsilon: 0.5040001000107677    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1734   score: 3.0   memory length: 350736   epsilon: 0.5035407400107776    steps: 232    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1735   score: 5.0   memory length: 351062   epsilon: 0.5028952600107917    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1736   score: 3.0   memory length: 351291   epsilon: 0.5024418400108015    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1737   score: 1.0   memory length: 351441   epsilon: 0.502144840010808    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1738   score: 0.0   memory length: 351563   epsilon: 0.5019032800108132    steps: 122    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1739   score: 2.0   memory length: 351779   epsilon: 0.5014756000108225    steps: 216    lr: 1.6000000000000003e-05     evaluation reward: 3.52\n",
      "episode: 1740   score: 3.0   memory length: 352005   epsilon: 0.5010281200108322    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1741   score: 3.0   memory length: 352255   epsilon: 0.5005331200108429    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 3.5\n",
      "episode: 1742   score: 5.0   memory length: 352585   epsilon: 0.49987972001085373    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1743   score: 2.0   memory length: 352767   epsilon: 0.49951936001085145    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1744   score: 4.0   memory length: 353026   epsilon: 0.4990065400108482    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1745   score: 3.0   memory length: 353235   epsilon: 0.4985927200108456    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1746   score: 6.0   memory length: 353581   epsilon: 0.49790764001084126    steps: 346    lr: 1.6000000000000003e-05     evaluation reward: 3.55\n",
      "episode: 1747   score: 1.0   memory length: 353753   epsilon: 0.4975670800108391    steps: 172    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1748   score: 3.0   memory length: 353982   epsilon: 0.49711366001083623    steps: 229    lr: 1.6000000000000003e-05     evaluation reward: 3.53\n",
      "episode: 1749   score: 4.0   memory length: 354241   epsilon: 0.496600840010833    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1750   score: 4.0   memory length: 354518   epsilon: 0.4960523800108295    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1751   score: 4.0   memory length: 354817   epsilon: 0.4954603600108258    steps: 299    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1752   score: 3.0   memory length: 355044   epsilon: 0.49501090001082293    steps: 227    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1753   score: 4.0   memory length: 355318   epsilon: 0.4944683800108195    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1754   score: 1.0   memory length: 355487   epsilon: 0.4941337600108174    steps: 169    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1755   score: 3.0   memory length: 355696   epsilon: 0.49371994001081476    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1756   score: 3.0   memory length: 355905   epsilon: 0.49330612001081214    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 3.54\n",
      "episode: 1757   score: 6.0   memory length: 356260   epsilon: 0.4926032200108077    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1758   score: 4.0   memory length: 356519   epsilon: 0.49209040001080445    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1759   score: 4.0   memory length: 356795   epsilon: 0.491543920010801    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1760   score: 1.0   memory length: 356945   epsilon: 0.4912469200107991    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.56\n",
      "episode: 1761   score: 8.0   memory length: 357377   epsilon: 0.4903915600107937    steps: 432    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1762   score: 4.0   memory length: 357638   epsilon: 0.48987478001079043    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1763   score: 6.0   memory length: 357974   epsilon: 0.4892095000107862    steps: 336    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1764   score: 3.0   memory length: 358187   epsilon: 0.48878776001078356    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1765   score: 5.0   memory length: 358483   epsilon: 0.48820168001077985    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1766   score: 3.0   memory length: 358711   epsilon: 0.487750240010777    steps: 228    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1767   score: 3.0   memory length: 358979   epsilon: 0.48721960001077363    steps: 268    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1768   score: 4.0   memory length: 359240   epsilon: 0.48670282001077037    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1769   score: 4.0   memory length: 359518   epsilon: 0.4861523800107669    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1770   score: 2.0   memory length: 359698   epsilon: 0.4857959800107646    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1771   score: 1.0   memory length: 359848   epsilon: 0.48549898001076275    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.63\n",
      "episode: 1772   score: 6.0   memory length: 360220   epsilon: 0.4847624200107581    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1773   score: 3.0   memory length: 360468   epsilon: 0.484271380010755    steps: 248    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1774   score: 3.0   memory length: 360679   epsilon: 0.48385360001075234    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1775   score: 5.0   memory length: 361003   epsilon: 0.4832120800107483    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1776   score: 4.0   memory length: 361281   epsilon: 0.4826616400107448    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1777   score: 3.0   memory length: 361511   epsilon: 0.4822062400107419    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1778   score: 5.0   memory length: 361815   epsilon: 0.4816043200107381    steps: 304    lr: 1.6000000000000003e-05     evaluation reward: 3.62\n",
      "episode: 1779   score: 8.0   memory length: 362284   epsilon: 0.48067570001073223    steps: 469    lr: 1.6000000000000003e-05     evaluation reward: 3.68\n",
      "episode: 1780   score: 4.0   memory length: 362580   epsilon: 0.4800896200107285    steps: 296    lr: 1.6000000000000003e-05     evaluation reward: 3.7\n",
      "episode: 1781   score: 6.0   memory length: 362937   epsilon: 0.47938276001072405    steps: 357    lr: 1.6000000000000003e-05     evaluation reward: 3.72\n",
      "episode: 1782   score: 3.0   memory length: 363150   epsilon: 0.4789610200107214    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1783   score: 3.0   memory length: 363359   epsilon: 0.47854720001071877    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 3.69\n",
      "episode: 1784   score: 3.0   memory length: 363606   epsilon: 0.47805814001071567    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.69\n",
      "episode: 1785   score: 3.0   memory length: 363836   epsilon: 0.4776027400107128    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.7\n",
      "episode: 1786   score: 7.0   memory length: 364224   epsilon: 0.47683450001070793    steps: 388    lr: 1.6000000000000003e-05     evaluation reward: 3.75\n",
      "episode: 1787   score: 1.0   memory length: 364374   epsilon: 0.47653750001070605    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1788   score: 5.0   memory length: 364719   epsilon: 0.47585440001070173    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 3.77\n",
      "episode: 1789   score: 6.0   memory length: 365080   epsilon: 0.4751396200106972    steps: 361    lr: 1.6000000000000003e-05     evaluation reward: 3.8\n",
      "episode: 1790   score: 3.0   memory length: 365329   epsilon: 0.4746466000106941    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 3.8\n",
      "episode: 1791   score: 2.0   memory length: 365509   epsilon: 0.47429020001069183    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 3.79\n",
      "episode: 1792   score: 4.0   memory length: 365784   epsilon: 0.4737457000106884    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.79\n",
      "episode: 1793   score: 4.0   memory length: 366063   epsilon: 0.4731932800106849    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 3.8\n",
      "episode: 1794   score: 6.0   memory length: 366414   epsilon: 0.4724983000106805    steps: 351    lr: 1.6000000000000003e-05     evaluation reward: 3.77\n",
      "episode: 1795   score: 0.0   memory length: 366536   epsilon: 0.47225674001067897    steps: 122    lr: 1.6000000000000003e-05     evaluation reward: 3.72\n",
      "episode: 1796   score: 4.0   memory length: 366830   epsilon: 0.4716746200106753    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1797   score: 5.0   memory length: 367137   epsilon: 0.47106676001067144    steps: 307    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1798   score: 1.0   memory length: 367309   epsilon: 0.4707262000106693    steps: 172    lr: 1.6000000000000003e-05     evaluation reward: 3.7\n",
      "episode: 1799   score: 5.0   memory length: 367636   epsilon: 0.4700787400106652    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 3.69\n",
      "episode: 1800   score: 3.0   memory length: 367886   epsilon: 0.46958374001066205    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1801   score: 6.0   memory length: 368240   epsilon: 0.4688828200106576    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1802   score: 5.0   memory length: 368549   epsilon: 0.46827100001065375    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 3.68\n",
      "episode: 1803   score: 7.0   memory length: 368949   epsilon: 0.46747900001064874    steps: 400    lr: 1.6000000000000003e-05     evaluation reward: 3.72\n",
      "episode: 1804   score: 2.0   memory length: 369128   epsilon: 0.4671245800106465    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1805   score: 1.0   memory length: 369278   epsilon: 0.4668275800106446    steps: 150    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1806   score: 5.0   memory length: 369568   epsilon: 0.466253380010641    steps: 290    lr: 1.6000000000000003e-05     evaluation reward: 3.67\n",
      "episode: 1807   score: 7.0   memory length: 369997   epsilon: 0.4654039600106356    steps: 429    lr: 1.6000000000000003e-05     evaluation reward: 3.69\n",
      "episode: 1808   score: 3.0   memory length: 370223   epsilon: 0.4649564800106328    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1809   score: 5.0   memory length: 370532   epsilon: 0.4643446600106289    steps: 309    lr: 1.6000000000000003e-05     evaluation reward: 3.68\n",
      "episode: 1810   score: 1.0   memory length: 370683   epsilon: 0.464045680010627    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.66\n",
      "episode: 1811   score: 2.0   memory length: 370865   epsilon: 0.46368532001062474    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.64\n",
      "episode: 1812   score: 4.0   memory length: 371143   epsilon: 0.46313488001062125    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1813   score: 2.0   memory length: 371322   epsilon: 0.462780460010619    steps: 179    lr: 1.6000000000000003e-05     evaluation reward: 3.63\n",
      "episode: 1814   score: 4.0   memory length: 371562   epsilon: 0.462305260010616    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1815   score: 1.0   memory length: 371730   epsilon: 0.4619726200106139    steps: 168    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1816   score: 3.0   memory length: 371938   epsilon: 0.4615607800106113    steps: 208    lr: 1.6000000000000003e-05     evaluation reward: 3.6\n",
      "episode: 1817   score: 2.0   memory length: 372119   epsilon: 0.461202400010609    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1818   score: 4.0   memory length: 372377   epsilon: 0.4606915600106058    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1819   score: 6.0   memory length: 372752   epsilon: 0.4599490600106011    steps: 375    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1820   score: 6.0   memory length: 373125   epsilon: 0.4592105200105964    steps: 373    lr: 1.6000000000000003e-05     evaluation reward: 3.59\n",
      "episode: 1821   score: 4.0   memory length: 373384   epsilon: 0.4586977000105932    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1822   score: 2.0   memory length: 373581   epsilon: 0.4583076400105907    steps: 197    lr: 1.6000000000000003e-05     evaluation reward: 3.57\n",
      "episode: 1823   score: 3.0   memory length: 373807   epsilon: 0.4578601600105879    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.58\n",
      "episode: 1824   score: 5.0   memory length: 374090   epsilon: 0.45729982001058433    steps: 283    lr: 1.6000000000000003e-05     evaluation reward: 3.61\n",
      "episode: 1825   score: 7.0   memory length: 374515   epsilon: 0.456458320010579    steps: 425    lr: 1.6000000000000003e-05     evaluation reward: 3.65\n",
      "episode: 1826   score: 6.0   memory length: 374850   epsilon: 0.4557950200105748    steps: 335    lr: 1.6000000000000003e-05     evaluation reward: 3.68\n",
      "episode: 1827   score: 3.0   memory length: 375061   epsilon: 0.45537724001057217    steps: 211    lr: 1.6000000000000003e-05     evaluation reward: 3.68\n",
      "episode: 1828   score: 4.0   memory length: 375338   epsilon: 0.4548287800105687    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.68\n",
      "episode: 1829   score: 2.0   memory length: 375536   epsilon: 0.4544367400105662    steps: 198    lr: 1.6000000000000003e-05     evaluation reward: 3.68\n",
      "episode: 1830   score: 5.0   memory length: 375866   epsilon: 0.4537833400105621    steps: 330    lr: 1.6000000000000003e-05     evaluation reward: 3.69\n",
      "episode: 1831   score: 4.0   memory length: 376126   epsilon: 0.45326854001055883    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.7\n",
      "episode: 1832   score: 4.0   memory length: 376402   epsilon: 0.45272206001055537    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1833   score: 3.0   memory length: 376649   epsilon: 0.4522330000105523    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.71\n",
      "episode: 1834   score: 5.0   memory length: 376936   epsilon: 0.4516647400105487    steps: 287    lr: 1.6000000000000003e-05     evaluation reward: 3.73\n",
      "episode: 1835   score: 6.0   memory length: 377322   epsilon: 0.45090046001054385    steps: 386    lr: 1.6000000000000003e-05     evaluation reward: 3.74\n",
      "episode: 1836   score: 4.0   memory length: 377600   epsilon: 0.45035002001054036    steps: 278    lr: 1.6000000000000003e-05     evaluation reward: 3.75\n",
      "episode: 1837   score: 8.0   memory length: 378012   epsilon: 0.4495342600105352    steps: 412    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1838   score: 3.0   memory length: 378221   epsilon: 0.4491204400105326    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1839   score: 4.0   memory length: 378479   epsilon: 0.44860960001052935    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1840   score: 0.0   memory length: 378602   epsilon: 0.4483660600105278    steps: 123    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1841   score: 3.0   memory length: 378848   epsilon: 0.44787898001052473    steps: 246    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1842   score: 4.0   memory length: 379104   epsilon: 0.4473721000105215    steps: 256    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1843   score: 2.0   memory length: 379284   epsilon: 0.44701570001051927    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1844   score: 3.0   memory length: 379534   epsilon: 0.44652070001051614    steps: 250    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1845   score: 4.0   memory length: 379795   epsilon: 0.44600392001051287    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1846   score: 4.0   memory length: 380070   epsilon: 0.4454594200105094    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1847   score: 11.0   memory length: 380642   epsilon: 0.44432686001050226    steps: 572    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1848   score: 3.0   memory length: 380851   epsilon: 0.44391304001049964    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1849   score: 6.0   memory length: 381203   epsilon: 0.44321608001049523    steps: 352    lr: 1.6000000000000003e-05     evaluation reward: 3.93\n",
      "episode: 1850   score: 5.0   memory length: 381508   epsilon: 0.4426121800104914    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 3.94\n",
      "episode: 1851   score: 5.0   memory length: 381834   epsilon: 0.4419667000104873    steps: 326    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1852   score: 3.0   memory length: 382101   epsilon: 0.441438040010484    steps: 267    lr: 1.6000000000000003e-05     evaluation reward: 3.95\n",
      "episode: 1853   score: 3.0   memory length: 382331   epsilon: 0.4409826400104811    steps: 230    lr: 1.6000000000000003e-05     evaluation reward: 3.94\n",
      "episode: 1854   score: 3.0   memory length: 382540   epsilon: 0.4405688200104785    steps: 209    lr: 1.6000000000000003e-05     evaluation reward: 3.96\n",
      "episode: 1855   score: 5.0   memory length: 382864   epsilon: 0.4399273000104744    steps: 324    lr: 1.6000000000000003e-05     evaluation reward: 3.98\n",
      "episode: 1856   score: 3.0   memory length: 383095   epsilon: 0.4394699200104715    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.98\n",
      "episode: 1857   score: 1.0   memory length: 383246   epsilon: 0.43917094001046963    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.93\n",
      "episode: 1858   score: 3.0   memory length: 383493   epsilon: 0.43868188001046654    steps: 247    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1859   score: 4.0   memory length: 383775   epsilon: 0.438123520010463    steps: 282    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1860   score: 2.0   memory length: 383957   epsilon: 0.43776316001046073    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.93\n",
      "episode: 1861   score: 2.0   memory length: 384139   epsilon: 0.43740280001045845    steps: 182    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1862   score: 4.0   memory length: 384433   epsilon: 0.43682068001045476    steps: 294    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1863   score: 3.0   memory length: 384659   epsilon: 0.43637320001045193    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1864   score: 5.0   memory length: 384944   epsilon: 0.43580890001044836    steps: 285    lr: 1.6000000000000003e-05     evaluation reward: 3.86\n",
      "episode: 1865   score: 6.0   memory length: 385321   epsilon: 0.43506244001044364    steps: 377    lr: 1.6000000000000003e-05     evaluation reward: 3.87\n",
      "episode: 1866   score: 8.0   memory length: 385735   epsilon: 0.43424272001043845    steps: 414    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1867   score: 4.0   memory length: 386012   epsilon: 0.433694260010435    steps: 277    lr: 1.6000000000000003e-05     evaluation reward: 3.93\n",
      "episode: 1868   score: 3.0   memory length: 386220   epsilon: 0.4332824200104324    steps: 208    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1869   score: 1.0   memory length: 386371   epsilon: 0.4329834400104305    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1870   score: 4.0   memory length: 386647   epsilon: 0.43243696001042703    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1871   score: 1.0   memory length: 386798   epsilon: 0.43213798001042514    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1872   score: 4.0   memory length: 387077   epsilon: 0.43158556001042164    steps: 279    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1873   score: 3.0   memory length: 387287   epsilon: 0.431169760010419    steps: 210    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1874   score: 5.0   memory length: 387574   epsilon: 0.4306015000104154    steps: 287    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1875   score: 2.0   memory length: 387755   epsilon: 0.43024312001041315    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1876   score: 4.0   memory length: 388031   epsilon: 0.4296966400104097    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1877   score: 6.0   memory length: 388409   epsilon: 0.42894820001040496    steps: 378    lr: 1.6000000000000003e-05     evaluation reward: 3.91\n",
      "episode: 1878   score: 2.0   memory length: 388609   epsilon: 0.42855220001040245    steps: 200    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1879   score: 3.0   memory length: 388840   epsilon: 0.42809482001039956    steps: 231    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1880   score: 3.0   memory length: 389053   epsilon: 0.4276730800103969    steps: 213    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1881   score: 4.0   memory length: 389313   epsilon: 0.42715828001039363    steps: 260    lr: 1.6000000000000003e-05     evaluation reward: 3.8\n",
      "episode: 1882   score: 4.0   memory length: 389571   epsilon: 0.4266474400103904    steps: 258    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1883   score: 3.0   memory length: 389786   epsilon: 0.4262217400103877    steps: 215    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1884   score: 3.0   memory length: 390053   epsilon: 0.42569308001038436    steps: 267    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1885   score: 6.0   memory length: 390444   epsilon: 0.42491890001037946    steps: 391    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1886   score: 4.0   memory length: 390720   epsilon: 0.424372420010376    steps: 276    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1887   score: 5.0   memory length: 391043   epsilon: 0.42373288001037196    steps: 323    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1888   score: 5.0   memory length: 391335   epsilon: 0.4231547200103683    steps: 292    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1889   score: 0.0   memory length: 391458   epsilon: 0.42291118001036676    steps: 123    lr: 1.6000000000000003e-05     evaluation reward: 3.79\n",
      "episode: 1890   score: 3.0   memory length: 391683   epsilon: 0.42246568001036394    steps: 225    lr: 1.6000000000000003e-05     evaluation reward: 3.79\n",
      "episode: 1891   score: 8.0   memory length: 392151   epsilon: 0.4215390400103581    steps: 468    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1892   score: 1.0   memory length: 392302   epsilon: 0.4212400600103562    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1893   score: 3.0   memory length: 392509   epsilon: 0.4208302000103536    steps: 207    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1894   score: 4.0   memory length: 392824   epsilon: 0.42020650001034965    steps: 315    lr: 1.6000000000000003e-05     evaluation reward: 3.79\n",
      "episode: 1895   score: 3.0   memory length: 393050   epsilon: 0.4197590200103468    steps: 226    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1896   score: 1.0   memory length: 393201   epsilon: 0.4194600400103449    steps: 151    lr: 1.6000000000000003e-05     evaluation reward: 3.79\n",
      "episode: 1897   score: 6.0   memory length: 393556   epsilon: 0.4187571400103405    steps: 355    lr: 1.6000000000000003e-05     evaluation reward: 3.8\n",
      "episode: 1898   score: 4.0   memory length: 393830   epsilon: 0.41821462001033705    steps: 274    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1899   score: 6.0   memory length: 394143   epsilon: 0.4175948800103331    steps: 313    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1900   score: 3.0   memory length: 394355   epsilon: 0.41717512001033047    steps: 212    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1901   score: 5.0   memory length: 394682   epsilon: 0.4165276600103264    steps: 327    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1902   score: 4.0   memory length: 394957   epsilon: 0.4159831600103229    steps: 275    lr: 1.6000000000000003e-05     evaluation reward: 3.82\n",
      "episode: 1903   score: 6.0   memory length: 395302   epsilon: 0.4153000600103186    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 3.81\n",
      "episode: 1904   score: 4.0   memory length: 395561   epsilon: 0.41478724001031536    steps: 259    lr: 1.6000000000000003e-05     evaluation reward: 3.83\n",
      "episode: 1905   score: 6.0   memory length: 395915   epsilon: 0.4140863200103109    steps: 354    lr: 1.6000000000000003e-05     evaluation reward: 3.88\n",
      "episode: 1906   score: 2.0   memory length: 396095   epsilon: 0.41372992001030867    steps: 180    lr: 1.6000000000000003e-05     evaluation reward: 3.85\n",
      "episode: 1907   score: 6.0   memory length: 396458   epsilon: 0.4130111800103041    steps: 363    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1908   score: 3.0   memory length: 396709   epsilon: 0.412514200010301    steps: 251    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1909   score: 5.0   memory length: 396995   epsilon: 0.4119479200102974    steps: 286    lr: 1.6000000000000003e-05     evaluation reward: 3.84\n",
      "episode: 1910   score: 6.0   memory length: 397393   epsilon: 0.4111598800102924    steps: 398    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1911   score: 2.0   memory length: 397574   epsilon: 0.41080150001029014    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.89\n",
      "episode: 1912   score: 5.0   memory length: 397879   epsilon: 0.4101976000102863    steps: 305    lr: 1.6000000000000003e-05     evaluation reward: 3.9\n",
      "episode: 1913   score: 2.0   memory length: 398060   epsilon: 0.40983922001028406    steps: 181    lr: 1.6000000000000003e-05     evaluation reward: 3.9\n",
      "episode: 1914   score: 6.0   memory length: 398432   epsilon: 0.4091026600102794    steps: 372    lr: 1.6000000000000003e-05     evaluation reward: 3.92\n",
      "episode: 1915   score: 7.0   memory length: 398830   epsilon: 0.4083146200102744    steps: 398    lr: 1.6000000000000003e-05     evaluation reward: 3.98\n",
      "episode: 1916   score: 4.0   memory length: 399070   epsilon: 0.4078394200102714    steps: 240    lr: 1.6000000000000003e-05     evaluation reward: 3.99\n",
      "episode: 1917   score: 3.0   memory length: 399319   epsilon: 0.4073464000102683    steps: 249    lr: 1.6000000000000003e-05     evaluation reward: 4.0\n",
      "episode: 1918   score: 5.0   memory length: 399664   epsilon: 0.40666330001026396    steps: 345    lr: 1.6000000000000003e-05     evaluation reward: 4.01\n",
      "episode: 1919   score: 4.0   memory length: 399925   epsilon: 0.4061465200102607    steps: 261    lr: 1.6000000000000003e-05     evaluation reward: 3.99\n",
      "episode: 1920   score: 3.0   memory length: 400138   epsilon: 0.405724780010258    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 3.96\n",
      "episode: 1921   score: 3.0   memory length: 400365   epsilon: 0.4052753200102552    steps: 227    lr: 6.400000000000001e-06     evaluation reward: 3.95\n",
      "episode: 1922   score: 5.0   memory length: 400669   epsilon: 0.40467340001025137    steps: 304    lr: 6.400000000000001e-06     evaluation reward: 3.98\n",
      "episode: 1923   score: 5.0   memory length: 400975   epsilon: 0.40406752001024754    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 4.0\n",
      "episode: 1924   score: 4.0   memory length: 401274   epsilon: 0.4034755000102438    steps: 299    lr: 6.400000000000001e-06     evaluation reward: 3.99\n",
      "episode: 1925   score: 3.0   memory length: 401503   epsilon: 0.4030220800102409    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 3.95\n",
      "episode: 1926   score: 2.0   memory length: 401701   epsilon: 0.40263004001023844    steps: 198    lr: 6.400000000000001e-06     evaluation reward: 3.91\n",
      "episode: 1927   score: 4.0   memory length: 401959   epsilon: 0.4021192000102352    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 3.92\n",
      "episode: 1928   score: 2.0   memory length: 402141   epsilon: 0.40175884001023293    steps: 182    lr: 6.400000000000001e-06     evaluation reward: 3.9\n",
      "episode: 1929   score: 3.0   memory length: 402389   epsilon: 0.4012678000102298    steps: 248    lr: 6.400000000000001e-06     evaluation reward: 3.91\n",
      "episode: 1930   score: 3.0   memory length: 402601   epsilon: 0.40084804001022717    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 3.89\n",
      "episode: 1931   score: 2.0   memory length: 402798   epsilon: 0.4004579800102247    steps: 197    lr: 6.400000000000001e-06     evaluation reward: 3.87\n",
      "episode: 1932   score: 4.0   memory length: 403057   epsilon: 0.39994516001022146    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 3.87\n",
      "episode: 1933   score: 2.0   memory length: 403257   epsilon: 0.39954916001021895    steps: 200    lr: 6.400000000000001e-06     evaluation reward: 3.86\n",
      "episode: 1934   score: 3.0   memory length: 403465   epsilon: 0.39913732001021635    steps: 208    lr: 6.400000000000001e-06     evaluation reward: 3.84\n",
      "episode: 1935   score: 3.0   memory length: 403675   epsilon: 0.3987215200102137    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 3.81\n",
      "episode: 1936   score: 5.0   memory length: 403965   epsilon: 0.3981473200102101    steps: 290    lr: 6.400000000000001e-06     evaluation reward: 3.82\n",
      "episode: 1937   score: 3.0   memory length: 404214   epsilon: 0.39765430001020696    steps: 249    lr: 6.400000000000001e-06     evaluation reward: 3.77\n",
      "episode: 1938   score: 3.0   memory length: 404423   epsilon: 0.39724048001020434    steps: 209    lr: 6.400000000000001e-06     evaluation reward: 3.77\n",
      "episode: 1939   score: 4.0   memory length: 404664   epsilon: 0.3967633000102013    steps: 241    lr: 6.400000000000001e-06     evaluation reward: 3.77\n",
      "episode: 1940   score: 6.0   memory length: 405036   epsilon: 0.39602674001019667    steps: 372    lr: 6.400000000000001e-06     evaluation reward: 3.83\n",
      "episode: 1941   score: 6.0   memory length: 405369   epsilon: 0.3953674000101925    steps: 333    lr: 6.400000000000001e-06     evaluation reward: 3.86\n",
      "episode: 1942   score: 4.0   memory length: 405643   epsilon: 0.39482488001018906    steps: 274    lr: 6.400000000000001e-06     evaluation reward: 3.86\n",
      "episode: 1943   score: 3.0   memory length: 405872   epsilon: 0.3943714600101862    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 3.87\n",
      "episode: 1944   score: 6.0   memory length: 406247   epsilon: 0.3936289600101815    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 3.9\n",
      "episode: 1945   score: 3.0   memory length: 406475   epsilon: 0.39317752001017864    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 3.89\n",
      "episode: 1946   score: 4.0   memory length: 406749   epsilon: 0.3926350000101752    steps: 274    lr: 6.400000000000001e-06     evaluation reward: 3.89\n",
      "episode: 1947   score: 2.0   memory length: 406930   epsilon: 0.39227662001017294    steps: 181    lr: 6.400000000000001e-06     evaluation reward: 3.8\n",
      "episode: 1948   score: 4.0   memory length: 407188   epsilon: 0.3917657800101697    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 3.81\n",
      "episode: 1949   score: 3.0   memory length: 407398   epsilon: 0.3913499800101671    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 3.78\n",
      "episode: 1950   score: 8.0   memory length: 407811   epsilon: 0.3905322400101619    steps: 413    lr: 6.400000000000001e-06     evaluation reward: 3.81\n",
      "episode: 1951   score: 4.0   memory length: 408087   epsilon: 0.38998576001015844    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 3.8\n",
      "episode: 1952   score: 4.0   memory length: 408351   epsilon: 0.38946304001015514    steps: 264    lr: 6.400000000000001e-06     evaluation reward: 3.81\n",
      "episode: 1953   score: 6.0   memory length: 408725   epsilon: 0.38872252001015045    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 3.84\n",
      "episode: 1954   score: 5.0   memory length: 409050   epsilon: 0.3880790200101464    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 3.86\n",
      "episode: 1955   score: 6.0   memory length: 409431   epsilon: 0.3873246400101416    steps: 381    lr: 6.400000000000001e-06     evaluation reward: 3.87\n",
      "episode: 1956   score: 5.0   memory length: 409737   epsilon: 0.3867187600101378    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 3.89\n",
      "episode: 1957   score: 2.0   memory length: 409953   epsilon: 0.38629108001013507    steps: 216    lr: 6.400000000000001e-06     evaluation reward: 3.9\n",
      "episode: 1958   score: 4.0   memory length: 410233   epsilon: 0.38573668001013156    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 3.91\n",
      "episode: 1959   score: 3.0   memory length: 410458   epsilon: 0.38529118001012874    steps: 225    lr: 6.400000000000001e-06     evaluation reward: 3.9\n",
      "episode: 1960   score: 7.0   memory length: 410864   epsilon: 0.38448730001012366    steps: 406    lr: 6.400000000000001e-06     evaluation reward: 3.95\n",
      "episode: 1961   score: 1.0   memory length: 411015   epsilon: 0.38418832001012176    steps: 151    lr: 6.400000000000001e-06     evaluation reward: 3.94\n",
      "episode: 1962   score: 6.0   memory length: 411337   epsilon: 0.38355076001011773    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 3.96\n",
      "episode: 1963   score: 6.0   memory length: 411676   epsilon: 0.3828795400101135    steps: 339    lr: 6.400000000000001e-06     evaluation reward: 3.99\n",
      "episode: 1964   score: 4.0   memory length: 411935   epsilon: 0.38236672001011024    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 3.98\n",
      "episode: 1965   score: 3.0   memory length: 412183   epsilon: 0.38187568001010713    steps: 248    lr: 6.400000000000001e-06     evaluation reward: 3.95\n",
      "episode: 1966   score: 4.0   memory length: 412459   epsilon: 0.3813292000101037    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 3.91\n",
      "episode: 1967   score: 5.0   memory length: 412783   epsilon: 0.3806876800100996    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 3.92\n",
      "episode: 1968   score: 6.0   memory length: 413101   epsilon: 0.38005804001009563    steps: 318    lr: 6.400000000000001e-06     evaluation reward: 3.95\n",
      "episode: 1969   score: 3.0   memory length: 413310   epsilon: 0.379644220010093    steps: 209    lr: 6.400000000000001e-06     evaluation reward: 3.97\n",
      "episode: 1970   score: 2.0   memory length: 413507   epsilon: 0.37925416001009055    steps: 197    lr: 6.400000000000001e-06     evaluation reward: 3.95\n",
      "episode: 1971   score: 1.0   memory length: 413658   epsilon: 0.37895518001008865    steps: 151    lr: 6.400000000000001e-06     evaluation reward: 3.95\n",
      "episode: 1972   score: 8.0   memory length: 414058   epsilon: 0.37816318001008364    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 3.99\n",
      "episode: 1973   score: 5.0   memory length: 414346   epsilon: 0.37759294001008004    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 4.01\n",
      "episode: 1974   score: 6.0   memory length: 414696   epsilon: 0.37689994001007565    steps: 350    lr: 6.400000000000001e-06     evaluation reward: 4.02\n",
      "episode: 1975   score: 3.0   memory length: 414928   epsilon: 0.37644058001007275    steps: 232    lr: 6.400000000000001e-06     evaluation reward: 4.03\n",
      "episode: 1976   score: 8.0   memory length: 415353   epsilon: 0.3755990800100674    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 4.07\n",
      "episode: 1977   score: 4.0   memory length: 415610   epsilon: 0.3750902200100642    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 4.05\n",
      "episode: 1978   score: 6.0   memory length: 415987   epsilon: 0.3743437600100595    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 4.09\n",
      "episode: 1979   score: 9.0   memory length: 416411   epsilon: 0.37350424001005417    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 4.15\n",
      "episode: 1980   score: 3.0   memory length: 416621   epsilon: 0.37308844001005154    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 4.15\n",
      "episode: 1981   score: 4.0   memory length: 416878   epsilon: 0.3725795800100483    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 4.15\n",
      "episode: 1982   score: 6.0   memory length: 417230   epsilon: 0.3718826200100439    steps: 352    lr: 6.400000000000001e-06     evaluation reward: 4.17\n",
      "episode: 1983   score: 4.0   memory length: 417487   epsilon: 0.3713737600100407    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 4.18\n",
      "episode: 1984   score: 3.0   memory length: 417741   epsilon: 0.3708708400100375    steps: 254    lr: 6.400000000000001e-06     evaluation reward: 4.18\n",
      "episode: 1985   score: 2.0   memory length: 417923   epsilon: 0.3705104800100352    steps: 182    lr: 6.400000000000001e-06     evaluation reward: 4.14\n",
      "episode: 1986   score: 9.0   memory length: 418426   epsilon: 0.3695145400100289    steps: 503    lr: 6.400000000000001e-06     evaluation reward: 4.19\n",
      "episode: 1987   score: 4.0   memory length: 418707   epsilon: 0.3689581600100254    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 4.18\n",
      "episode: 1988   score: 11.0   memory length: 419216   epsilon: 0.36795034001001903    steps: 509    lr: 6.400000000000001e-06     evaluation reward: 4.24\n",
      "episode: 1989   score: 3.0   memory length: 419427   epsilon: 0.3675325600100164    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 4.27\n",
      "episode: 1990   score: 1.0   memory length: 419597   epsilon: 0.36719596001001426    steps: 170    lr: 6.400000000000001e-06     evaluation reward: 4.25\n",
      "episode: 1991   score: 0.0   memory length: 419720   epsilon: 0.3669524200100127    steps: 123    lr: 6.400000000000001e-06     evaluation reward: 4.17\n",
      "episode: 1992   score: 4.0   memory length: 419996   epsilon: 0.36640594001000926    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 4.2\n",
      "episode: 1993   score: 3.0   memory length: 420226   epsilon: 0.3659505400100064    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 4.2\n",
      "episode: 1994   score: 5.0   memory length: 420536   epsilon: 0.3653367400100025    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 4.21\n",
      "episode: 1995   score: 7.0   memory length: 420944   epsilon: 0.3645289000099974    steps: 408    lr: 6.400000000000001e-06     evaluation reward: 4.25\n",
      "episode: 1996   score: 6.0   memory length: 421284   epsilon: 0.3638557000099931    steps: 340    lr: 6.400000000000001e-06     evaluation reward: 4.3\n",
      "episode: 1997   score: 3.0   memory length: 421495   epsilon: 0.3634379200099905    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 4.27\n",
      "episode: 1998   score: 2.0   memory length: 421675   epsilon: 0.3630815200099882    steps: 180    lr: 6.400000000000001e-06     evaluation reward: 4.25\n",
      "episode: 1999   score: 9.0   memory length: 422139   epsilon: 0.3621628000099824    steps: 464    lr: 6.400000000000001e-06     evaluation reward: 4.28\n",
      "episode: 2000   score: 3.0   memory length: 422350   epsilon: 0.36174502000997977    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 4.28\n",
      "episode: 2001   score: 3.0   memory length: 422581   epsilon: 0.3612876400099769    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 4.26\n",
      "episode: 2002   score: 2.0   memory length: 422780   epsilon: 0.3608936200099744    steps: 199    lr: 6.400000000000001e-06     evaluation reward: 4.24\n",
      "episode: 2003   score: 4.0   memory length: 423061   epsilon: 0.36033724000997086    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 4.22\n",
      "episode: 2004   score: 3.0   memory length: 423290   epsilon: 0.359883820009968    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 4.21\n",
      "episode: 2005   score: 5.0   memory length: 423596   epsilon: 0.35927794000996416    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 4.2\n",
      "episode: 2006   score: 3.0   memory length: 423824   epsilon: 0.3588265000099613    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 4.21\n",
      "episode: 2007   score: 3.0   memory length: 424052   epsilon: 0.35837506000995845    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 4.18\n",
      "episode: 2008   score: 4.0   memory length: 424367   epsilon: 0.3577513600099545    steps: 315    lr: 6.400000000000001e-06     evaluation reward: 4.19\n",
      "episode: 2009   score: 3.0   memory length: 424630   epsilon: 0.3572306200099512    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 4.17\n",
      "episode: 2010   score: 3.0   memory length: 424860   epsilon: 0.3567752200099483    steps: 230    lr: 6.400000000000001e-06     evaluation reward: 4.14\n",
      "episode: 2011   score: 5.0   memory length: 425207   epsilon: 0.356088160009944    steps: 347    lr: 6.400000000000001e-06     evaluation reward: 4.17\n",
      "episode: 2012   score: 5.0   memory length: 425501   epsilon: 0.3555060400099403    steps: 294    lr: 6.400000000000001e-06     evaluation reward: 4.17\n",
      "episode: 2013   score: 1.0   memory length: 425651   epsilon: 0.3552090400099384    steps: 150    lr: 6.400000000000001e-06     evaluation reward: 4.16\n",
      "episode: 2014   score: 2.0   memory length: 425833   epsilon: 0.35484868000993613    steps: 182    lr: 6.400000000000001e-06     evaluation reward: 4.12\n",
      "episode: 2015   score: 5.0   memory length: 426105   epsilon: 0.3543101200099327    steps: 272    lr: 6.400000000000001e-06     evaluation reward: 4.1\n",
      "episode: 2016   score: 5.0   memory length: 426394   epsilon: 0.3537379000099291    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 4.11\n",
      "episode: 2017   score: 3.0   memory length: 426605   epsilon: 0.35332012000992646    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 4.11\n",
      "episode: 2018   score: 3.0   memory length: 426815   epsilon: 0.35290432000992383    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 4.09\n",
      "episode: 2019   score: 8.0   memory length: 427215   epsilon: 0.3521123200099188    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 4.13\n",
      "episode: 2020   score: 4.0   memory length: 427492   epsilon: 0.35156386000991535    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 4.14\n",
      "episode: 2021   score: 5.0   memory length: 427820   epsilon: 0.35091442000991124    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 4.16\n",
      "episode: 2022   score: 6.0   memory length: 428195   epsilon: 0.35017192000990655    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 4.17\n",
      "episode: 2023   score: 7.0   memory length: 428599   epsilon: 0.3493720000099015    steps: 404    lr: 6.400000000000001e-06     evaluation reward: 4.19\n",
      "episode: 2024   score: 5.0   memory length: 428887   epsilon: 0.3488017600098979    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 4.2\n",
      "episode: 2025   score: 3.0   memory length: 429115   epsilon: 0.348350320009895    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 4.2\n",
      "episode: 2026   score: 8.0   memory length: 429596   epsilon: 0.347397940009889    steps: 481    lr: 6.400000000000001e-06     evaluation reward: 4.26\n",
      "episode: 2027   score: 6.0   memory length: 429951   epsilon: 0.34669504000988455    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 4.28\n",
      "episode: 2028   score: 4.0   memory length: 430228   epsilon: 0.3461465800098811    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 4.3\n",
      "episode: 2029   score: 4.0   memory length: 430507   epsilon: 0.3455941600098776    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 4.31\n",
      "episode: 2030   score: 7.0   memory length: 430910   epsilon: 0.34479622000987253    steps: 403    lr: 6.400000000000001e-06     evaluation reward: 4.35\n",
      "episode: 2031   score: 3.0   memory length: 431137   epsilon: 0.3443467600098697    steps: 227    lr: 6.400000000000001e-06     evaluation reward: 4.36\n",
      "episode: 2032   score: 3.0   memory length: 431350   epsilon: 0.343925020009867    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 4.35\n",
      "episode: 2033   score: 5.0   memory length: 431657   epsilon: 0.3433171600098632    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 4.38\n",
      "episode: 2034   score: 6.0   memory length: 432007   epsilon: 0.3426241600098588    steps: 350    lr: 6.400000000000001e-06     evaluation reward: 4.41\n",
      "episode: 2035   score: 3.0   memory length: 432218   epsilon: 0.34220638000985615    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 4.41\n",
      "episode: 2036   score: 4.0   memory length: 432481   epsilon: 0.34168564000985285    steps: 263    lr: 6.400000000000001e-06     evaluation reward: 4.4\n",
      "episode: 2037   score: 3.0   memory length: 432710   epsilon: 0.34123222000985    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 4.4\n",
      "episode: 2038   score: 6.0   memory length: 433051   epsilon: 0.3405570400098457    steps: 341    lr: 6.400000000000001e-06     evaluation reward: 4.43\n",
      "episode: 2039   score: 2.0   memory length: 433271   epsilon: 0.34012144000984296    steps: 220    lr: 6.400000000000001e-06     evaluation reward: 4.41\n",
      "episode: 2040   score: 6.0   memory length: 433648   epsilon: 0.33937498000983823    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 4.41\n",
      "episode: 2041   score: 5.0   memory length: 433953   epsilon: 0.3387710800098344    steps: 305    lr: 6.400000000000001e-06     evaluation reward: 4.4\n",
      "episode: 2042   score: 5.0   memory length: 434242   epsilon: 0.3381988600098308    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 4.41\n",
      "episode: 2043   score: 4.0   memory length: 434522   epsilon: 0.3376444600098273    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 4.42\n",
      "episode: 2044   score: 11.0   memory length: 435091   epsilon: 0.33651784000982016    steps: 569    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2045   score: 3.0   memory length: 435303   epsilon: 0.3360980800098175    steps: 212    lr: 6.400000000000001e-06     evaluation reward: 4.47\n",
      "episode: 2046   score: 10.0   memory length: 435696   epsilon: 0.3353199400098126    steps: 393    lr: 6.400000000000001e-06     evaluation reward: 4.53\n",
      "episode: 2047   score: 10.0   memory length: 436229   epsilon: 0.3342646000098059    steps: 533    lr: 6.400000000000001e-06     evaluation reward: 4.61\n",
      "episode: 2048   score: 10.0   memory length: 436712   epsilon: 0.33330826000979985    steps: 483    lr: 6.400000000000001e-06     evaluation reward: 4.67\n",
      "episode: 2049   score: 5.0   memory length: 437035   epsilon: 0.3326687200097958    steps: 323    lr: 6.400000000000001e-06     evaluation reward: 4.69\n",
      "episode: 2050   score: 4.0   memory length: 437296   epsilon: 0.33215194000979253    steps: 261    lr: 6.400000000000001e-06     evaluation reward: 4.65\n",
      "episode: 2051   score: 5.0   memory length: 437621   epsilon: 0.33150844000978846    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 4.66\n",
      "episode: 2052   score: 6.0   memory length: 437978   epsilon: 0.330801580009784    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 4.68\n",
      "episode: 2053   score: 3.0   memory length: 438207   epsilon: 0.3303481600097811    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 4.65\n",
      "episode: 2054   score: 6.0   memory length: 438581   epsilon: 0.32960764000977644    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 4.66\n",
      "episode: 2055   score: 4.0   memory length: 438839   epsilon: 0.3290968000097732    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 4.64\n",
      "episode: 2056   score: 2.0   memory length: 439020   epsilon: 0.32873842000977094    steps: 181    lr: 6.400000000000001e-06     evaluation reward: 4.61\n",
      "episode: 2057   score: 5.0   memory length: 439294   epsilon: 0.3281959000097675    steps: 274    lr: 6.400000000000001e-06     evaluation reward: 4.64\n",
      "episode: 2058   score: 2.0   memory length: 439475   epsilon: 0.32783752000976524    steps: 181    lr: 6.400000000000001e-06     evaluation reward: 4.62\n",
      "episode: 2059   score: 2.0   memory length: 439657   epsilon: 0.32747716000976296    steps: 182    lr: 6.400000000000001e-06     evaluation reward: 4.61\n",
      "episode: 2060   score: 7.0   memory length: 440051   epsilon: 0.326697040009758    steps: 394    lr: 6.400000000000001e-06     evaluation reward: 4.61\n",
      "episode: 2061   score: 8.0   memory length: 440505   epsilon: 0.32579812000975233    steps: 454    lr: 6.400000000000001e-06     evaluation reward: 4.68\n",
      "episode: 2062   score: 8.0   memory length: 440920   epsilon: 0.32497642000974714    steps: 415    lr: 6.400000000000001e-06     evaluation reward: 4.7\n",
      "episode: 2063   score: 4.0   memory length: 441194   epsilon: 0.3244339000097437    steps: 274    lr: 6.400000000000001e-06     evaluation reward: 4.68\n",
      "episode: 2064   score: 14.0   memory length: 441557   epsilon: 0.32371516000973916    steps: 363    lr: 6.400000000000001e-06     evaluation reward: 4.78\n",
      "episode: 2065   score: 3.0   memory length: 441768   epsilon: 0.3232973800097365    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 4.78\n",
      "episode: 2066   score: 3.0   memory length: 441977   epsilon: 0.3228835600097339    steps: 209    lr: 6.400000000000001e-06     evaluation reward: 4.77\n",
      "episode: 2067   score: 3.0   memory length: 442224   epsilon: 0.3223945000097308    steps: 247    lr: 6.400000000000001e-06     evaluation reward: 4.75\n",
      "episode: 2068   score: 3.0   memory length: 442453   epsilon: 0.32194108000972793    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 4.72\n",
      "episode: 2069   score: 3.0   memory length: 442679   epsilon: 0.3214936000097251    steps: 226    lr: 6.400000000000001e-06     evaluation reward: 4.72\n",
      "episode: 2070   score: 3.0   memory length: 442892   epsilon: 0.32107186000972243    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 4.73\n",
      "episode: 2071   score: 4.0   memory length: 443192   epsilon: 0.3204778600097187    steps: 300    lr: 6.400000000000001e-06     evaluation reward: 4.76\n",
      "episode: 2072   score: 4.0   memory length: 443465   epsilon: 0.31993732000971525    steps: 273    lr: 6.400000000000001e-06     evaluation reward: 4.72\n",
      "episode: 2073   score: 4.0   memory length: 443720   epsilon: 0.31943242000971206    steps: 255    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2074   score: 2.0   memory length: 443939   epsilon: 0.3189988000097093    steps: 219    lr: 6.400000000000001e-06     evaluation reward: 4.67\n",
      "episode: 2075   score: 5.0   memory length: 444248   epsilon: 0.31838698000970544    steps: 309    lr: 6.400000000000001e-06     evaluation reward: 4.69\n",
      "episode: 2076   score: 4.0   memory length: 444496   epsilon: 0.31789594000970234    steps: 248    lr: 6.400000000000001e-06     evaluation reward: 4.65\n",
      "episode: 2077   score: 5.0   memory length: 444820   epsilon: 0.3172544200096983    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 4.66\n",
      "episode: 2078   score: 7.0   memory length: 445191   epsilon: 0.31651984000969363    steps: 371    lr: 6.400000000000001e-06     evaluation reward: 4.67\n",
      "episode: 2079   score: 5.0   memory length: 445534   epsilon: 0.31584070000968933    steps: 343    lr: 6.400000000000001e-06     evaluation reward: 4.63\n",
      "episode: 2080   score: 3.0   memory length: 445763   epsilon: 0.31538728000968647    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 4.63\n",
      "episode: 2081   score: 6.0   memory length: 446161   epsilon: 0.3145992400096815    steps: 398    lr: 6.400000000000001e-06     evaluation reward: 4.65\n",
      "episode: 2082   score: 6.0   memory length: 446482   epsilon: 0.31396366000967746    steps: 321    lr: 6.400000000000001e-06     evaluation reward: 4.65\n",
      "episode: 2083   score: 8.0   memory length: 446939   epsilon: 0.31305880000967173    steps: 457    lr: 6.400000000000001e-06     evaluation reward: 4.69\n",
      "episode: 2084   score: 6.0   memory length: 447313   epsilon: 0.31231828000966705    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 4.72\n",
      "episode: 2085   score: 4.0   memory length: 447559   epsilon: 0.31183120000966397    steps: 246    lr: 6.400000000000001e-06     evaluation reward: 4.74\n",
      "episode: 2086   score: 2.0   memory length: 447740   epsilon: 0.3114728200096617    steps: 181    lr: 6.400000000000001e-06     evaluation reward: 4.67\n",
      "episode: 2087   score: 5.0   memory length: 448050   epsilon: 0.3108590200096578    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 4.68\n",
      "episode: 2088   score: 7.0   memory length: 448474   epsilon: 0.3100195000096525    steps: 424    lr: 6.400000000000001e-06     evaluation reward: 4.64\n",
      "episode: 2089   score: 4.0   memory length: 448751   epsilon: 0.30947104000964903    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 4.65\n",
      "episode: 2090   score: 5.0   memory length: 449057   epsilon: 0.3088651600096452    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 4.69\n",
      "episode: 2091   score: 4.0   memory length: 449301   epsilon: 0.30838204000964214    steps: 244    lr: 6.400000000000001e-06     evaluation reward: 4.73\n",
      "episode: 2092   score: 2.0   memory length: 449501   epsilon: 0.30798604000963964    steps: 200    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2093   score: 6.0   memory length: 449823   epsilon: 0.3073484800096356    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 4.74\n",
      "episode: 2094   score: 3.0   memory length: 450089   epsilon: 0.3068218000096323    steps: 266    lr: 6.400000000000001e-06     evaluation reward: 4.72\n",
      "episode: 2095   score: 4.0   memory length: 450387   epsilon: 0.30623176000962854    steps: 298    lr: 6.400000000000001e-06     evaluation reward: 4.69\n",
      "episode: 2096   score: 4.0   memory length: 450681   epsilon: 0.30564964000962486    steps: 294    lr: 6.400000000000001e-06     evaluation reward: 4.67\n",
      "episode: 2097   score: 3.0   memory length: 450894   epsilon: 0.3052279000096222    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 4.67\n",
      "episode: 2098   score: 6.0   memory length: 451268   epsilon: 0.3044873800096175    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2099   score: 10.0   memory length: 451742   epsilon: 0.30354886000961157    steps: 474    lr: 6.400000000000001e-06     evaluation reward: 4.72\n",
      "episode: 2100   score: 2.0   memory length: 451924   epsilon: 0.3031885000096093    steps: 182    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2101   score: 3.0   memory length: 452134   epsilon: 0.30277270000960665    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 4.71\n",
      "episode: 2102   score: 5.0   memory length: 452431   epsilon: 0.30218464000960293    steps: 297    lr: 6.400000000000001e-06     evaluation reward: 4.74\n",
      "episode: 2103   score: 5.0   memory length: 452750   epsilon: 0.30155302000959894    steps: 319    lr: 6.400000000000001e-06     evaluation reward: 4.75\n",
      "episode: 2104   score: 6.0   memory length: 453089   epsilon: 0.3008818000095947    steps: 339    lr: 6.400000000000001e-06     evaluation reward: 4.78\n",
      "episode: 2105   score: 3.0   memory length: 453317   epsilon: 0.30043036000959183    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 4.76\n",
      "episode: 2106   score: 9.0   memory length: 453802   epsilon: 0.29947006000958576    steps: 485    lr: 6.400000000000001e-06     evaluation reward: 4.82\n",
      "episode: 2107   score: 2.0   memory length: 453987   epsilon: 0.29910376000958344    steps: 185    lr: 6.400000000000001e-06     evaluation reward: 4.81\n",
      "episode: 2108   score: 6.0   memory length: 454342   epsilon: 0.298400860009579    steps: 355    lr: 6.400000000000001e-06     evaluation reward: 4.83\n",
      "episode: 2109   score: 3.0   memory length: 454573   epsilon: 0.2979434800095761    steps: 231    lr: 6.400000000000001e-06     evaluation reward: 4.83\n",
      "episode: 2110   score: 8.0   memory length: 454996   epsilon: 0.2971059400095708    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 4.88\n",
      "episode: 2111   score: 8.0   memory length: 455403   epsilon: 0.2963000800095657    steps: 407    lr: 6.400000000000001e-06     evaluation reward: 4.91\n",
      "episode: 2112   score: 4.0   memory length: 455679   epsilon: 0.29575360000956225    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 4.9\n",
      "episode: 2113   score: 4.0   memory length: 455956   epsilon: 0.2952051400095588    steps: 277    lr: 6.400000000000001e-06     evaluation reward: 4.93\n",
      "episode: 2114   score: 4.0   memory length: 456234   epsilon: 0.2946547000095553    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 4.95\n",
      "episode: 2115   score: 7.0   memory length: 456609   epsilon: 0.2939122000095506    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 4.97\n",
      "episode: 2116   score: 6.0   memory length: 456966   epsilon: 0.2932053400095461    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 4.98\n",
      "episode: 2117   score: 5.0   memory length: 457291   epsilon: 0.29256184000954205    steps: 325    lr: 6.400000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2118   score: 4.0   memory length: 457550   epsilon: 0.2920490200095388    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 5.01\n",
      "episode: 2119   score: 6.0   memory length: 457891   epsilon: 0.29137384000953453    steps: 341    lr: 6.400000000000001e-06     evaluation reward: 4.99\n",
      "episode: 2120   score: 6.0   memory length: 458254   epsilon: 0.29065510000953    steps: 363    lr: 6.400000000000001e-06     evaluation reward: 5.01\n",
      "episode: 2121   score: 4.0   memory length: 458512   epsilon: 0.29014426000952676    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2122   score: 5.0   memory length: 458823   epsilon: 0.28952848000952286    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 4.99\n",
      "episode: 2123   score: 7.0   memory length: 459244   epsilon: 0.2886949000095176    steps: 421    lr: 6.400000000000001e-06     evaluation reward: 4.99\n",
      "episode: 2124   score: 4.0   memory length: 459484   epsilon: 0.2882197000095146    steps: 240    lr: 6.400000000000001e-06     evaluation reward: 4.98\n",
      "episode: 2125   score: 5.0   memory length: 459772   epsilon: 0.28764946000951097    steps: 288    lr: 6.400000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2126   score: 5.0   memory length: 460063   epsilon: 0.2870732800095073    steps: 291    lr: 6.400000000000001e-06     evaluation reward: 4.97\n",
      "episode: 2127   score: 2.0   memory length: 460243   epsilon: 0.28671688000950507    steps: 180    lr: 6.400000000000001e-06     evaluation reward: 4.93\n",
      "episode: 2128   score: 7.0   memory length: 460611   epsilon: 0.28598824000950046    steps: 368    lr: 6.400000000000001e-06     evaluation reward: 4.96\n",
      "episode: 2129   score: 6.0   memory length: 460965   epsilon: 0.285287320009496    steps: 354    lr: 6.400000000000001e-06     evaluation reward: 4.98\n",
      "episode: 2130   score: 5.0   memory length: 461293   epsilon: 0.2846378800094919    steps: 328    lr: 6.400000000000001e-06     evaluation reward: 4.96\n",
      "episode: 2131   score: 3.0   memory length: 461503   epsilon: 0.2842220800094893    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 4.96\n",
      "episode: 2132   score: 8.0   memory length: 461916   epsilon: 0.2834043400094841    steps: 413    lr: 6.400000000000001e-06     evaluation reward: 5.01\n",
      "episode: 2133   score: 9.0   memory length: 462344   epsilon: 0.28255690000947875    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 5.05\n",
      "episode: 2134   score: 6.0   memory length: 462674   epsilon: 0.2819035000094746    steps: 330    lr: 6.400000000000001e-06     evaluation reward: 5.05\n",
      "episode: 2135   score: 6.0   memory length: 463044   epsilon: 0.28117090000947    steps: 370    lr: 6.400000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2136   score: 4.0   memory length: 463299   epsilon: 0.2806660000094668    steps: 255    lr: 6.400000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2137   score: 4.0   memory length: 463547   epsilon: 0.2801749600094637    steps: 248    lr: 6.400000000000001e-06     evaluation reward: 5.09\n",
      "episode: 2138   score: 3.0   memory length: 463756   epsilon: 0.27976114000946106    steps: 209    lr: 6.400000000000001e-06     evaluation reward: 5.06\n",
      "episode: 2139   score: 4.0   memory length: 464023   epsilon: 0.2792324800094577    steps: 267    lr: 6.400000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2140   score: 3.0   memory length: 464234   epsilon: 0.2788147000094551    steps: 211    lr: 6.400000000000001e-06     evaluation reward: 5.05\n",
      "episode: 2141   score: 5.0   memory length: 464513   epsilon: 0.2782622800094516    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 5.05\n",
      "episode: 2142   score: 8.0   memory length: 464950   epsilon: 0.2773970200094461    steps: 437    lr: 6.400000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2143   score: 8.0   memory length: 465388   epsilon: 0.2765297800094406    steps: 438    lr: 6.400000000000001e-06     evaluation reward: 5.12\n",
      "episode: 2144   score: 6.0   memory length: 465761   epsilon: 0.27579124000943595    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2145   score: 4.0   memory length: 466036   epsilon: 0.2752467400094325    steps: 275    lr: 6.400000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2146   score: 3.0   memory length: 466265   epsilon: 0.27479332000942963    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 5.01\n",
      "episode: 2147   score: 1.0   memory length: 466416   epsilon: 0.27449434000942774    steps: 151    lr: 6.400000000000001e-06     evaluation reward: 4.92\n",
      "episode: 2148   score: 8.0   memory length: 466812   epsilon: 0.2737102600094228    steps: 396    lr: 6.400000000000001e-06     evaluation reward: 4.9\n",
      "episode: 2149   score: 5.0   memory length: 467097   epsilon: 0.2731459600094192    steps: 285    lr: 6.400000000000001e-06     evaluation reward: 4.9\n",
      "episode: 2150   score: 4.0   memory length: 467393   epsilon: 0.2725598800094155    steps: 296    lr: 6.400000000000001e-06     evaluation reward: 4.9\n",
      "episode: 2151   score: 6.0   memory length: 467727   epsilon: 0.2718985600094113    steps: 334    lr: 6.400000000000001e-06     evaluation reward: 4.91\n",
      "episode: 2152   score: 5.0   memory length: 468033   epsilon: 0.2712926800094075    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 4.9\n",
      "episode: 2153   score: 7.0   memory length: 468401   epsilon: 0.2705640400094029    steps: 368    lr: 6.400000000000001e-06     evaluation reward: 4.94\n",
      "episode: 2154   score: 5.0   memory length: 468707   epsilon: 0.26995816000939904    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 4.93\n",
      "episode: 2155   score: 4.0   memory length: 468965   epsilon: 0.2694473200093958    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 4.93\n",
      "episode: 2156   score: 10.0   memory length: 469445   epsilon: 0.2684969200093898    steps: 480    lr: 6.400000000000001e-06     evaluation reward: 5.01\n",
      "episode: 2157   score: 6.0   memory length: 469780   epsilon: 0.2678336200093856    steps: 335    lr: 6.400000000000001e-06     evaluation reward: 5.02\n",
      "episode: 2158   score: 5.0   memory length: 470128   epsilon: 0.26714458000938124    steps: 348    lr: 6.400000000000001e-06     evaluation reward: 5.05\n",
      "episode: 2159   score: 9.0   memory length: 470551   epsilon: 0.26630704000937594    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 5.12\n",
      "episode: 2160   score: 3.0   memory length: 470780   epsilon: 0.26585362000937307    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2161   score: 7.0   memory length: 471138   epsilon: 0.2651447800093686    steps: 358    lr: 6.400000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2162   score: 13.0   memory length: 471584   epsilon: 0.264261700009363    steps: 446    lr: 6.400000000000001e-06     evaluation reward: 5.12\n",
      "episode: 2163   score: 3.0   memory length: 471812   epsilon: 0.26381026000936014    steps: 228    lr: 6.400000000000001e-06     evaluation reward: 5.11\n",
      "episode: 2164   score: 2.0   memory length: 471992   epsilon: 0.2634538600093579    steps: 180    lr: 6.400000000000001e-06     evaluation reward: 4.99\n",
      "episode: 2165   score: 4.0   memory length: 472270   epsilon: 0.2629034200093544    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2166   score: 3.0   memory length: 472478   epsilon: 0.2624915800093518    steps: 208    lr: 6.400000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2167   score: 3.0   memory length: 472704   epsilon: 0.26204410000934897    steps: 226    lr: 6.400000000000001e-06     evaluation reward: 5.0\n",
      "episode: 2168   score: 5.0   memory length: 473015   epsilon: 0.26142832000934507    steps: 311    lr: 6.400000000000001e-06     evaluation reward: 5.02\n",
      "episode: 2169   score: 5.0   memory length: 473341   epsilon: 0.260782840009341    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 5.04\n",
      "episode: 2170   score: 6.0   memory length: 473668   epsilon: 0.2601353800093369    steps: 327    lr: 6.400000000000001e-06     evaluation reward: 5.07\n",
      "episode: 2171   score: 1.0   memory length: 473819   epsilon: 0.259836400009335    steps: 151    lr: 6.400000000000001e-06     evaluation reward: 5.04\n",
      "episode: 2172   score: 3.0   memory length: 474028   epsilon: 0.2594225800093324    steps: 209    lr: 6.400000000000001e-06     evaluation reward: 5.03\n",
      "episode: 2173   score: 6.0   memory length: 474402   epsilon: 0.2586820600093277    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 5.05\n",
      "episode: 2174   score: 6.0   memory length: 474747   epsilon: 0.2579989600093234    steps: 345    lr: 6.400000000000001e-06     evaluation reward: 5.09\n",
      "episode: 2175   score: 10.0   memory length: 475269   epsilon: 0.25696540000931684    steps: 522    lr: 6.400000000000001e-06     evaluation reward: 5.14\n",
      "episode: 2176   score: 5.0   memory length: 475571   epsilon: 0.25636744000931305    steps: 302    lr: 6.400000000000001e-06     evaluation reward: 5.15\n",
      "episode: 2177   score: 9.0   memory length: 476013   epsilon: 0.2554922800093075    steps: 442    lr: 6.400000000000001e-06     evaluation reward: 5.19\n",
      "episode: 2178   score: 5.0   memory length: 476312   epsilon: 0.25490026000930377    steps: 299    lr: 6.400000000000001e-06     evaluation reward: 5.17\n",
      "episode: 2179   score: 5.0   memory length: 476634   epsilon: 0.25426270000929974    steps: 322    lr: 6.400000000000001e-06     evaluation reward: 5.17\n",
      "episode: 2180   score: 6.0   memory length: 476960   epsilon: 0.25361722000929565    steps: 326    lr: 6.400000000000001e-06     evaluation reward: 5.2\n",
      "episode: 2181   score: 6.0   memory length: 477295   epsilon: 0.25295392000929146    steps: 335    lr: 6.400000000000001e-06     evaluation reward: 5.2\n",
      "episode: 2182   score: 3.0   memory length: 477524   epsilon: 0.2525005000092886    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 5.17\n",
      "episode: 2183   score: 4.0   memory length: 477765   epsilon: 0.25202332000928557    steps: 241    lr: 6.400000000000001e-06     evaluation reward: 5.13\n",
      "episode: 2184   score: 1.0   memory length: 477916   epsilon: 0.2517243400092837    steps: 151    lr: 6.400000000000001e-06     evaluation reward: 5.08\n",
      "episode: 2185   score: 5.0   memory length: 478224   epsilon: 0.2511145000092798    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 5.09\n",
      "episode: 2186   score: 10.0   memory length: 478675   epsilon: 0.25022152000927417    steps: 451    lr: 6.400000000000001e-06     evaluation reward: 5.17\n",
      "episode: 2187   score: 5.0   memory length: 478999   epsilon: 0.2495800000092701    steps: 324    lr: 6.400000000000001e-06     evaluation reward: 5.17\n",
      "episode: 2188   score: 7.0   memory length: 479355   epsilon: 0.24887512000926565    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 5.17\n",
      "episode: 2189   score: 4.0   memory length: 479613   epsilon: 0.24836428000926242    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 5.17\n",
      "episode: 2190   score: 3.0   memory length: 479840   epsilon: 0.24791482000925957    steps: 227    lr: 6.400000000000001e-06     evaluation reward: 5.15\n",
      "episode: 2191   score: 7.0   memory length: 480239   epsilon: 0.24712480000925457    steps: 399    lr: 6.400000000000001e-06     evaluation reward: 5.18\n",
      "episode: 2192   score: 6.0   memory length: 480595   epsilon: 0.24641992000925012    steps: 356    lr: 6.400000000000001e-06     evaluation reward: 5.22\n",
      "episode: 2193   score: 3.0   memory length: 480821   epsilon: 0.24597244000924728    steps: 226    lr: 6.400000000000001e-06     evaluation reward: 5.19\n",
      "episode: 2194   score: 6.0   memory length: 481196   epsilon: 0.2452299400092426    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 5.22\n",
      "episode: 2195   score: 6.0   memory length: 481532   epsilon: 0.24456466000923838    steps: 336    lr: 6.400000000000001e-06     evaluation reward: 5.24\n",
      "episode: 2196   score: 3.0   memory length: 481742   epsilon: 0.24414886000923575    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2197   score: 3.0   memory length: 481955   epsilon: 0.24372712000923308    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2198   score: 3.0   memory length: 482184   epsilon: 0.2432737000092302    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 5.2\n",
      "episode: 2199   score: 5.0   memory length: 482490   epsilon: 0.24266782000922638    steps: 306    lr: 6.400000000000001e-06     evaluation reward: 5.15\n",
      "episode: 2200   score: 10.0   memory length: 482997   epsilon: 0.24166396000922002    steps: 507    lr: 6.400000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2201   score: 5.0   memory length: 483278   epsilon: 0.2411075800092165    steps: 281    lr: 6.400000000000001e-06     evaluation reward: 5.25\n",
      "episode: 2202   score: 4.0   memory length: 483537   epsilon: 0.24059476000921326    steps: 259    lr: 6.400000000000001e-06     evaluation reward: 5.24\n",
      "episode: 2203   score: 8.0   memory length: 483962   epsilon: 0.23975326000920794    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 5.27\n",
      "episode: 2204   score: 4.0   memory length: 484200   epsilon: 0.23928202000920495    steps: 238    lr: 6.400000000000001e-06     evaluation reward: 5.25\n",
      "episode: 2205   score: 5.0   memory length: 484479   epsilon: 0.23872960000920146    steps: 279    lr: 6.400000000000001e-06     evaluation reward: 5.27\n",
      "episode: 2206   score: 6.0   memory length: 484813   epsilon: 0.23806828000919728    steps: 334    lr: 6.400000000000001e-06     evaluation reward: 5.24\n",
      "episode: 2207   score: 6.0   memory length: 485149   epsilon: 0.23740300000919307    steps: 336    lr: 6.400000000000001e-06     evaluation reward: 5.28\n",
      "episode: 2208   score: 9.0   memory length: 485618   epsilon: 0.2364743800091872    steps: 469    lr: 6.400000000000001e-06     evaluation reward: 5.31\n",
      "episode: 2209   score: 3.0   memory length: 485866   epsilon: 0.23598334000918408    steps: 248    lr: 6.400000000000001e-06     evaluation reward: 5.31\n",
      "episode: 2210   score: 8.0   memory length: 486294   epsilon: 0.23513590000917872    steps: 428    lr: 6.400000000000001e-06     evaluation reward: 5.31\n",
      "episode: 2211   score: 8.0   memory length: 486720   epsilon: 0.23429242000917339    steps: 426    lr: 6.400000000000001e-06     evaluation reward: 5.31\n",
      "episode: 2212   score: 4.0   memory length: 486977   epsilon: 0.23378356000917017    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 5.31\n",
      "episode: 2213   score: 5.0   memory length: 487284   epsilon: 0.23317570000916632    steps: 307    lr: 6.400000000000001e-06     evaluation reward: 5.32\n",
      "episode: 2214   score: 6.0   memory length: 487645   epsilon: 0.2324609200091618    steps: 361    lr: 6.400000000000001e-06     evaluation reward: 5.34\n",
      "episode: 2215   score: 6.0   memory length: 487955   epsilon: 0.23184712000915791    steps: 310    lr: 6.400000000000001e-06     evaluation reward: 5.33\n",
      "episode: 2216   score: 2.0   memory length: 488135   epsilon: 0.23149072000915566    steps: 180    lr: 6.400000000000001e-06     evaluation reward: 5.29\n",
      "episode: 2217   score: 3.0   memory length: 488345   epsilon: 0.23107492000915303    steps: 210    lr: 6.400000000000001e-06     evaluation reward: 5.27\n",
      "episode: 2218   score: 3.0   memory length: 488554   epsilon: 0.2306611000091504    steps: 209    lr: 6.400000000000001e-06     evaluation reward: 5.26\n",
      "episode: 2219   score: 6.0   memory length: 488911   epsilon: 0.22995424000914594    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 5.26\n",
      "episode: 2220   score: 4.0   memory length: 489169   epsilon: 0.2294434000091427    steps: 258    lr: 6.400000000000001e-06     evaluation reward: 5.24\n",
      "episode: 2221   score: 5.0   memory length: 489477   epsilon: 0.22883356000913885    steps: 308    lr: 6.400000000000001e-06     evaluation reward: 5.25\n",
      "episode: 2222   score: 3.0   memory length: 489690   epsilon: 0.22841182000913618    steps: 213    lr: 6.400000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2223   score: 6.0   memory length: 490041   epsilon: 0.22771684000913178    steps: 351    lr: 6.400000000000001e-06     evaluation reward: 5.22\n",
      "episode: 2224   score: 5.0   memory length: 490330   epsilon: 0.22714462000912816    steps: 289    lr: 6.400000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2225   score: 4.0   memory length: 490608   epsilon: 0.22659418000912468    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 5.22\n",
      "episode: 2226   score: 6.0   memory length: 491000   epsilon: 0.22581802000911977    steps: 392    lr: 6.400000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2227   score: 3.0   memory length: 491229   epsilon: 0.2253646000091169    steps: 229    lr: 6.400000000000001e-06     evaluation reward: 5.24\n",
      "episode: 2228   score: 6.0   memory length: 491604   epsilon: 0.2246221000091122    steps: 375    lr: 6.400000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2229   score: 7.0   memory length: 492009   epsilon: 0.22382020000910713    steps: 405    lr: 6.400000000000001e-06     evaluation reward: 5.24\n",
      "episode: 2230   score: 5.0   memory length: 492338   epsilon: 0.223168780009103    steps: 329    lr: 6.400000000000001e-06     evaluation reward: 5.24\n",
      "episode: 2231   score: 4.0   memory length: 492594   epsilon: 0.2226619000090998    steps: 256    lr: 6.400000000000001e-06     evaluation reward: 5.25\n",
      "episode: 2232   score: 4.0   memory length: 492870   epsilon: 0.22211542000909634    steps: 276    lr: 6.400000000000001e-06     evaluation reward: 5.21\n",
      "episode: 2233   score: 5.0   memory length: 493162   epsilon: 0.22153726000909268    steps: 292    lr: 6.400000000000001e-06     evaluation reward: 5.17\n",
      "episode: 2234   score: 4.0   memory length: 493419   epsilon: 0.22102840000908947    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 5.15\n",
      "episode: 2235   score: 6.0   memory length: 493776   epsilon: 0.220321540009085    steps: 357    lr: 6.400000000000001e-06     evaluation reward: 5.15\n",
      "episode: 2236   score: 7.0   memory length: 494178   epsilon: 0.21952558000907996    steps: 402    lr: 6.400000000000001e-06     evaluation reward: 5.18\n",
      "episode: 2237   score: 6.0   memory length: 494527   epsilon: 0.21883456000907558    steps: 349    lr: 6.400000000000001e-06     evaluation reward: 5.2\n",
      "episode: 2238   score: 6.0   memory length: 494829   epsilon: 0.2182366000090718    steps: 302    lr: 6.400000000000001e-06     evaluation reward: 5.23\n",
      "episode: 2239   score: 8.0   memory length: 495229   epsilon: 0.2174446000090668    steps: 400    lr: 6.400000000000001e-06     evaluation reward: 5.27\n",
      "episode: 2240   score: 7.0   memory length: 495652   epsilon: 0.2166070600090615    steps: 423    lr: 6.400000000000001e-06     evaluation reward: 5.31\n",
      "episode: 2241   score: 6.0   memory length: 496025   epsilon: 0.21586852000905682    steps: 373    lr: 6.400000000000001e-06     evaluation reward: 5.32\n",
      "episode: 2242   score: 6.0   memory length: 496399   epsilon: 0.21512800000905213    steps: 374    lr: 6.400000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2243   score: 8.0   memory length: 496824   epsilon: 0.2142865000090468    steps: 425    lr: 6.400000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2244   score: 4.0   memory length: 497102   epsilon: 0.21373606000904333    steps: 278    lr: 6.400000000000001e-06     evaluation reward: 5.28\n",
      "episode: 2245   score: 7.0   memory length: 497471   epsilon: 0.2130054400090387    steps: 369    lr: 6.400000000000001e-06     evaluation reward: 5.31\n",
      "episode: 2246   score: 3.0   memory length: 497680   epsilon: 0.2125916200090361    steps: 209    lr: 6.400000000000001e-06     evaluation reward: 5.31\n",
      "episode: 2247   score: 4.0   memory length: 497937   epsilon: 0.21208276000903287    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 5.34\n",
      "episode: 2248   score: 4.0   memory length: 498194   epsilon: 0.21157390000902965    steps: 257    lr: 6.400000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2249   score: 4.0   memory length: 498474   epsilon: 0.21101950000902614    steps: 280    lr: 6.400000000000001e-06     evaluation reward: 5.29\n",
      "episode: 2250   score: 8.0   memory length: 498869   epsilon: 0.2102374000090212    steps: 395    lr: 6.400000000000001e-06     evaluation reward: 5.33\n",
      "episode: 2251   score: 6.0   memory length: 499246   epsilon: 0.20949094000901647    steps: 377    lr: 6.400000000000001e-06     evaluation reward: 5.33\n",
      "episode: 2252   score: 3.0   memory length: 499473   epsilon: 0.20904148000901362    steps: 227    lr: 6.400000000000001e-06     evaluation reward: 5.31\n",
      "episode: 2253   score: 4.0   memory length: 499729   epsilon: 0.20853460000901042    steps: 256    lr: 6.400000000000001e-06     evaluation reward: 5.28\n",
      "episode: 2254   score: 7.0   memory length: 500149   epsilon: 0.20770300000900516    steps: 420    lr: 2.560000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2255   score: 4.0   memory length: 500409   epsilon: 0.2071882000090019    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2256   score: 10.0   memory length: 500933   epsilon: 0.20615068000899534    steps: 524    lr: 2.560000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2257   score: 8.0   memory length: 501385   epsilon: 0.20525572000898967    steps: 452    lr: 2.560000000000001e-06     evaluation reward: 5.32\n",
      "episode: 2258   score: 3.0   memory length: 501613   epsilon: 0.20480428000898682    steps: 228    lr: 2.560000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2259   score: 4.0   memory length: 501870   epsilon: 0.2042954200089836    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.25\n",
      "episode: 2260   score: 8.0   memory length: 502286   epsilon: 0.20347174000897839    steps: 416    lr: 2.560000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2261   score: 4.0   memory length: 502562   epsilon: 0.20292526000897493    steps: 276    lr: 2.560000000000001e-06     evaluation reward: 5.27\n",
      "episode: 2262   score: 4.0   memory length: 502837   epsilon: 0.20238076000897148    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 5.18\n",
      "episode: 2263   score: 4.0   memory length: 503110   epsilon: 0.20184022000896806    steps: 273    lr: 2.560000000000001e-06     evaluation reward: 5.19\n",
      "episode: 2264   score: 8.0   memory length: 503486   epsilon: 0.20109574000896335    steps: 376    lr: 2.560000000000001e-06     evaluation reward: 5.25\n",
      "episode: 2265   score: 3.0   memory length: 503717   epsilon: 0.20063836000896046    steps: 231    lr: 2.560000000000001e-06     evaluation reward: 5.24\n",
      "episode: 2266   score: 9.0   memory length: 504192   epsilon: 0.1996978600089545    steps: 475    lr: 2.560000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2267   score: 9.0   memory length: 504656   epsilon: 0.1987791400089487    steps: 464    lr: 2.560000000000001e-06     evaluation reward: 5.36\n",
      "episode: 2268   score: 6.0   memory length: 505008   epsilon: 0.1980821800089443    steps: 352    lr: 2.560000000000001e-06     evaluation reward: 5.37\n",
      "episode: 2269   score: 3.0   memory length: 505235   epsilon: 0.19763272000894144    steps: 227    lr: 2.560000000000001e-06     evaluation reward: 5.35\n",
      "episode: 2270   score: 4.0   memory length: 505530   epsilon: 0.19704862000893775    steps: 295    lr: 2.560000000000001e-06     evaluation reward: 5.33\n",
      "episode: 2271   score: 3.0   memory length: 505740   epsilon: 0.19663282000893512    steps: 210    lr: 2.560000000000001e-06     evaluation reward: 5.35\n",
      "episode: 2272   score: 4.0   memory length: 506018   epsilon: 0.19608238000893163    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 5.36\n",
      "episode: 2273   score: 6.0   memory length: 506374   epsilon: 0.19537750000892717    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 5.36\n",
      "episode: 2274   score: 3.0   memory length: 506607   epsilon: 0.19491616000892426    steps: 233    lr: 2.560000000000001e-06     evaluation reward: 5.33\n",
      "episode: 2275   score: 6.0   memory length: 506978   epsilon: 0.1941815800089196    steps: 371    lr: 2.560000000000001e-06     evaluation reward: 5.29\n",
      "episode: 2276   score: 6.0   memory length: 507298   epsilon: 0.1935479800089156    steps: 320    lr: 2.560000000000001e-06     evaluation reward: 5.3\n",
      "episode: 2277   score: 7.0   memory length: 507722   epsilon: 0.1927084600089103    steps: 424    lr: 2.560000000000001e-06     evaluation reward: 5.28\n",
      "episode: 2278   score: 10.0   memory length: 508200   epsilon: 0.1917620200089043    steps: 478    lr: 2.560000000000001e-06     evaluation reward: 5.33\n",
      "episode: 2279   score: 9.0   memory length: 508651   epsilon: 0.19086904000889865    steps: 451    lr: 2.560000000000001e-06     evaluation reward: 5.37\n",
      "episode: 2280   score: 6.0   memory length: 508979   epsilon: 0.19021960000889454    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 5.37\n",
      "episode: 2281   score: 7.0   memory length: 509365   epsilon: 0.1894553200088897    steps: 386    lr: 2.560000000000001e-06     evaluation reward: 5.38\n",
      "episode: 2282   score: 8.0   memory length: 509806   epsilon: 0.18858214000888418    steps: 441    lr: 2.560000000000001e-06     evaluation reward: 5.43\n",
      "episode: 2283   score: 7.0   memory length: 510211   epsilon: 0.1877802400088791    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 5.46\n",
      "episode: 2284   score: 5.0   memory length: 510499   epsilon: 0.1872100000088755    steps: 288    lr: 2.560000000000001e-06     evaluation reward: 5.5\n",
      "episode: 2285   score: 5.0   memory length: 510802   epsilon: 0.1866100600088717    steps: 303    lr: 2.560000000000001e-06     evaluation reward: 5.5\n",
      "episode: 2286   score: 5.0   memory length: 511096   epsilon: 0.18602794000886802    steps: 294    lr: 2.560000000000001e-06     evaluation reward: 5.45\n",
      "episode: 2287   score: 4.0   memory length: 511377   epsilon: 0.1854715600088645    steps: 281    lr: 2.560000000000001e-06     evaluation reward: 5.44\n",
      "episode: 2288   score: 2.0   memory length: 511557   epsilon: 0.18511516000886225    steps: 180    lr: 2.560000000000001e-06     evaluation reward: 5.39\n",
      "episode: 2289   score: 11.0   memory length: 512063   epsilon: 0.1841132800088559    steps: 506    lr: 2.560000000000001e-06     evaluation reward: 5.46\n",
      "episode: 2290   score: 4.0   memory length: 512338   epsilon: 0.18356878000885246    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 5.47\n",
      "episode: 2291   score: 9.0   memory length: 512818   epsilon: 0.18261838000884645    steps: 480    lr: 2.560000000000001e-06     evaluation reward: 5.49\n",
      "episode: 2292   score: 7.0   memory length: 513172   epsilon: 0.181917460008842    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 5.5\n",
      "episode: 2293   score: 5.0   memory length: 513514   epsilon: 0.18124030000883773    steps: 342    lr: 2.560000000000001e-06     evaluation reward: 5.52\n",
      "episode: 2294   score: 4.0   memory length: 513771   epsilon: 0.1807314400088345    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.5\n",
      "episode: 2295   score: 4.0   memory length: 514028   epsilon: 0.1802225800088313    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.48\n",
      "episode: 2296   score: 5.0   memory length: 514353   epsilon: 0.17957908000882722    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 5.5\n",
      "episode: 2297   score: 4.0   memory length: 514628   epsilon: 0.17903458000882377    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 5.51\n",
      "episode: 2298   score: 7.0   memory length: 515009   epsilon: 0.178280200008819    steps: 381    lr: 2.560000000000001e-06     evaluation reward: 5.55\n",
      "episode: 2299   score: 4.0   memory length: 515266   epsilon: 0.17777134000881578    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.54\n",
      "episode: 2300   score: 9.0   memory length: 515760   epsilon: 0.1767932200088096    steps: 494    lr: 2.560000000000001e-06     evaluation reward: 5.53\n",
      "episode: 2301   score: 6.0   memory length: 516083   epsilon: 0.17615368000880555    steps: 323    lr: 2.560000000000001e-06     evaluation reward: 5.54\n",
      "episode: 2302   score: 5.0   memory length: 516371   epsilon: 0.17558344000880194    steps: 288    lr: 2.560000000000001e-06     evaluation reward: 5.55\n",
      "episode: 2303   score: 6.0   memory length: 516710   epsilon: 0.1749122200087977    steps: 339    lr: 2.560000000000001e-06     evaluation reward: 5.53\n",
      "episode: 2304   score: 7.0   memory length: 517097   epsilon: 0.17414596000879284    steps: 387    lr: 2.560000000000001e-06     evaluation reward: 5.56\n",
      "episode: 2305   score: 5.0   memory length: 517410   epsilon: 0.17352622000878892    steps: 313    lr: 2.560000000000001e-06     evaluation reward: 5.56\n",
      "episode: 2306   score: 8.0   memory length: 517828   epsilon: 0.1726985800087837    steps: 418    lr: 2.560000000000001e-06     evaluation reward: 5.58\n",
      "episode: 2307   score: 4.0   memory length: 518085   epsilon: 0.17218972000878047    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.56\n",
      "episode: 2308   score: 6.0   memory length: 518407   epsilon: 0.17155216000877643    steps: 322    lr: 2.560000000000001e-06     evaluation reward: 5.53\n",
      "episode: 2309   score: 9.0   memory length: 518825   epsilon: 0.1707245200087712    steps: 418    lr: 2.560000000000001e-06     evaluation reward: 5.59\n",
      "episode: 2310   score: 9.0   memory length: 519290   epsilon: 0.16980382000876537    steps: 465    lr: 2.560000000000001e-06     evaluation reward: 5.6\n",
      "episode: 2311   score: 6.0   memory length: 519625   epsilon: 0.16914052000876117    steps: 335    lr: 2.560000000000001e-06     evaluation reward: 5.58\n",
      "episode: 2312   score: 2.0   memory length: 519805   epsilon: 0.16878412000875892    steps: 180    lr: 2.560000000000001e-06     evaluation reward: 5.56\n",
      "episode: 2313   score: 3.0   memory length: 520034   epsilon: 0.16833070000875605    steps: 229    lr: 2.560000000000001e-06     evaluation reward: 5.54\n",
      "episode: 2314   score: 7.0   memory length: 520425   epsilon: 0.16755652000875115    steps: 391    lr: 2.560000000000001e-06     evaluation reward: 5.55\n",
      "episode: 2315   score: 3.0   memory length: 520636   epsilon: 0.1671387400087485    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 5.52\n",
      "episode: 2316   score: 3.0   memory length: 520849   epsilon: 0.16671700000874584    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 5.53\n",
      "episode: 2317   score: 3.0   memory length: 521057   epsilon: 0.16630516000874324    steps: 208    lr: 2.560000000000001e-06     evaluation reward: 5.53\n",
      "episode: 2318   score: 4.0   memory length: 521299   epsilon: 0.1658260000087402    steps: 242    lr: 2.560000000000001e-06     evaluation reward: 5.54\n",
      "episode: 2319   score: 3.0   memory length: 521528   epsilon: 0.16537258000873734    steps: 229    lr: 2.560000000000001e-06     evaluation reward: 5.51\n",
      "episode: 2320   score: 8.0   memory length: 521925   epsilon: 0.16458652000873236    steps: 397    lr: 2.560000000000001e-06     evaluation reward: 5.55\n",
      "episode: 2321   score: 3.0   memory length: 522138   epsilon: 0.1641647800087297    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 5.53\n",
      "episode: 2322   score: 8.0   memory length: 522525   epsilon: 0.16339852000872485    steps: 387    lr: 2.560000000000001e-06     evaluation reward: 5.58\n",
      "episode: 2323   score: 5.0   memory length: 522833   epsilon: 0.162788680008721    steps: 308    lr: 2.560000000000001e-06     evaluation reward: 5.57\n",
      "episode: 2324   score: 4.0   memory length: 523081   epsilon: 0.16229764000871788    steps: 248    lr: 2.560000000000001e-06     evaluation reward: 5.56\n",
      "episode: 2325   score: 3.0   memory length: 523308   epsilon: 0.16184818000871504    steps: 227    lr: 2.560000000000001e-06     evaluation reward: 5.55\n",
      "episode: 2326   score: 6.0   memory length: 523635   epsilon: 0.16120072000871094    steps: 327    lr: 2.560000000000001e-06     evaluation reward: 5.55\n",
      "episode: 2327   score: 7.0   memory length: 523992   epsilon: 0.16049386000870647    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 5.59\n",
      "episode: 2328   score: 5.0   memory length: 524303   epsilon: 0.15987808000870257    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 5.58\n",
      "episode: 2329   score: 11.0   memory length: 524854   epsilon: 0.15878710000869567    steps: 551    lr: 2.560000000000001e-06     evaluation reward: 5.62\n",
      "episode: 2330   score: 8.0   memory length: 525287   epsilon: 0.15792976000869025    steps: 433    lr: 2.560000000000001e-06     evaluation reward: 5.65\n",
      "episode: 2331   score: 12.0   memory length: 525819   epsilon: 0.15687640000868358    steps: 532    lr: 2.560000000000001e-06     evaluation reward: 5.73\n",
      "episode: 2332   score: 3.0   memory length: 526068   epsilon: 0.15638338000868046    steps: 249    lr: 2.560000000000001e-06     evaluation reward: 5.72\n",
      "episode: 2333   score: 5.0   memory length: 526394   epsilon: 0.15573790000867638    steps: 326    lr: 2.560000000000001e-06     evaluation reward: 5.72\n",
      "episode: 2334   score: 10.0   memory length: 526901   epsilon: 0.15473404000867003    steps: 507    lr: 2.560000000000001e-06     evaluation reward: 5.78\n",
      "episode: 2335   score: 7.0   memory length: 527311   epsilon: 0.1539222400086649    steps: 410    lr: 2.560000000000001e-06     evaluation reward: 5.79\n",
      "episode: 2336   score: 6.0   memory length: 527639   epsilon: 0.15327280000866078    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 5.78\n",
      "episode: 2337   score: 4.0   memory length: 527897   epsilon: 0.15276196000865755    steps: 258    lr: 2.560000000000001e-06     evaluation reward: 5.76\n",
      "episode: 2338   score: 4.0   memory length: 528173   epsilon: 0.1522154800086541    steps: 276    lr: 2.560000000000001e-06     evaluation reward: 5.74\n",
      "episode: 2339   score: 4.0   memory length: 528414   epsilon: 0.15173830000865107    steps: 241    lr: 2.560000000000001e-06     evaluation reward: 5.7\n",
      "episode: 2340   score: 5.0   memory length: 528725   epsilon: 0.15112252000864718    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 5.68\n",
      "episode: 2341   score: 4.0   memory length: 529000   epsilon: 0.15057802000864373    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 5.66\n",
      "episode: 2342   score: 3.0   memory length: 529211   epsilon: 0.1501602400086411    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 5.63\n",
      "episode: 2343   score: 5.0   memory length: 529536   epsilon: 0.14951674000863702    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 5.6\n",
      "episode: 2344   score: 4.0   memory length: 529814   epsilon: 0.14896630000863353    steps: 278    lr: 2.560000000000001e-06     evaluation reward: 5.6\n",
      "episode: 2345   score: 11.0   memory length: 530407   epsilon: 0.1477921600086261    steps: 593    lr: 2.560000000000001e-06     evaluation reward: 5.64\n",
      "episode: 2346   score: 3.0   memory length: 530620   epsilon: 0.14737042000862344    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 5.64\n",
      "episode: 2347   score: 6.0   memory length: 530949   epsilon: 0.14671900000861932    steps: 329    lr: 2.560000000000001e-06     evaluation reward: 5.66\n",
      "episode: 2348   score: 4.0   memory length: 531206   epsilon: 0.1462101400086161    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.66\n",
      "episode: 2349   score: 4.0   memory length: 531464   epsilon: 0.14569930000861286    steps: 258    lr: 2.560000000000001e-06     evaluation reward: 5.66\n",
      "episode: 2350   score: 3.0   memory length: 531674   epsilon: 0.14528350000861023    steps: 210    lr: 2.560000000000001e-06     evaluation reward: 5.61\n",
      "episode: 2351   score: 5.0   memory length: 531997   epsilon: 0.1446439600086062    steps: 323    lr: 2.560000000000001e-06     evaluation reward: 5.6\n",
      "episode: 2352   score: 9.0   memory length: 532441   epsilon: 0.14376484000860063    steps: 444    lr: 2.560000000000001e-06     evaluation reward: 5.66\n",
      "episode: 2353   score: 4.0   memory length: 532701   epsilon: 0.14325004000859737    steps: 260    lr: 2.560000000000001e-06     evaluation reward: 5.66\n",
      "episode: 2354   score: 4.0   memory length: 532960   epsilon: 0.14273722000859412    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 5.63\n",
      "episode: 2355   score: 6.0   memory length: 533298   epsilon: 0.1420679800085899    steps: 338    lr: 2.560000000000001e-06     evaluation reward: 5.65\n",
      "episode: 2356   score: 7.0   memory length: 533634   epsilon: 0.14140270000858568    steps: 336    lr: 2.560000000000001e-06     evaluation reward: 5.62\n",
      "episode: 2357   score: 6.0   memory length: 534011   epsilon: 0.14065624000858096    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 5.6\n",
      "episode: 2358   score: 5.0   memory length: 534319   epsilon: 0.1400464000085771    steps: 308    lr: 2.560000000000001e-06     evaluation reward: 5.62\n",
      "episode: 2359   score: 6.0   memory length: 534630   epsilon: 0.1394306200085732    steps: 311    lr: 2.560000000000001e-06     evaluation reward: 5.64\n",
      "episode: 2360   score: 3.0   memory length: 534859   epsilon: 0.13897720000857033    steps: 229    lr: 2.560000000000001e-06     evaluation reward: 5.59\n",
      "episode: 2361   score: 3.0   memory length: 535088   epsilon: 0.13852378000856747    steps: 229    lr: 2.560000000000001e-06     evaluation reward: 5.58\n",
      "episode: 2362   score: 3.0   memory length: 535299   epsilon: 0.13810600000856482    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 5.57\n",
      "episode: 2363   score: 11.0   memory length: 535843   epsilon: 0.137028880008558    steps: 544    lr: 2.560000000000001e-06     evaluation reward: 5.64\n",
      "episode: 2364   score: 4.0   memory length: 536100   epsilon: 0.1365200200085548    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.6\n",
      "episode: 2365   score: 6.0   memory length: 536461   epsilon: 0.13580524000855027    steps: 361    lr: 2.560000000000001e-06     evaluation reward: 5.63\n",
      "episode: 2366   score: 2.0   memory length: 536661   epsilon: 0.13540924000854776    steps: 200    lr: 2.560000000000001e-06     evaluation reward: 5.56\n",
      "episode: 2367   score: 6.0   memory length: 536983   epsilon: 0.13477168000854373    steps: 322    lr: 2.560000000000001e-06     evaluation reward: 5.53\n",
      "episode: 2368   score: 8.0   memory length: 537391   epsilon: 0.13396384000853861    steps: 408    lr: 2.560000000000001e-06     evaluation reward: 5.55\n",
      "episode: 2369   score: 11.0   memory length: 537911   epsilon: 0.1329342400085321    steps: 520    lr: 2.560000000000001e-06     evaluation reward: 5.63\n",
      "episode: 2370   score: 6.0   memory length: 538250   epsilon: 0.13226302000852785    steps: 339    lr: 2.560000000000001e-06     evaluation reward: 5.65\n",
      "episode: 2371   score: 5.0   memory length: 538559   epsilon: 0.13165120000852398    steps: 309    lr: 2.560000000000001e-06     evaluation reward: 5.67\n",
      "episode: 2372   score: 8.0   memory length: 538966   epsilon: 0.13084534000851888    steps: 407    lr: 2.560000000000001e-06     evaluation reward: 5.71\n",
      "episode: 2373   score: 2.0   memory length: 539166   epsilon: 0.13044934000851638    steps: 200    lr: 2.560000000000001e-06     evaluation reward: 5.67\n",
      "episode: 2374   score: 7.0   memory length: 539571   epsilon: 0.1296474400085113    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 5.71\n",
      "episode: 2375   score: 5.0   memory length: 539852   epsilon: 0.12909106000850779    steps: 281    lr: 2.560000000000001e-06     evaluation reward: 5.7\n",
      "episode: 2376   score: 6.0   memory length: 540208   epsilon: 0.12838618000850333    steps: 356    lr: 2.560000000000001e-06     evaluation reward: 5.7\n",
      "episode: 2377   score: 5.0   memory length: 540535   epsilon: 0.12773872000849923    steps: 327    lr: 2.560000000000001e-06     evaluation reward: 5.68\n",
      "episode: 2378   score: 9.0   memory length: 540969   epsilon: 0.1268794000084938    steps: 434    lr: 2.560000000000001e-06     evaluation reward: 5.67\n",
      "episode: 2379   score: 8.0   memory length: 541376   epsilon: 0.1260735400084887    steps: 407    lr: 2.560000000000001e-06     evaluation reward: 5.66\n",
      "episode: 2380   score: 9.0   memory length: 541843   epsilon: 0.12514888000848284    steps: 467    lr: 2.560000000000001e-06     evaluation reward: 5.69\n",
      "episode: 2381   score: 9.0   memory length: 542274   epsilon: 0.12429550000848238    steps: 431    lr: 2.560000000000001e-06     evaluation reward: 5.71\n",
      "episode: 2382   score: 9.0   memory length: 542739   epsilon: 0.12337480000848301    steps: 465    lr: 2.560000000000001e-06     evaluation reward: 5.72\n",
      "episode: 2383   score: 9.0   memory length: 543204   epsilon: 0.12245410000848364    steps: 465    lr: 2.560000000000001e-06     evaluation reward: 5.74\n",
      "episode: 2384   score: 7.0   memory length: 543608   epsilon: 0.12165418000848419    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 5.76\n",
      "episode: 2385   score: 5.0   memory length: 543898   epsilon: 0.12107998000848458    steps: 290    lr: 2.560000000000001e-06     evaluation reward: 5.76\n",
      "episode: 2386   score: 5.0   memory length: 544207   epsilon: 0.120468160008485    steps: 309    lr: 2.560000000000001e-06     evaluation reward: 5.76\n",
      "episode: 2387   score: 3.0   memory length: 544436   epsilon: 0.1200147400084853    steps: 229    lr: 2.560000000000001e-06     evaluation reward: 5.75\n",
      "episode: 2388   score: 5.0   memory length: 544759   epsilon: 0.11937520000848574    steps: 323    lr: 2.560000000000001e-06     evaluation reward: 5.78\n",
      "episode: 2389   score: 3.0   memory length: 544988   epsilon: 0.11892178000848605    steps: 229    lr: 2.560000000000001e-06     evaluation reward: 5.7\n",
      "episode: 2390   score: 14.0   memory length: 545566   epsilon: 0.11777734000848683    steps: 578    lr: 2.560000000000001e-06     evaluation reward: 5.8\n",
      "episode: 2391   score: 4.0   memory length: 545841   epsilon: 0.1172328400084872    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 5.75\n",
      "episode: 2392   score: 5.0   memory length: 546121   epsilon: 0.11667844000848758    steps: 280    lr: 2.560000000000001e-06     evaluation reward: 5.73\n",
      "episode: 2393   score: 5.0   memory length: 546441   epsilon: 0.11604484000848801    steps: 320    lr: 2.560000000000001e-06     evaluation reward: 5.73\n",
      "episode: 2394   score: 9.0   memory length: 546911   epsilon: 0.11511424000848865    steps: 470    lr: 2.560000000000001e-06     evaluation reward: 5.78\n",
      "episode: 2395   score: 3.0   memory length: 547139   epsilon: 0.11466280000848895    steps: 228    lr: 2.560000000000001e-06     evaluation reward: 5.77\n",
      "episode: 2396   score: 10.0   memory length: 547640   epsilon: 0.11367082000848963    steps: 501    lr: 2.560000000000001e-06     evaluation reward: 5.82\n",
      "episode: 2397   score: 7.0   memory length: 547976   epsilon: 0.11300554000849009    steps: 336    lr: 2.560000000000001e-06     evaluation reward: 5.85\n",
      "episode: 2398   score: 5.0   memory length: 548283   epsilon: 0.1123976800084905    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 5.83\n",
      "episode: 2399   score: 7.0   memory length: 548640   epsilon: 0.11169082000849098    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 5.86\n",
      "episode: 2400   score: 5.0   memory length: 548929   epsilon: 0.11111860000849137    steps: 289    lr: 2.560000000000001e-06     evaluation reward: 5.82\n",
      "episode: 2401   score: 5.0   memory length: 549237   epsilon: 0.11050876000849179    steps: 308    lr: 2.560000000000001e-06     evaluation reward: 5.81\n",
      "episode: 2402   score: 4.0   memory length: 549496   epsilon: 0.10999594000849214    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 5.8\n",
      "episode: 2403   score: 12.0   memory length: 550085   epsilon: 0.10882972000849293    steps: 589    lr: 2.560000000000001e-06     evaluation reward: 5.86\n",
      "episode: 2404   score: 2.0   memory length: 550267   epsilon: 0.10846936000849318    steps: 182    lr: 2.560000000000001e-06     evaluation reward: 5.81\n",
      "episode: 2405   score: 4.0   memory length: 550542   epsilon: 0.10792486000849355    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 5.8\n",
      "episode: 2406   score: 6.0   memory length: 550883   epsilon: 0.10724968000849401    steps: 341    lr: 2.560000000000001e-06     evaluation reward: 5.78\n",
      "episode: 2407   score: 8.0   memory length: 551351   epsilon: 0.10632304000849464    steps: 468    lr: 2.560000000000001e-06     evaluation reward: 5.82\n",
      "episode: 2408   score: 4.0   memory length: 551626   epsilon: 0.10577854000849501    steps: 275    lr: 2.560000000000001e-06     evaluation reward: 5.8\n",
      "episode: 2409   score: 3.0   memory length: 551836   epsilon: 0.1053627400084953    steps: 210    lr: 2.560000000000001e-06     evaluation reward: 5.74\n",
      "episode: 2410   score: 4.0   memory length: 552095   epsilon: 0.10484992000849565    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 5.69\n",
      "episode: 2411   score: 4.0   memory length: 552352   epsilon: 0.104341060008496    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.67\n",
      "episode: 2412   score: 8.0   memory length: 552759   epsilon: 0.10353520000849654    steps: 407    lr: 2.560000000000001e-06     evaluation reward: 5.73\n",
      "episode: 2413   score: 8.0   memory length: 553189   epsilon: 0.10268380000849713    steps: 430    lr: 2.560000000000001e-06     evaluation reward: 5.78\n",
      "episode: 2414   score: 7.0   memory length: 553578   epsilon: 0.10191358000849765    steps: 389    lr: 2.560000000000001e-06     evaluation reward: 5.78\n",
      "episode: 2415   score: 4.0   memory length: 553833   epsilon: 0.101408680008498    steps: 255    lr: 2.560000000000001e-06     evaluation reward: 5.79\n",
      "episode: 2416   score: 4.0   memory length: 554090   epsilon: 0.10089982000849834    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.8\n",
      "episode: 2417   score: 7.0   memory length: 554447   epsilon: 0.10019296000849882    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 5.84\n",
      "episode: 2418   score: 3.0   memory length: 554660   epsilon: 0.09977122000849911    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 5.83\n",
      "episode: 2419   score: 4.0   memory length: 554937   epsilon: 0.09922276000849949    steps: 277    lr: 2.560000000000001e-06     evaluation reward: 5.84\n",
      "episode: 2420   score: 3.0   memory length: 555147   epsilon: 0.09880696000849977    steps: 210    lr: 2.560000000000001e-06     evaluation reward: 5.79\n",
      "episode: 2421   score: 3.0   memory length: 555360   epsilon: 0.09838522000850006    steps: 213    lr: 2.560000000000001e-06     evaluation reward: 5.79\n",
      "episode: 2422   score: 11.0   memory length: 555906   epsilon: 0.0973041400085008    steps: 546    lr: 2.560000000000001e-06     evaluation reward: 5.82\n",
      "episode: 2423   score: 5.0   memory length: 556215   epsilon: 0.09669232000850121    steps: 309    lr: 2.560000000000001e-06     evaluation reward: 5.82\n",
      "episode: 2424   score: 4.0   memory length: 556472   epsilon: 0.09618346000850156    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.82\n",
      "episode: 2425   score: 5.0   memory length: 556778   epsilon: 0.09557758000850197    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 5.84\n",
      "episode: 2426   score: 9.0   memory length: 557264   epsilon: 0.09461530000850263    steps: 486    lr: 2.560000000000001e-06     evaluation reward: 5.87\n",
      "episode: 2427   score: 2.0   memory length: 557444   epsilon: 0.09425890000850287    steps: 180    lr: 2.560000000000001e-06     evaluation reward: 5.82\n",
      "episode: 2428   score: 5.0   memory length: 557747   epsilon: 0.09365896000850328    steps: 303    lr: 2.560000000000001e-06     evaluation reward: 5.82\n",
      "episode: 2429   score: 6.0   memory length: 558124   epsilon: 0.09291250000850379    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 5.77\n",
      "episode: 2430   score: 9.0   memory length: 558561   epsilon: 0.09204724000850438    steps: 437    lr: 2.560000000000001e-06     evaluation reward: 5.78\n",
      "episode: 2431   score: 5.0   memory length: 558889   epsilon: 0.09139780000850482    steps: 328    lr: 2.560000000000001e-06     evaluation reward: 5.71\n",
      "episode: 2432   score: 7.0   memory length: 559293   epsilon: 0.09059788000850537    steps: 404    lr: 2.560000000000001e-06     evaluation reward: 5.75\n",
      "episode: 2433   score: 3.0   memory length: 559502   epsilon: 0.09018406000850565    steps: 209    lr: 2.560000000000001e-06     evaluation reward: 5.73\n",
      "episode: 2434   score: 6.0   memory length: 559822   epsilon: 0.08955046000850608    steps: 320    lr: 2.560000000000001e-06     evaluation reward: 5.69\n",
      "episode: 2435   score: 9.0   memory length: 560233   epsilon: 0.08873668000850664    steps: 411    lr: 2.560000000000001e-06     evaluation reward: 5.71\n",
      "episode: 2436   score: 6.0   memory length: 560588   epsilon: 0.08803378000850712    steps: 355    lr: 2.560000000000001e-06     evaluation reward: 5.71\n",
      "episode: 2437   score: 6.0   memory length: 560942   epsilon: 0.0873328600085076    steps: 354    lr: 2.560000000000001e-06     evaluation reward: 5.73\n",
      "episode: 2438   score: 4.0   memory length: 561199   epsilon: 0.08682400000850794    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.73\n",
      "episode: 2439   score: 4.0   memory length: 561475   epsilon: 0.08627752000850832    steps: 276    lr: 2.560000000000001e-06     evaluation reward: 5.73\n",
      "episode: 2440   score: 11.0   memory length: 561896   epsilon: 0.08544394000850888    steps: 421    lr: 2.560000000000001e-06     evaluation reward: 5.79\n",
      "episode: 2441   score: 1.0   memory length: 562047   epsilon: 0.08514496000850909    steps: 151    lr: 2.560000000000001e-06     evaluation reward: 5.76\n",
      "episode: 2442   score: 5.0   memory length: 562328   epsilon: 0.08458858000850947    steps: 281    lr: 2.560000000000001e-06     evaluation reward: 5.78\n",
      "episode: 2443   score: 11.0   memory length: 562838   epsilon: 0.08357878000851016    steps: 510    lr: 2.560000000000001e-06     evaluation reward: 5.84\n",
      "episode: 2444   score: 6.0   memory length: 563216   epsilon: 0.08283034000851067    steps: 378    lr: 2.560000000000001e-06     evaluation reward: 5.86\n",
      "episode: 2445   score: 10.0   memory length: 563716   epsilon: 0.08184034000851134    steps: 500    lr: 2.560000000000001e-06     evaluation reward: 5.85\n",
      "episode: 2446   score: 13.0   memory length: 564325   epsilon: 0.08063452000851216    steps: 609    lr: 2.560000000000001e-06     evaluation reward: 5.95\n",
      "episode: 2447   score: 4.0   memory length: 564582   epsilon: 0.08012566000851251    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 5.93\n",
      "episode: 2448   score: 5.0   memory length: 564873   epsilon: 0.0795494800085129    steps: 291    lr: 2.560000000000001e-06     evaluation reward: 5.94\n",
      "episode: 2449   score: 11.0   memory length: 565364   epsilon: 0.07857730000851357    steps: 491    lr: 2.560000000000001e-06     evaluation reward: 6.01\n",
      "episode: 2450   score: 7.0   memory length: 565741   epsilon: 0.07783084000851408    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 6.05\n",
      "episode: 2451   score: 7.0   memory length: 566164   epsilon: 0.07699330000851465    steps: 423    lr: 2.560000000000001e-06     evaluation reward: 6.07\n",
      "episode: 2452   score: 5.0   memory length: 566444   epsilon: 0.07643890000851503    steps: 280    lr: 2.560000000000001e-06     evaluation reward: 6.03\n",
      "episode: 2453   score: 4.0   memory length: 566701   epsilon: 0.07593004000851537    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 6.03\n",
      "episode: 2454   score: 7.0   memory length: 567079   epsilon: 0.07518160000851588    steps: 378    lr: 2.560000000000001e-06     evaluation reward: 6.06\n",
      "episode: 2455   score: 8.0   memory length: 567488   epsilon: 0.07437178000851644    steps: 409    lr: 2.560000000000001e-06     evaluation reward: 6.08\n",
      "episode: 2456   score: 7.0   memory length: 567893   epsilon: 0.07356988000851698    steps: 405    lr: 2.560000000000001e-06     evaluation reward: 6.08\n",
      "episode: 2457   score: 7.0   memory length: 568270   epsilon: 0.07282342000851749    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 6.09\n",
      "episode: 2458   score: 10.0   memory length: 568772   epsilon: 0.07182946000851817    steps: 502    lr: 2.560000000000001e-06     evaluation reward: 6.14\n",
      "episode: 2459   score: 4.0   memory length: 569031   epsilon: 0.07131664000851852    steps: 259    lr: 2.560000000000001e-06     evaluation reward: 6.12\n",
      "episode: 2460   score: 4.0   memory length: 569269   epsilon: 0.07084540000851884    steps: 238    lr: 2.560000000000001e-06     evaluation reward: 6.13\n",
      "episode: 2461   score: 6.0   memory length: 569578   epsilon: 0.07023358000851926    steps: 309    lr: 2.560000000000001e-06     evaluation reward: 6.16\n",
      "episode: 2462   score: 10.0   memory length: 570115   epsilon: 0.06917032000851998    steps: 537    lr: 2.560000000000001e-06     evaluation reward: 6.23\n",
      "episode: 2463   score: 13.0   memory length: 570673   epsilon: 0.06806548000852074    steps: 558    lr: 2.560000000000001e-06     evaluation reward: 6.25\n",
      "episode: 2464   score: 8.0   memory length: 571031   epsilon: 0.06735664000852122    steps: 358    lr: 2.560000000000001e-06     evaluation reward: 6.29\n",
      "episode: 2465   score: 9.0   memory length: 571475   epsilon: 0.06647752000852182    steps: 444    lr: 2.560000000000001e-06     evaluation reward: 6.32\n",
      "episode: 2466   score: 8.0   memory length: 571919   epsilon: 0.06559840000852242    steps: 444    lr: 2.560000000000001e-06     evaluation reward: 6.38\n",
      "episode: 2467   score: 8.0   memory length: 572358   epsilon: 0.06472918000852301    steps: 439    lr: 2.560000000000001e-06     evaluation reward: 6.4\n",
      "episode: 2468   score: 3.0   memory length: 572587   epsilon: 0.06427576000852332    steps: 229    lr: 2.560000000000001e-06     evaluation reward: 6.35\n",
      "episode: 2469   score: 15.0   memory length: 573060   epsilon: 0.06333922000852396    steps: 473    lr: 2.560000000000001e-06     evaluation reward: 6.39\n",
      "episode: 2470   score: 7.0   memory length: 573417   epsilon: 0.06263236000852444    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 6.4\n",
      "episode: 2471   score: 6.0   memory length: 573726   epsilon: 0.06202054000852486    steps: 309    lr: 2.560000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2472   score: 7.0   memory length: 574083   epsilon: 0.06131368000852534    steps: 357    lr: 2.560000000000001e-06     evaluation reward: 6.4\n",
      "episode: 2473   score: 6.0   memory length: 574392   epsilon: 0.06070186000852576    steps: 309    lr: 2.560000000000001e-06     evaluation reward: 6.44\n",
      "episode: 2474   score: 4.0   memory length: 574671   epsilon: 0.06014944000852614    steps: 279    lr: 2.560000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2475   score: 11.0   memory length: 575178   epsilon: 0.05914558000852682    steps: 507    lr: 2.560000000000001e-06     evaluation reward: 6.47\n",
      "episode: 2476   score: 6.0   memory length: 575520   epsilon: 0.05846842000852728    steps: 342    lr: 2.560000000000001e-06     evaluation reward: 6.47\n",
      "episode: 2477   score: 9.0   memory length: 575972   epsilon: 0.057573460008527894    steps: 452    lr: 2.560000000000001e-06     evaluation reward: 6.51\n",
      "episode: 2478   score: 11.0   memory length: 576478   epsilon: 0.05657158000852858    steps: 506    lr: 2.560000000000001e-06     evaluation reward: 6.53\n",
      "episode: 2479   score: 7.0   memory length: 576849   epsilon: 0.05583700000852908    steps: 371    lr: 2.560000000000001e-06     evaluation reward: 6.52\n",
      "episode: 2480   score: 7.0   memory length: 577255   epsilon: 0.055033120008529626    steps: 406    lr: 2.560000000000001e-06     evaluation reward: 6.5\n",
      "episode: 2481   score: 9.0   memory length: 577694   epsilon: 0.05416390000853022    steps: 439    lr: 2.560000000000001e-06     evaluation reward: 6.5\n",
      "episode: 2482   score: 3.0   memory length: 577923   epsilon: 0.05371048000853053    steps: 229    lr: 2.560000000000001e-06     evaluation reward: 6.44\n",
      "episode: 2483   score: 9.0   memory length: 578341   epsilon: 0.05288284000853109    steps: 418    lr: 2.560000000000001e-06     evaluation reward: 6.44\n",
      "episode: 2484   score: 4.0   memory length: 578614   epsilon: 0.05234230000853146    steps: 273    lr: 2.560000000000001e-06     evaluation reward: 6.41\n",
      "episode: 2485   score: 7.0   memory length: 578950   epsilon: 0.051677020008531915    steps: 336    lr: 2.560000000000001e-06     evaluation reward: 6.43\n",
      "episode: 2486   score: 7.0   memory length: 579324   epsilon: 0.05093650000853242    steps: 374    lr: 2.560000000000001e-06     evaluation reward: 6.45\n",
      "episode: 2487   score: 9.0   memory length: 579753   epsilon: 0.050087080008533    steps: 429    lr: 2.560000000000001e-06     evaluation reward: 6.51\n",
      "episode: 2488   score: 9.0   memory length: 580164   epsilon: 0.049273300008533555    steps: 411    lr: 2.560000000000001e-06     evaluation reward: 6.55\n",
      "episode: 2489   score: 11.0   memory length: 580660   epsilon: 0.048291220008534225    steps: 496    lr: 2.560000000000001e-06     evaluation reward: 6.63\n",
      "episode: 2490   score: 9.0   memory length: 581114   epsilon: 0.04739230000853484    steps: 454    lr: 2.560000000000001e-06     evaluation reward: 6.58\n",
      "episode: 2491   score: 10.0   memory length: 581553   epsilon: 0.04652308000853543    steps: 439    lr: 2.560000000000001e-06     evaluation reward: 6.64\n",
      "episode: 2492   score: 3.0   memory length: 581762   epsilon: 0.04610926000853571    steps: 209    lr: 2.560000000000001e-06     evaluation reward: 6.62\n",
      "episode: 2493   score: 9.0   memory length: 582195   epsilon: 0.0452519200085363    steps: 433    lr: 2.560000000000001e-06     evaluation reward: 6.66\n",
      "episode: 2494   score: 9.0   memory length: 582619   epsilon: 0.04441240000853687    steps: 424    lr: 2.560000000000001e-06     evaluation reward: 6.66\n",
      "episode: 2495   score: 10.0   memory length: 583104   epsilon: 0.043452100008537525    steps: 485    lr: 2.560000000000001e-06     evaluation reward: 6.73\n",
      "episode: 2496   score: 4.0   memory length: 583361   epsilon: 0.04294324000853787    steps: 257    lr: 2.560000000000001e-06     evaluation reward: 6.67\n",
      "episode: 2497   score: 10.0   memory length: 583848   epsilon: 0.04197898000853853    steps: 487    lr: 2.560000000000001e-06     evaluation reward: 6.7\n",
      "episode: 2498   score: 7.0   memory length: 584236   epsilon: 0.041210740008539054    steps: 388    lr: 2.560000000000001e-06     evaluation reward: 6.72\n",
      "episode: 2499   score: 5.0   memory length: 584505   epsilon: 0.04067812000853942    steps: 269    lr: 2.560000000000001e-06     evaluation reward: 6.7\n",
      "episode: 2500   score: 11.0   memory length: 585015   epsilon: 0.039668320008540106    steps: 510    lr: 2.560000000000001e-06     evaluation reward: 6.76\n",
      "episode: 2501   score: 6.0   memory length: 585324   epsilon: 0.03905650000854052    steps: 309    lr: 2.560000000000001e-06     evaluation reward: 6.77\n",
      "episode: 2502   score: 13.0   memory length: 585875   epsilon: 0.03796552000854127    steps: 551    lr: 2.560000000000001e-06     evaluation reward: 6.86\n",
      "episode: 2503   score: 5.0   memory length: 586163   epsilon: 0.037395280008541656    steps: 288    lr: 2.560000000000001e-06     evaluation reward: 6.79\n",
      "episode: 2504   score: 8.0   memory length: 586562   epsilon: 0.036605260008542195    steps: 399    lr: 2.560000000000001e-06     evaluation reward: 6.85\n",
      "episode: 2505   score: 9.0   memory length: 586959   epsilon: 0.03581920000854273    steps: 397    lr: 2.560000000000001e-06     evaluation reward: 6.9\n",
      "episode: 2506   score: 5.0   memory length: 587263   epsilon: 0.03521728000854314    steps: 304    lr: 2.560000000000001e-06     evaluation reward: 6.89\n",
      "episode: 2507   score: 15.0   memory length: 587827   epsilon: 0.034100560008543904    steps: 564    lr: 2.560000000000001e-06     evaluation reward: 6.96\n",
      "episode: 2508   score: 8.0   memory length: 588229   epsilon: 0.03330460000854445    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 7.0\n",
      "episode: 2509   score: 9.0   memory length: 588666   epsilon: 0.03243934000854504    steps: 437    lr: 2.560000000000001e-06     evaluation reward: 7.06\n",
      "episode: 2510   score: 7.0   memory length: 589027   epsilon: 0.031724560008545524    steps: 361    lr: 2.560000000000001e-06     evaluation reward: 7.09\n",
      "episode: 2511   score: 9.0   memory length: 589480   epsilon: 0.030827620008546136    steps: 453    lr: 2.560000000000001e-06     evaluation reward: 7.14\n",
      "episode: 2512   score: 7.0   memory length: 589865   epsilon: 0.030065320008546656    steps: 385    lr: 2.560000000000001e-06     evaluation reward: 7.13\n",
      "episode: 2513   score: 10.0   memory length: 590325   epsilon: 0.029154520008547277    steps: 460    lr: 2.560000000000001e-06     evaluation reward: 7.15\n",
      "episode: 2514   score: 12.0   memory length: 590851   epsilon: 0.028113040008547988    steps: 526    lr: 2.560000000000001e-06     evaluation reward: 7.2\n",
      "episode: 2515   score: 5.0   memory length: 591176   epsilon: 0.027469540008548426    steps: 325    lr: 2.560000000000001e-06     evaluation reward: 7.21\n",
      "episode: 2516   score: 5.0   memory length: 591456   epsilon: 0.026915140008548805    steps: 280    lr: 2.560000000000001e-06     evaluation reward: 7.22\n",
      "episode: 2517   score: 9.0   memory length: 591886   epsilon: 0.026063740008549385    steps: 430    lr: 2.560000000000001e-06     evaluation reward: 7.24\n",
      "episode: 2518   score: 10.0   memory length: 592386   epsilon: 0.02507374000855006    steps: 500    lr: 2.560000000000001e-06     evaluation reward: 7.31\n",
      "episode: 2519   score: 12.0   memory length: 592899   epsilon: 0.024058000008550753    steps: 513    lr: 2.560000000000001e-06     evaluation reward: 7.39\n",
      "episode: 2520   score: 5.0   memory length: 593206   epsilon: 0.023450140008551168    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 7.41\n",
      "episode: 2521   score: 11.0   memory length: 593724   epsilon: 0.022424500008551868    steps: 518    lr: 2.560000000000001e-06     evaluation reward: 7.49\n",
      "episode: 2522   score: 11.0   memory length: 594243   epsilon: 0.02139688000855257    steps: 519    lr: 2.560000000000001e-06     evaluation reward: 7.49\n",
      "episode: 2523   score: 5.0   memory length: 594515   epsilon: 0.020858320008552936    steps: 272    lr: 2.560000000000001e-06     evaluation reward: 7.49\n",
      "episode: 2524   score: 5.0   memory length: 594822   epsilon: 0.02025046000855335    steps: 307    lr: 2.560000000000001e-06     evaluation reward: 7.5\n",
      "episode: 2525   score: 5.0   memory length: 595095   epsilon: 0.01970992000855372    steps: 273    lr: 2.560000000000001e-06     evaluation reward: 7.5\n",
      "episode: 2526   score: 6.0   memory length: 595433   epsilon: 0.019040680008554176    steps: 338    lr: 2.560000000000001e-06     evaluation reward: 7.47\n",
      "episode: 2527   score: 7.0   memory length: 595810   epsilon: 0.018294220008554685    steps: 377    lr: 2.560000000000001e-06     evaluation reward: 7.52\n",
      "episode: 2528   score: 8.0   memory length: 596200   epsilon: 0.01752202000855521    steps: 390    lr: 2.560000000000001e-06     evaluation reward: 7.55\n",
      "episode: 2529   score: 10.0   memory length: 596734   epsilon: 0.016464700008555933    steps: 534    lr: 2.560000000000001e-06     evaluation reward: 7.59\n",
      "episode: 2530   score: 5.0   memory length: 597040   epsilon: 0.015858820008556346    steps: 306    lr: 2.560000000000001e-06     evaluation reward: 7.55\n",
      "episode: 2531   score: 5.0   memory length: 597364   epsilon: 0.015217300008556426    steps: 324    lr: 2.560000000000001e-06     evaluation reward: 7.55\n",
      "episode: 2532   score: 8.0   memory length: 597766   epsilon: 0.014421340008556272    steps: 402    lr: 2.560000000000001e-06     evaluation reward: 7.56\n",
      "episode: 2533   score: 7.0   memory length: 598147   epsilon: 0.013666960008556125    steps: 381    lr: 2.560000000000001e-06     evaluation reward: 7.6\n",
      "episode: 2534   score: 6.0   memory length: 598499   epsilon: 0.01297000000855599    steps: 352    lr: 2.560000000000001e-06     evaluation reward: 7.6\n",
      "episode: 2535   score: 3.0   memory length: 598710   epsilon: 0.012552220008555909    steps: 211    lr: 2.560000000000001e-06     evaluation reward: 7.54\n",
      "episode: 2536   score: 13.0   memory length: 599182   epsilon: 0.011617660008555727    steps: 472    lr: 2.560000000000001e-06     evaluation reward: 7.61\n",
      "episode: 2537   score: 5.0   memory length: 599452   epsilon: 0.011083060008555624    steps: 270    lr: 2.560000000000001e-06     evaluation reward: 7.6\n",
      "episode: 2538   score: 4.0   memory length: 599748   epsilon: 0.01049698000855551    steps: 296    lr: 2.560000000000001e-06     evaluation reward: 7.6\n",
      "episode: 2539   score: 6.0   memory length: 600088   epsilon: 0.009998020008555413    steps: 340    lr: 1.0240000000000005e-06     evaluation reward: 7.62\n",
      "episode: 2540   score: 7.0   memory length: 600443   epsilon: 0.009998020008555413    steps: 355    lr: 1.0240000000000005e-06     evaluation reward: 7.58\n",
      "episode: 2541   score: 12.0   memory length: 600976   epsilon: 0.009998020008555413    steps: 533    lr: 1.0240000000000005e-06     evaluation reward: 7.69\n",
      "episode: 2542   score: 5.0   memory length: 601256   epsilon: 0.009998020008555413    steps: 280    lr: 1.0240000000000005e-06     evaluation reward: 7.69\n",
      "episode: 2543   score: 10.0   memory length: 601731   epsilon: 0.009998020008555413    steps: 475    lr: 1.0240000000000005e-06     evaluation reward: 7.68\n",
      "episode: 2544   score: 11.0   memory length: 602250   epsilon: 0.009998020008555413    steps: 519    lr: 1.0240000000000005e-06     evaluation reward: 7.73\n",
      "episode: 2545   score: 10.0   memory length: 602692   epsilon: 0.009998020008555413    steps: 442    lr: 1.0240000000000005e-06     evaluation reward: 7.73\n",
      "episode: 2546   score: 13.0   memory length: 603271   epsilon: 0.009998020008555413    steps: 579    lr: 1.0240000000000005e-06     evaluation reward: 7.73\n",
      "episode: 2547   score: 8.0   memory length: 603656   epsilon: 0.009998020008555413    steps: 385    lr: 1.0240000000000005e-06     evaluation reward: 7.77\n",
      "episode: 2548   score: 12.0   memory length: 604249   epsilon: 0.009998020008555413    steps: 593    lr: 1.0240000000000005e-06     evaluation reward: 7.84\n",
      "episode: 2549   score: 11.0   memory length: 604750   epsilon: 0.009998020008555413    steps: 501    lr: 1.0240000000000005e-06     evaluation reward: 7.84\n",
      "episode: 2550   score: 11.0   memory length: 605253   epsilon: 0.009998020008555413    steps: 503    lr: 1.0240000000000005e-06     evaluation reward: 7.88\n",
      "episode: 2551   score: 18.0   memory length: 605738   epsilon: 0.009998020008555413    steps: 485    lr: 1.0240000000000005e-06     evaluation reward: 7.99\n",
      "episode: 2552   score: 10.0   memory length: 606237   epsilon: 0.009998020008555413    steps: 499    lr: 1.0240000000000005e-06     evaluation reward: 8.04\n",
      "episode: 2553   score: 13.0   memory length: 606816   epsilon: 0.009998020008555413    steps: 579    lr: 1.0240000000000005e-06     evaluation reward: 8.13\n",
      "episode: 2554   score: 10.0   memory length: 607273   epsilon: 0.009998020008555413    steps: 457    lr: 1.0240000000000005e-06     evaluation reward: 8.16\n",
      "episode: 2555   score: 5.0   memory length: 607566   epsilon: 0.009998020008555413    steps: 293    lr: 1.0240000000000005e-06     evaluation reward: 8.13\n",
      "episode: 2556   score: 13.0   memory length: 608145   epsilon: 0.009998020008555413    steps: 579    lr: 1.0240000000000005e-06     evaluation reward: 8.19\n",
      "episode: 2557   score: 6.0   memory length: 608473   epsilon: 0.009998020008555413    steps: 328    lr: 1.0240000000000005e-06     evaluation reward: 8.18\n",
      "episode: 2558   score: 10.0   memory length: 608910   epsilon: 0.009998020008555413    steps: 437    lr: 1.0240000000000005e-06     evaluation reward: 8.18\n",
      "episode: 2559   score: 6.0   memory length: 609263   epsilon: 0.009998020008555413    steps: 353    lr: 1.0240000000000005e-06     evaluation reward: 8.2\n",
      "episode: 2560   score: 10.0   memory length: 609738   epsilon: 0.009998020008555413    steps: 475    lr: 1.0240000000000005e-06     evaluation reward: 8.26\n",
      "episode: 2561   score: 3.0   memory length: 609967   epsilon: 0.009998020008555413    steps: 229    lr: 1.0240000000000005e-06     evaluation reward: 8.23\n",
      "episode: 2562   score: 11.0   memory length: 610478   epsilon: 0.009998020008555413    steps: 511    lr: 1.0240000000000005e-06     evaluation reward: 8.24\n",
      "episode: 2563   score: 11.0   memory length: 610998   epsilon: 0.009998020008555413    steps: 520    lr: 1.0240000000000005e-06     evaluation reward: 8.22\n",
      "episode: 2564   score: 4.0   memory length: 611255   epsilon: 0.009998020008555413    steps: 257    lr: 1.0240000000000005e-06     evaluation reward: 8.18\n",
      "episode: 2565   score: 6.0   memory length: 611586   epsilon: 0.009998020008555413    steps: 331    lr: 1.0240000000000005e-06     evaluation reward: 8.15\n",
      "episode: 2566   score: 3.0   memory length: 611815   epsilon: 0.009998020008555413    steps: 229    lr: 1.0240000000000005e-06     evaluation reward: 8.1\n",
      "episode: 2567   score: 11.0   memory length: 612333   epsilon: 0.009998020008555413    steps: 518    lr: 1.0240000000000005e-06     evaluation reward: 8.13\n",
      "episode: 2568   score: 10.0   memory length: 612779   epsilon: 0.009998020008555413    steps: 446    lr: 1.0240000000000005e-06     evaluation reward: 8.2\n",
      "episode: 2569   score: 6.0   memory length: 613135   epsilon: 0.009998020008555413    steps: 356    lr: 1.0240000000000005e-06     evaluation reward: 8.11\n",
      "episode: 2570   score: 13.0   memory length: 613719   epsilon: 0.009998020008555413    steps: 584    lr: 1.0240000000000005e-06     evaluation reward: 8.17\n",
      "episode: 2571   score: 8.0   memory length: 614171   epsilon: 0.009998020008555413    steps: 452    lr: 1.0240000000000005e-06     evaluation reward: 8.19\n",
      "episode: 2572   score: 11.0   memory length: 614691   epsilon: 0.009998020008555413    steps: 520    lr: 1.0240000000000005e-06     evaluation reward: 8.23\n",
      "episode: 2573   score: 8.0   memory length: 615076   epsilon: 0.009998020008555413    steps: 385    lr: 1.0240000000000005e-06     evaluation reward: 8.25\n",
      "episode: 2574   score: 9.0   memory length: 615487   epsilon: 0.009998020008555413    steps: 411    lr: 1.0240000000000005e-06     evaluation reward: 8.3\n",
      "episode: 2575   score: 8.0   memory length: 615872   epsilon: 0.009998020008555413    steps: 385    lr: 1.0240000000000005e-06     evaluation reward: 8.27\n",
      "episode: 2576   score: 9.0   memory length: 616283   epsilon: 0.009998020008555413    steps: 411    lr: 1.0240000000000005e-06     evaluation reward: 8.3\n",
      "episode: 2577   score: 7.0   memory length: 616640   epsilon: 0.009998020008555413    steps: 357    lr: 1.0240000000000005e-06     evaluation reward: 8.28\n",
      "episode: 2578   score: 7.0   memory length: 616995   epsilon: 0.009998020008555413    steps: 355    lr: 1.0240000000000005e-06     evaluation reward: 8.24\n",
      "episode: 2579   score: 9.0   memory length: 617451   epsilon: 0.009998020008555413    steps: 456    lr: 1.0240000000000005e-06     evaluation reward: 8.26\n",
      "episode: 2580   score: 12.0   memory length: 618022   epsilon: 0.009998020008555413    steps: 571    lr: 1.0240000000000005e-06     evaluation reward: 8.31\n",
      "episode: 2581   score: 10.0   memory length: 618493   epsilon: 0.009998020008555413    steps: 471    lr: 1.0240000000000005e-06     evaluation reward: 8.32\n",
      "episode: 2582   score: 10.0   memory length: 618969   epsilon: 0.009998020008555413    steps: 476    lr: 1.0240000000000005e-06     evaluation reward: 8.39\n",
      "episode: 2583   score: 10.0   memory length: 619469   epsilon: 0.009998020008555413    steps: 500    lr: 1.0240000000000005e-06     evaluation reward: 8.4\n",
      "episode: 2584   score: 7.0   memory length: 619804   epsilon: 0.009998020008555413    steps: 335    lr: 1.0240000000000005e-06     evaluation reward: 8.43\n",
      "episode: 2585   score: 5.0   memory length: 620108   epsilon: 0.009998020008555413    steps: 304    lr: 1.0240000000000005e-06     evaluation reward: 8.41\n",
      "episode: 2586   score: 6.0   memory length: 620444   epsilon: 0.009998020008555413    steps: 336    lr: 1.0240000000000005e-06     evaluation reward: 8.4\n",
      "episode: 2587   score: 11.0   memory length: 620993   epsilon: 0.009998020008555413    steps: 549    lr: 1.0240000000000005e-06     evaluation reward: 8.42\n",
      "episode: 2588   score: 13.0   memory length: 621572   epsilon: 0.009998020008555413    steps: 579    lr: 1.0240000000000005e-06     evaluation reward: 8.46\n",
      "episode: 2589   score: 10.0   memory length: 622052   epsilon: 0.009998020008555413    steps: 480    lr: 1.0240000000000005e-06     evaluation reward: 8.45\n",
      "episode: 2590   score: 4.0   memory length: 622309   epsilon: 0.009998020008555413    steps: 257    lr: 1.0240000000000005e-06     evaluation reward: 8.4\n",
      "episode: 2591   score: 8.0   memory length: 622695   epsilon: 0.009998020008555413    steps: 386    lr: 1.0240000000000005e-06     evaluation reward: 8.38\n",
      "episode: 2592   score: 12.0   memory length: 623288   epsilon: 0.009998020008555413    steps: 593    lr: 1.0240000000000005e-06     evaluation reward: 8.47\n",
      "episode: 2593   score: 15.0   memory length: 623800   epsilon: 0.009998020008555413    steps: 512    lr: 1.0240000000000005e-06     evaluation reward: 8.53\n",
      "episode: 2594   score: 4.0   memory length: 624041   epsilon: 0.009998020008555413    steps: 241    lr: 1.0240000000000005e-06     evaluation reward: 8.48\n",
      "episode: 2595   score: 5.0   memory length: 624314   epsilon: 0.009998020008555413    steps: 273    lr: 1.0240000000000005e-06     evaluation reward: 8.43\n",
      "episode: 2596   score: 11.0   memory length: 624802   epsilon: 0.009998020008555413    steps: 488    lr: 1.0240000000000005e-06     evaluation reward: 8.5\n",
      "episode: 2597   score: 13.0   memory length: 625281   epsilon: 0.009998020008555413    steps: 479    lr: 1.0240000000000005e-06     evaluation reward: 8.53\n",
      "episode: 2598   score: 7.0   memory length: 625686   epsilon: 0.009998020008555413    steps: 405    lr: 1.0240000000000005e-06     evaluation reward: 8.53\n",
      "episode: 2599   score: 11.0   memory length: 626203   epsilon: 0.009998020008555413    steps: 517    lr: 1.0240000000000005e-06     evaluation reward: 8.59\n",
      "episode: 2600   score: 5.0   memory length: 626496   epsilon: 0.009998020008555413    steps: 293    lr: 1.0240000000000005e-06     evaluation reward: 8.53\n",
      "episode: 2601   score: 6.0   memory length: 626832   epsilon: 0.009998020008555413    steps: 336    lr: 1.0240000000000005e-06     evaluation reward: 8.53\n",
      "episode: 2602   score: 11.0   memory length: 627320   epsilon: 0.009998020008555413    steps: 488    lr: 1.0240000000000005e-06     evaluation reward: 8.51\n",
      "episode: 2603   score: 10.0   memory length: 627790   epsilon: 0.009998020008555413    steps: 470    lr: 1.0240000000000005e-06     evaluation reward: 8.56\n",
      "episode: 2604   score: 9.0   memory length: 628201   epsilon: 0.009998020008555413    steps: 411    lr: 1.0240000000000005e-06     evaluation reward: 8.57\n",
      "episode: 2605   score: 9.0   memory length: 628634   epsilon: 0.009998020008555413    steps: 433    lr: 1.0240000000000005e-06     evaluation reward: 8.57\n",
      "episode: 2606   score: 10.0   memory length: 629074   epsilon: 0.009998020008555413    steps: 440    lr: 1.0240000000000005e-06     evaluation reward: 8.62\n",
      "episode: 2607   score: 5.0   memory length: 629362   epsilon: 0.009998020008555413    steps: 288    lr: 1.0240000000000005e-06     evaluation reward: 8.52\n",
      "episode: 2608   score: 4.0   memory length: 629619   epsilon: 0.009998020008555413    steps: 257    lr: 1.0240000000000005e-06     evaluation reward: 8.48\n",
      "episode: 2609   score: 12.0   memory length: 630152   epsilon: 0.009998020008555413    steps: 533    lr: 1.0240000000000005e-06     evaluation reward: 8.51\n",
      "episode: 2610   score: 7.0   memory length: 630555   epsilon: 0.009998020008555413    steps: 403    lr: 1.0240000000000005e-06     evaluation reward: 8.51\n",
      "episode: 2611   score: 7.0   memory length: 630906   epsilon: 0.009998020008555413    steps: 351    lr: 1.0240000000000005e-06     evaluation reward: 8.49\n",
      "episode: 2612   score: 11.0   memory length: 631385   epsilon: 0.009998020008555413    steps: 479    lr: 1.0240000000000005e-06     evaluation reward: 8.53\n",
      "episode: 2613   score: 3.0   memory length: 631596   epsilon: 0.009998020008555413    steps: 211    lr: 1.0240000000000005e-06     evaluation reward: 8.46\n",
      "episode: 2614   score: 3.0   memory length: 631807   epsilon: 0.009998020008555413    steps: 211    lr: 1.0240000000000005e-06     evaluation reward: 8.37\n",
      "episode: 2615   score: 5.0   memory length: 632080   epsilon: 0.009998020008555413    steps: 273    lr: 1.0240000000000005e-06     evaluation reward: 8.37\n",
      "episode: 2616   score: 3.0   memory length: 632291   epsilon: 0.009998020008555413    steps: 211    lr: 1.0240000000000005e-06     evaluation reward: 8.35\n",
      "episode: 2617   score: 6.0   memory length: 632621   epsilon: 0.009998020008555413    steps: 330    lr: 1.0240000000000005e-06     evaluation reward: 8.32\n",
      "episode: 2618   score: 10.0   memory length: 633058   epsilon: 0.009998020008555413    steps: 437    lr: 1.0240000000000005e-06     evaluation reward: 8.32\n",
      "episode: 2619   score: 9.0   memory length: 633504   epsilon: 0.009998020008555413    steps: 446    lr: 1.0240000000000005e-06     evaluation reward: 8.29\n",
      "episode: 2620   score: 8.0   memory length: 633888   epsilon: 0.009998020008555413    steps: 384    lr: 1.0240000000000005e-06     evaluation reward: 8.32\n",
      "episode: 2621   score: 11.0   memory length: 634421   epsilon: 0.009998020008555413    steps: 533    lr: 1.0240000000000005e-06     evaluation reward: 8.32\n",
      "episode: 2622   score: 8.0   memory length: 634808   epsilon: 0.009998020008555413    steps: 387    lr: 1.0240000000000005e-06     evaluation reward: 8.29\n",
      "episode: 2623   score: 4.0   memory length: 635048   epsilon: 0.009998020008555413    steps: 240    lr: 1.0240000000000005e-06     evaluation reward: 8.28\n",
      "episode: 2624   score: 8.0   memory length: 635448   epsilon: 0.009998020008555413    steps: 400    lr: 1.0240000000000005e-06     evaluation reward: 8.31\n",
      "episode: 2625   score: 4.0   memory length: 635727   epsilon: 0.009998020008555413    steps: 279    lr: 1.0240000000000005e-06     evaluation reward: 8.3\n",
      "episode: 2626   score: 3.0   memory length: 635938   epsilon: 0.009998020008555413    steps: 211    lr: 1.0240000000000005e-06     evaluation reward: 8.27\n",
      "episode: 2627   score: 8.0   memory length: 636392   epsilon: 0.009998020008555413    steps: 454    lr: 1.0240000000000005e-06     evaluation reward: 8.28\n",
      "episode: 2628   score: 9.0   memory length: 636808   epsilon: 0.009998020008555413    steps: 416    lr: 1.0240000000000005e-06     evaluation reward: 8.29\n",
      "episode: 2629   score: 8.0   memory length: 637184   epsilon: 0.009998020008555413    steps: 376    lr: 1.0240000000000005e-06     evaluation reward: 8.27\n",
      "episode: 2630   score: 4.0   memory length: 637425   epsilon: 0.009998020008555413    steps: 241    lr: 1.0240000000000005e-06     evaluation reward: 8.26\n",
      "episode: 2631   score: 7.0   memory length: 637792   epsilon: 0.009998020008555413    steps: 367    lr: 1.0240000000000005e-06     evaluation reward: 8.28\n",
      "episode: 2632   score: 8.0   memory length: 638192   epsilon: 0.009998020008555413    steps: 400    lr: 1.0240000000000005e-06     evaluation reward: 8.28\n",
      "episode: 2633   score: 4.0   memory length: 638433   epsilon: 0.009998020008555413    steps: 241    lr: 1.0240000000000005e-06     evaluation reward: 8.25\n",
      "episode: 2634   score: 7.0   memory length: 638802   epsilon: 0.009998020008555413    steps: 369    lr: 1.0240000000000005e-06     evaluation reward: 8.26\n",
      "episode: 2635   score: 4.0   memory length: 639043   epsilon: 0.009998020008555413    steps: 241    lr: 1.0240000000000005e-06     evaluation reward: 8.27\n",
      "episode: 2636   score: 7.0   memory length: 639412   epsilon: 0.009998020008555413    steps: 369    lr: 1.0240000000000005e-06     evaluation reward: 8.21\n",
      "episode: 2637   score: 9.0   memory length: 639861   epsilon: 0.009998020008555413    steps: 449    lr: 1.0240000000000005e-06     evaluation reward: 8.25\n",
      "episode: 2638   score: 9.0   memory length: 640310   epsilon: 0.009998020008555413    steps: 449    lr: 1.0240000000000005e-06     evaluation reward: 8.3\n",
      "episode: 2639   score: 5.0   memory length: 640591   epsilon: 0.009998020008555413    steps: 281    lr: 1.0240000000000005e-06     evaluation reward: 8.29\n",
      "episode: 2640   score: 8.0   memory length: 640992   epsilon: 0.009998020008555413    steps: 401    lr: 1.0240000000000005e-06     evaluation reward: 8.3\n",
      "episode: 2641   score: 13.0   memory length: 641604   epsilon: 0.009998020008555413    steps: 612    lr: 1.0240000000000005e-06     evaluation reward: 8.31\n",
      "episode: 2642   score: 6.0   memory length: 641934   epsilon: 0.009998020008555413    steps: 330    lr: 1.0240000000000005e-06     evaluation reward: 8.32\n",
      "episode: 2643   score: 15.0   memory length: 642538   epsilon: 0.009998020008555413    steps: 604    lr: 1.0240000000000005e-06     evaluation reward: 8.37\n",
      "episode: 2644   score: 6.0   memory length: 642858   epsilon: 0.009998020008555413    steps: 320    lr: 1.0240000000000005e-06     evaluation reward: 8.32\n",
      "episode: 2645   score: 9.0   memory length: 643304   epsilon: 0.009998020008555413    steps: 446    lr: 1.0240000000000005e-06     evaluation reward: 8.31\n",
      "episode: 2646   score: 12.0   memory length: 643862   epsilon: 0.009998020008555413    steps: 558    lr: 1.0240000000000005e-06     evaluation reward: 8.3\n",
      "episode: 2647   score: 5.0   memory length: 644169   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 8.27\n",
      "episode: 2648   score: 12.0   memory length: 644724   epsilon: 0.009998020008555413    steps: 555    lr: 1.0240000000000005e-06     evaluation reward: 8.27\n",
      "episode: 2649   score: 10.0   memory length: 645187   epsilon: 0.009998020008555413    steps: 463    lr: 1.0240000000000005e-06     evaluation reward: 8.26\n",
      "episode: 2650   score: 6.0   memory length: 645489   epsilon: 0.009998020008555413    steps: 302    lr: 1.0240000000000005e-06     evaluation reward: 8.21\n",
      "episode: 2651   score: 11.0   memory length: 646000   epsilon: 0.009998020008555413    steps: 511    lr: 1.0240000000000005e-06     evaluation reward: 8.14\n",
      "episode: 2652   score: 5.0   memory length: 646307   epsilon: 0.009998020008555413    steps: 307    lr: 1.0240000000000005e-06     evaluation reward: 8.09\n",
      "episode: 2653   score: 6.0   memory length: 646648   epsilon: 0.009998020008555413    steps: 341    lr: 1.0240000000000005e-06     evaluation reward: 8.02\n",
      "episode: 2654   score: 11.0   memory length: 647132   epsilon: 0.009998020008555413    steps: 484    lr: 1.0240000000000005e-06     evaluation reward: 8.03\n",
      "episode: 2655   score: 8.0   memory length: 647575   epsilon: 0.009998020008555413    steps: 443    lr: 1.0240000000000005e-06     evaluation reward: 8.06\n",
      "episode: 2656   score: 4.0   memory length: 647816   epsilon: 0.009998020008555413    steps: 241    lr: 1.0240000000000005e-06     evaluation reward: 7.97\n",
      "episode: 2657   score: 13.0   memory length: 648294   epsilon: 0.009998020008555413    steps: 478    lr: 1.0240000000000005e-06     evaluation reward: 8.04\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_23936\\1016637510.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     28\u001b[0m         \u001b[0mdone\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 30\u001b[1;33m         \u001b[0mframe_next_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     31\u001b[0m         \u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mframe_next_state\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mterminal_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_live\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlife\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lives'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\Personal_Stuff\\UIUC\\Sem_2\\DL for CV (CS_444)\\Assignments\\assignment5\\utils.py\u001b[0m in \u001b[0;36mget_frame\u001b[1;34m(X)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_frame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb2gray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mHEIGHT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mWIDTH\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'reflect'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m255\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\vaibh\\anaconda3\\lib\\site-packages\\skimage\\_shared\\utils.py\u001b[0m in \u001b[0;36mfixed_func\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mchannel_axis\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 394\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    395\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    396\u001b[0m             \u001b[1;31m# TODO: convert scalars to a tuple in anticipation of eventually\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\vaibh\\anaconda3\\lib\\site-packages\\skimage\\color\\colorconv.py\u001b[0m in \u001b[0;36mrgb2gray\u001b[1;34m(rgb, channel_axis)\u001b[0m\n\u001b[0;32m    873\u001b[0m     \u001b[1;33m>>\u001b[0m\u001b[1;33m>\u001b[0m \u001b[0mimg_gray\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrgb2gray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    874\u001b[0m     \"\"\"\n\u001b[1;32m--> 875\u001b[1;33m     \u001b[0mrgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_prepare_colorarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrgb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    876\u001b[0m     \u001b[0mcoeffs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0.2125\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.7154\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.0721\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    877\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mrgb\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mcoeffs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\vaibh\\anaconda3\\lib\\site-packages\\skimage\\color\\colorconv.py\u001b[0m in \u001b[0;36m_prepare_colorarray\u001b[1;34m(arr, force_copy, channel_axis)\u001b[0m\n\u001b[0;32m    145\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    146\u001b[0m         \u001b[0m_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimg_as_float64\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 147\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_copy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mforce_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    148\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\vaibh\\anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py\u001b[0m in \u001b[0;36mimg_as_float64\u001b[1;34m(image, force_copy)\u001b[0m\n\u001b[0;32m    437\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    438\u001b[0m     \"\"\"\n\u001b[1;32m--> 439\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_convert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat64\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_copy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    440\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    441\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\vaibh\\anaconda3\\lib\\site-packages\\skimage\\util\\dtype.py\u001b[0m in \u001b[0;36m_convert\u001b[1;34m(image, dtype, force_copy, uniform)\u001b[0m\n\u001b[0;32m    317\u001b[0m             \u001b[1;31m# using np.divide or np.multiply doesn't copy the data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    318\u001b[0m             \u001b[1;31m# until the computation time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m             image = np.multiply(image, 1. / imax_in,\n\u001b[0m\u001b[0;32m    320\u001b[0m                                 dtype=computation_type)\n\u001b[0;32m    321\u001b[0m             \u001b[1;31m# DirectX uses this conversion also for signed ints\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "rewards, episodes = [], []\n",
    "best_eval_reward = 0\n",
    "for e in range(EPISODES):\n",
    "    done = False\n",
    "    score = 0\n",
    "\n",
    "    history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "    step = 0\n",
    "    state, _ = env.reset()\n",
    "    next_state = state\n",
    "    life = number_lives\n",
    "\n",
    "    get_init_state(history, state, HISTORY_SIZE)\n",
    "\n",
    "    while not done:\n",
    "        step += 1\n",
    "        frame += 1\n",
    "\n",
    "        # Perform a fire action if ball is no longer on screen to continue onto next life\n",
    "        if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "            action = torch.tensor([[0]]).cuda()\n",
    "        else:\n",
    "            action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "        state = next_state\n",
    "        \n",
    "        next_state, reward, terminated, truncated, info = env.step(action + 1)\n",
    "        \n",
    "        done = terminated or truncated\n",
    "        \n",
    "        frame_next_state = get_frame(next_state)\n",
    "        history[4, :, :] = frame_next_state\n",
    "        terminal_state = check_live(life, info['lives'])\n",
    "\n",
    "        life = info['lives']\n",
    "        r = reward\n",
    "\n",
    "        # Store the transition in memory \n",
    "        agent.memory.push(deepcopy(frame_next_state), action.cpu(), r, terminal_state)\n",
    "        # Start training after random sample generation\n",
    "        if(frame >= train_frame): # You can set train_frame to a lower value while testing your starts training earlier\n",
    "            agent.train_policy_net(frame)\n",
    "            # Update the target network only for Double DQN only\n",
    "            if double_dqn and (frame % update_target_network_frequency)== 0:\n",
    "                agent.update_target_net()\n",
    "        score += reward\n",
    "        history[:4, :, :] = history[1:, :, :]\n",
    "            \n",
    "        if done:\n",
    "            evaluation_reward.append(score)\n",
    "            rewards.append(np.mean(evaluation_reward))\n",
    "            episodes.append(e)\n",
    "            pylab.plot(episodes, rewards, 'b')\n",
    "            pylab.xlabel('Episodes')\n",
    "            pylab.ylabel('Rewards') \n",
    "            pylab.title('Episodes vs Reward')\n",
    "            pylab.savefig(\"./save_graph/breakout_dqn.png\") # save graph for training visualization\n",
    "            \n",
    "            # every episode, plot the play time\n",
    "            print(\"episode:\", e, \"  score:\", score, \"  memory length:\",\n",
    "                  len(agent.memory), \"  epsilon:\", agent.epsilon, \"   steps:\", step,\n",
    "                  \"   lr:\", agent.optimizer.param_groups[0]['lr'], \"    evaluation reward:\", np.mean(evaluation_reward))\n",
    "\n",
    "            # if the mean of scores of last 100 episode is bigger than 5 save model\n",
    "            ### Change this save condition to whatever you prefer ###\n",
    "            if np.mean(evaluation_reward) > 5 and np.mean(evaluation_reward) > best_eval_reward:\n",
    "                torch.save(agent.policy_net, \"./save_model/breakout_dqn.pth\")\n",
    "                best_eval_reward = np.mean(evaluation_reward)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize Agent Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BE AWARE THIS CODE BELOW MAY CRASH THE KERNEL IF YOU RUN THE SAME CELL TWICE.\n",
    "\n",
    "Please save your model before running this portion of the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(agent.policy_net, \"./save_model/breakout_dqn_latest.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video e:\\Personal_Stuff\\UIUC\\Sem_2\\DL for CV (CS_444)\\Assignments\\assignment5\\video\\rl-video-episode-0.mp4.\n",
      "Moviepy - Writing video e:\\Personal_Stuff\\UIUC\\Sem_2\\DL for CV (CS_444)\\Assignments\\assignment5\\video\\rl-video-episode-0.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready e:\\Personal_Stuff\\UIUC\\Sem_2\\DL for CV (CS_444)\\Assignments\\assignment5\\video\\rl-video-episode-0.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "data": {
      "text/html": [
       "<video style=\"height: 400px;\" controls=\"\" loop=\"\" autoplay=\"\" alt=\"test\">\n",
       "                <source type=\"video/mp4\" src=\"data:video/mp4;base64,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\">\n",
       "             </video>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import sys\n",
    "import gym\n",
    "import torch\n",
    "import pylab\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from utils import find_max_lives, check_live, get_frame, get_init_state\n",
    "from model import DQN, DQN_LSTM\n",
    "from config import *\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# %load_ext autoreload\n",
    "# %autoreload 2\n",
    "\n",
    "from gym.wrappers import RecordVideo # If importing monitor raises issues, try using `from gym.wrappers import RecordVideo`\n",
    "import glob\n",
    "import io\n",
    "import base64\n",
    "\n",
    "from IPython.display import HTML\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "from pyvirtualdisplay import Display\n",
    "\n",
    "env = gym.make('BreakoutDeterministic-v4', render_mode='rgb_array')\n",
    "state = env.reset()\n",
    "number_lives = find_max_lives(env)\n",
    "state_size = env.observation_space.shape\n",
    "action_size = 3 #fire, left, and right\n",
    "\n",
    "# Displaying the game live\n",
    "def show_state(env, step=0, info=\"\"):\n",
    "    plt.figure(3)\n",
    "    plt.clf()\n",
    "    plt.imshow(env.render(mode='rgb_array'))\n",
    "    plt.title(\"%s | Step: %d %s\" % (\"Agent Playing\",step, info))\n",
    "    plt.axis('off')\n",
    "\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    \n",
    "# Recording the game and replaying the game afterwards\n",
    "def show_video():\n",
    "    mp4list = glob.glob('video/*.mp4')\n",
    "    if len(mp4list) > 0:\n",
    "        mp4 = mp4list[0]\n",
    "        video = io.open(mp4, 'r+b').read()\n",
    "        encoded = base64.b64encode(video)\n",
    "        ipythondisplay.display(HTML(data='''<video style=\"height: 400px;\" controls=\"\" loop=\"\" autoplay=\"\" alt=\"test\">\n",
    "                <source type=\"video/mp4\" src=\"data:video/mp4;base64,{0}\">\n",
    "             </video>'''.format(encoded.decode('ascii'))))\n",
    "    else: \n",
    "        print(\"Could not find video\")\n",
    "    \n",
    "\n",
    "def wrap_env(env):\n",
    "    env = RecordVideo(env, './video')\n",
    "    return env\n",
    "\n",
    "from agent import Agent\n",
    "action_size = 3 \n",
    "\n",
    "\n",
    "# Load agent\n",
    "agent = Agent(action_size)\n",
    "agent.load_policy_net(\"./save_model/breakout_dqn_latest.pth\")\n",
    "agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "env = wrap_env(env)\n",
    "\n",
    "done = False\n",
    "score = 0\n",
    "step = 0\n",
    "state, _ = env.reset()\n",
    "next_state = state\n",
    "life = number_lives\n",
    "history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "get_init_state(history, state, HISTORY_SIZE)\n",
    "frame = 0\n",
    "while not done:\n",
    "#     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "    step += 1\n",
    "    frame += 1\n",
    "\n",
    "    # Perform a fire action if ball is no longer on screen\n",
    "    if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "        action = 0\n",
    "    else:\n",
    "        action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "    state = next_state\n",
    "    \n",
    "    next_state, reward, done, _, info = env.step(action + 1)\n",
    "        \n",
    "    frame_next_state = get_frame(next_state)\n",
    "    history[4, :, :] = frame_next_state\n",
    "    terminal_state = check_live(life, info['lives'])\n",
    "        \n",
    "    life = info['lives']\n",
    "    r = np.clip(reward, -1, 1) \n",
    "    r = reward\n",
    "\n",
    "    # Store the transition in memory \n",
    "    agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "    # Start training after random sample generation\n",
    "    score += reward\n",
    "    \n",
    "    history[:4, :, :] = history[1:, :, :]\n",
    "env.close()\n",
    "show_video()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display = Display(visible=0, size=(300, 200))\n",
    "# display.start()\n",
    "\n",
    "# # Load agent\n",
    "# # agent.load_policy_net(\"./save_model/breakout_dqn.pth\")\n",
    "# agent.epsilon = 0.0 # Set agent to only exploit the best action\n",
    "\n",
    "# env = gym.make('BreakoutDeterministic-v4')\n",
    "# env = wrap_env(env)\n",
    "\n",
    "# done = False\n",
    "# score = 0\n",
    "# step = 0\n",
    "# state = env.reset()\n",
    "# next_state = state\n",
    "# life = number_lives\n",
    "# history = np.zeros([5, 84, 84], dtype=np.uint8)\n",
    "# get_init_state(history, state)\n",
    "\n",
    "# while not done:\n",
    "    \n",
    "#     # Render breakout\n",
    "#     env.render()\n",
    "# #     show_state(env,step) # uncommenting this provides another way to visualize the game\n",
    "\n",
    "#     step += 1\n",
    "#     frame += 1\n",
    "\n",
    "#     # Perform a fire action if ball is no longer on screen\n",
    "#     if step > 1 and len(np.unique(next_state[:189] == state[:189])) < 2:\n",
    "#         action = 0\n",
    "#     else:\n",
    "#         action = agent.get_action(np.float32(history[:4, :, :]) / 255.)\n",
    "#     state = next_state\n",
    "    \n",
    "#     next_state, reward, done, info = env.step(action + 1)\n",
    "        \n",
    "#     frame_next_state = get_frame(next_state)\n",
    "#     history[4, :, :] = frame_next_state\n",
    "#     terminal_state = check_live(life, info['ale.lives'])\n",
    "        \n",
    "#     life = info['ale.lives']\n",
    "#     r = np.clip(reward, -1, 1) \n",
    "#     r = reward\n",
    "\n",
    "#     # Store the transition in memory \n",
    "#     agent.memory.push(deepcopy(frame_next_state), action, r, terminal_state)\n",
    "#     # Start training after random sample generation\n",
    "#     score += reward\n",
    "    \n",
    "#     history[:4, :, :] = history[1:, :, :]\n",
    "# env.close()\n",
    "# show_video()\n",
    "# display.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
